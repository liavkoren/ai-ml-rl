{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class DqnBaseClass:\n",
    "    env = attr.ib()\n",
    "    discount_rate = attr.ib(default=0.99)\n",
    "    epsilon_max = attr.ib(default=1.0)\n",
    "    epsilon_min = attr.ib(default=0.01)\n",
    "    annealing_const = attr.ib(default=.001)  # aka Lambda\n",
    "    minibatch_size = attr.ib(default=64)\n",
    "    memory_size = attr.ib(default=int(1e6))\n",
    "    num_episodes = attr.ib(default=1000)  # num of episodes in a training epoch\n",
    "    num_hidden_units = attr.ib(default=64)\n",
    "    render_every = attr.ib(default=20)  # set to zero to turn off rendering\n",
    "    update_target_every = attr.ib(default=200)\n",
    "    reward_clip_ceiling = attr.ib(None)\n",
    "    reward_clip_floor = attr.ib(None)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.steps = 0\n",
    "        self.reset_memory()\n",
    "        self.reset_data_recorders()\n",
    "        self.state_shape = self.env.observation_space.shape\n",
    "        self.action_shape = self.env.action_space.n\n",
    "        self.online_net = make_network(self.state_shape, self.action_shape, self.num_hidden_units)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory = Memory(self.memory_size, self.minibatch_size)\n",
    "\n",
    "    def reset_data_recorders(self):\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "        self.td_errors = []\n",
    "        self.online_net_q_values = []\n",
    "        self.target_net_q_values = []\n",
    "        self.w1_gradient = []\n",
    "        self.w2_gradient = []\n",
    "\n",
    "    def q_value_one(self, net, state):\n",
    "        return net.predict(state.reshape((1, self.state_shape[0]))).flatten()\n",
    "\n",
    "    def training_rewards_string(self, episode):\n",
    "        last_ep = self.episode_rewards[-1]\n",
    "        ten_ep_mean = sum(self.episode_rewards[-10:])/len(self.episode_rewards[-10:])\n",
    "        hundred_ep_mean = sum(self.episode_rewards[-100:])/len(self.episode_rewards[-100:])\n",
    "        return f'Ep: {episode} // steps: {self.steps} // last ep reward: {last_ep:.2f} // {min(10, len(self.episode_rewards[-10:]))}-ep mean: {ten_ep_mean:.2f} // {min(100, len(self.episode_rewards[-100:]))}-ep mean: {hundred_ep_mean:.2f}'\n",
    "\n",
    "    def render(self, episode):\n",
    "        if self.render_every and episode % self.render_every == 0:\n",
    "            self.env.render()\n",
    "\n",
    "    # TODO: rename to train\n",
    "    def run(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def display(self, num_episodes=1):\n",
    "        state = self.env.reset()\n",
    "        episode_done = False\n",
    "        episode_td_errors = 0\n",
    "        self.reset_data_recorders()\n",
    "        for episode in range(num_episodes)\n",
    "            while not episode_done:            \n",
    "                action = self.q_value_one(self.online_net, state).argmax()                    \n",
    "                self.env.render()\n",
    "                state_, reward, episode_done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_td_errors = self.replay(target_net=self.online_net, online_net=self.online_net)\n",
    "                self.td_errors.append(episode_td_errors)\n",
    "                state = state_\n",
    "                if episode_done:\n",
    "                    self.episode_rewards.append(episode_reward)                    \n",
    "                    print(self.training_rewards_string(episode), end='\\r', flush=True)\n",
    "        self.env.close()        \n",
    "        return episode_td_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized Experience Replay tries to use experience more efficiently by assigning higher probabilities to transitions which may have more to teach us. \"More to teach us\" is, obviously, a difficult concept to quantify, but one potentially good proxy for it is the temporal difference error of a transition. A larger error indicates a greater surprise factor. \n",
    "\n",
    "In the [Rainbow DQN](https://arxiv.org/abs/1710.02298) paper, the authors found that PER was one of the most impactful improvements to the DQN framework, and there are some obvious additional prioritization schemes, for example providing a higher priority to rollouts where the total score was higher. However, at this point I have simply implemented PER as outline in the [original paper](https://arxiv.org/abs/1511.05952). \n",
    "\n",
    "Storing and retrieving transitions with priorities requires us to use a different data structure for our memory. The PER paper outlines two different ways to calculate transition priorities, each with their own custom data structure. For this implementation I am using Jaromiru's [SumTree](https://github.com/jaara/AI-blog/blob/master/SumTree.py) reference implementation. Please see their excellent [blog post](https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SumTree import SumTree\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class PriorityMemory:\n",
    "    size = attr.ib()\n",
    "    batch_size = attr.ib()\n",
    "    epsilon = attr.ib(default=0.01)\n",
    "    alpha = attr.ib(default=0.6)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.sumtree = SumTree(capacity=self.size)\n",
    "        self._len = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def _td_error_to_priority(self, td_error):\n",
    "        return (td_error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def store(self, td_error, transition):\n",
    "        self._len += 1\n",
    "        priority = self._td_error_to_priority(td_error)\n",
    "        self.sumtree.add(priority, transition)\n",
    "\n",
    "    def sample(self):\n",
    "        # https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py#L102\n",
    "        batch = []\n",
    "        segment_size = self.sumtree.total() // self.batch_size\n",
    "        for index in range(self.batch_size):\n",
    "            a = segment_size * index\n",
    "            b = segment_size * (index + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            batch.append(self.sumtree.get(s))  # a list of (idx, p, data) tuples\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, td_error):\n",
    "        priority = self._td_error_to_priority(td_error)\n",
    "        self.sumtree.update(idx, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class PrioritizedDqn(DqnBaseClass):\n",
    "    random_init_steps = attr.ib(1000)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        super().__attrs_post_init__()\n",
    "        self.target_net = make_network(self.state_shape, self.action_shape, self.num_hidden_units)\n",
    "        self.memory = PriorityMemory(self.memory_size, self.minibatch_size)\n",
    "        print('Starting to initializing memory with random agent.')\n",
    "        state = self.env.reset()\n",
    "        for _ in range(self.random_init_steps):\n",
    "            action = random.randint(0, self.action_shape-1)\n",
    "            state_, reward, episode_done, _ = self.env.step(action)\n",
    "            # See https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n",
    "            # for discussion of random init and rewards as naive error signal\n",
    "            self.memory.store(reward, (state, action, abs(reward), state_, episode_done))\n",
    "            if episode_done:\n",
    "                state = self.env.reset()\n",
    "            else:\n",
    "                state = state_\n",
    "        print('Finished initializing memory with random agent.')\n",
    "        \n",
    "    def replay(self):\n",
    "        batch = self.memory.sample()  # A list of (idx, prob, observation) tuples\n",
    "        observations = [observe[2] for observe in batch]\n",
    "        states = np.array([transition[0] for transition in observations])  # (observations x state-size)\n",
    "        transition_actions = np.array([transition[1] for transition in observations])\n",
    "        rewards = np.array([transition[2] for transition in observations])  # (observations x 1)\n",
    "        terminal_mask = np.array([True if transition[3] is None else False for transition in observations])  # (observations x 1)\n",
    "        terminal_state = np.zeros(self.state_shape)\n",
    "        states_ = np.array([transition[3] if transition[3] is not None else terminal_state for transition in observations])  # (observations x state-size)\n",
    "\n",
    "        # set y = r for terminal states:\n",
    "        terminal_state_actions = transition_actions[terminal_mask]\n",
    "        y = self.target_net.predict(states)  # (observations x num-actions)\n",
    "        y[terminal_mask, terminal_state_actions] = rewards[terminal_mask]\n",
    "\n",
    "        # DDQN update:\n",
    "        # set y = r + gamma * Q_hat(s', argmax Q(s', a')). Remember that y_ is the output of Q_hat, aka target_net.\n",
    "        non_terminal_mask = ~terminal_mask\n",
    "        online_predicted_actions_ = self.online_net.predict(states_).argmax(axis=1)  # observations x num-action\n",
    "        best_actions = online_predicted_actions_[non_terminal_mask]\n",
    "        non_terminal_actions = transition_actions[non_terminal_mask]\n",
    "        y_ = self.target_net.predict(states_)  # (observations x num-actions)\n",
    "        y[non_terminal_mask, non_terminal_actions] = rewards[non_terminal_mask] + self.discount_rate * y_[non_terminal_mask, best_actions]\n",
    "\n",
    "        priorities = np.array([observe[1] for observe in batch])\n",
    "        probabilities = priorities / sum(priorities)\n",
    "        BETA = 0.4\n",
    "        weights = (self.memory_size * probabilities) ** -BETA\n",
    "        weights = weights / weights.max()\n",
    "        self.online_net.fit(states, y, batch_size=64, epochs=1, sample_weight=weights, verbose=0)  # REMEBER, Q is a func from (state, action) pairs to values.\n",
    "\n",
    "        memory_indices = [observe[0] for observe in batch]\n",
    "        target_predictions = self.target_net.predict(states_)[range(self.minibatch_size), online_predicted_actions_]\n",
    "        online_predictions = self.online_net.predict(states)[range(self.minibatch_size), transition_actions]\n",
    "\n",
    "        td_errors = rewards + self.discount_rate * target_predictions - online_predictions  # should be (batch x 1)        \n",
    "        td_errors = np.abs(td_errors)\n",
    "        for idx, error in zip(memory_indices, td_errors):\n",
    "            self.memory.update(idx, error)\n",
    "        return np.linalg.norm(td_errors)/self.minibatch_size\n",
    "\n",
    "    def run(self):        \n",
    "        for episode in range(self.num_episodes):\n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_td_errors = 0\n",
    "            state = self.env.reset()\n",
    "            self.online_net_q_values.append(self.q_value_one(self.online_net, state).max())\n",
    "            self.target_net_q_values.append(self.q_value_one(self.target_net, state).max())\n",
    "            while not episode_done:\n",
    "                epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * math.exp(-self.annealing_const * self.steps)\n",
    "                self.steps += 1\n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, self.action_shape-1)\n",
    "                else:\n",
    "                    action = self.q_value_one(self.online_net, state).argmax()\n",
    "                self.render(episode)\n",
    "                state_, reward, episode_done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                if episode_done:\n",
    "                    state_ = None\n",
    "                # This is wrong, should be the TD error, not abs(reward)\n",
    "                self.memory.store(abs(reward), (state, action, reward, state_, episode_done))\n",
    "                episode_td_errors += self.replay()\n",
    "                state = state_\n",
    "                if self.steps % self.update_target_every == 0:\n",
    "                    self.target_net.set_weights(self.online_net.get_weights())\n",
    "                if episode_done:\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.td_errors.append(episode_td_errors)\n",
    "                    print(self.training_rewards_string(episode), end='\\r', flush=True)\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Ep: 1499 // steps: 318728 // last ep reward: 278.00 // 10-ep mean: 319.50 // 100-ep mean: 348.82\r"
     ]
    }
   ],
   "source": [
    "per = PrioritizedDqn(cartpole, num_episodes=1500)\n",
    "per.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'per' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-2c97bae58514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspines_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_every\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_every\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode rewards'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'per' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAD8CAYAAAB+Q1lpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAEntJREFUeJzt3V+o5Gd5B/DvY7ZRStUWgyDZVSPdgFstGELQm2qrlk0K2QuLJEVaSzDVNlKwLQS8qMQrW7QgBO2BStSiMXpRFowEahMCYjQL0WgSEtYozUZpSvxzIxqDTy/OpD2ent0ze3b+vDPz+cAPzsy8mXmfzMk3fGfmd6a6OwAAADCK5y17AwAAALCTogoAAMBQFFUAAACGoqgCAAAwFEUVAACAoSiqAAAADGXfolpVn6iqp6rq22e5varqo1V1uqoerKorZr9NgPmSdcCmkHfAKpjmHdXbkhw/x+1XJzk6OW5M8rEL3xbAwt0WWQdshtsi74DB7VtUu/veJD88x5ITST7V2+5L8ptV9bJZbRBgEWQdsCnkHbAKZnGO6qVJnthx+czkuv+nqm6sqlNVdWpra6uTOBwOx+5jVLLO4XDM8hjZVHkn6xwOxxTHgR26kH/4fHX3VpKt5y4u8rEBFkXWAZtA1gHzNIt3VJ9McmTH5cOT6wDWiawDNoW8A5ZuFkX1ZJI/nfyFuNcn+Ul3/2AG9wswElkHbAp5Byzdvh/9rarPJnlTkkuq6kySv0/ya0nS3R9PcmeSa5KcTvLTJH8+r80CzIusAzaFvANWQXUv7ZQC5zIAe6llb2DGZB2wF1kHbIIDZ90sPvoLAAAAM6OoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKFMVVSr6nhVPVpVp6vq5j1uf3lV3V1VD1TVg1V1zey3CjBfsg7YBLIOWAXV3edeUHVRkseSvDXJmST3J7m+ux/esWYryQPd/bGqOpbkzu5+5T6Pfe4HBjZVLeVBZR2wWLIO2AQHzrpp3lG9Ksnp7n68u59JcnuSE7vWdJIXTX5+cZLvH3RDAEsi64BNIOuAlTBNUb00yRM7Lp+ZXLfTB5K8o6rOJLkzyXv3uqOqurGqTlXVqa2trQNsF2BuZB2wCWQdsBIOzeh+rk9yW3d/uKrekOTTVfWa7v7lzkXdvZXkuSTzERFg1cg6YBPIOmDppnlH9ckkR3ZcPjy5bqcbktyRJN391SQvSHLJLDYIsCCyDtgEsg5YCdMU1fuTHK2qy6rq4iTXJTm5a81/JnlzklTVq7MdaP89y40CzJmsAzaBrANWwr5FtbufTXJTkruSPJLkju5+qKpuqaprJ8v+Jsm7quqbST6b5J29358TBhiIrAM2gawDVsW+X08zRwIP2MtSvrJhjmQdsBdZB2yCuX49DQAAACyMogoAAMBQFFUAAACGoqgCAAAwFEUVAACAoSiqAAAADEVRBQAAYCiKKgAAAENRVAEAABiKogoAAMBQFFUAAACGoqgCAAAwFEUVAACAoSiqAAAADEVRBQAAYCiKKgAAAENRVAEAABiKogoAAMBQFFUAAACGoqgCAAAwFEUVAACAoSiqAAAADEVRBQAAYCiKKgAAAENRVAEAABiKogoAAMBQFFUAAACGoqgCAAAwFEUVAACAoSiqAAAADEVRBQAAYCiKKgAAAENRVAEAABiKogoAAMBQpiqqVXW8qh6tqtNVdfNZ1ry9qh6uqoeq6jOz3SbA/Mk6YBPIOmAVVHefe0HVRUkeS/LWJGeS3J/k+u5+eMeao0nuSPIH3f2jqnppdz+1z2Of+4GBTVVLeVBZByyWrAM2wYGzbpp3VK9Kcrq7H+/uZ5LcnuTErjXvSnJrd/8oSaYIM4DRyDpgE8g6YCVMU1QvTfLEjstnJtftdHmSy6vqK1V1X1Ud3+uOqurGqjpVVae2trYOtmOA+ZB1wCaQdcBKODTD+zma5E1JDie5t6pe290/3rmou7eSPJdkPiICrBpZB2wCWQcs3TTvqD6Z5MiOy4cn1+10JsnJ7v5Fd3832+c+HJ3NFgEWQtYBm0DWASthmqJ6f5KjVXVZVV2c5LokJ3et+bdsv+qWqrok2x8ZeXyG+wSYN1kHbAJZB6yEfYtqdz+b5KYkdyV5JMkd3f1QVd1SVddOlt2V5OmqejjJ3Un+rrufntemAWZN1gGbQNYBq2Lfr6eZI+cyAHtZylc2zJGsA/Yi64BNMNevpwEAAICFUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUKYqqlV1vKoerarTVXXzOda9raq6qq6c3RYBFkPWAZtA1gGrYN+iWlUXJbk1ydVJjiW5vqqO7bHuhUn+OsnXZr1JgHmTdcAmkHXAqpjmHdWrkpzu7se7+5kktyc5sce6Dyb5UJKfzXB/AIsi64BNIOuAlTBNUb00yRM7Lp+ZXPe/quqKJEe6+4vnuqOqurGqTlXVqa2trfPeLMAcyTpgE8g6YCUcutA7qKrnJflIknfut7a7t5I8l2R9oY8NsCiyDtgEsg4YxTTvqD6Z5MiOy4cn1z3nhUlek+SeqvpektcnOenEe2DFyDpgE8g6YCVU97lfAKuqQ0keS/LmbAfZ/Un+pLsfOsv6e5L8bXef2uexvfIG7KWW8qCyDlgsWQdsggNn3b7vqHb3s0luSnJXkkeS3NHdD1XVLVV17UEfGGAksg7YBLIOWBX7vqM6R155A/aylHcZ5kjWAXuRdcAmmN87qgAAALBIiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDUVQBAAAYiqIKAADAUBRVAAAAhqKoAgAAMBRFFQAAgKEoqgAAAAxFUQUAAGAoiioAAABDmaqoVtXxqnq0qk5X1c173P6+qnq4qh6sqi9X1Stmv1WA+ZJ1wCaQdcAq2LeoVtVFSW5NcnWSY0mur6pju5Y9kOTK7v7dJF9I8g+z3ijAPMk6YBPIOmBVTPOO6lVJTnf34939TJLbk5zYuaC77+7un04u3pfk8Gy3CTB3sg7YBLIOWAnTFNVLkzyx4/KZyXVnc0OSL+11Q1XdWFWnqurU1tbW9LsEmD9ZB2wCWQeshEOzvLOqekeSK5O8ca/bu3sryXNJ1rN8bIBFkXXAJpB1wDJNU1SfTHJkx+XDk+t+RVW9Jcn7k7yxu38+m+0BLIysAzaBrANWwjQf/b0/ydGquqyqLk5yXZKTOxdU1euS/HOSa7v7qdlvE2DuZB2wCWQdsBL2Lard/WySm5LcleSRJHd090NVdUtVXTtZ9o9JfiPJ56vqG1V18ix3BzAkWQdsAlkHrIrqXtopBc5lAPZSy97AjMk6YC+yDtgEB866aT76CwAAAAujqAIAADAURRUAAIChKKoAAAAMRVEFAABgKIoqAAAAQ1FUAQAAGIqiCgAAwFAUVQAAAIaiqAIAADAURRUAAIChKKoAAAAMRVEFAABgKIoqAAAAQ1FUAQAAGIqiCgAAwFAUVQAAAIaiqAIAADAURRUAAIChKKoAAAAMRVEFAABgKIoqAAAAQ1FUAQAAGIqiCgAAwFAUVQAAAIaiqAIAADAURRUAAIChKKoAAAAMRVEFAABgKIoqAAAAQ1FUAQAAGIqiCgAAwFAUVQAAAIaiqAIAADAURRUAAIChTFVUq+p4VT1aVaer6uY9bn9+VX1ucvvXquqVs94owLzJOmATyDpgFexbVKvqoiS3Jrk6ybEk11fVsV3Lbkjyo+7+7ST/lORDs94owDzJOmATyDpgVUzzjupVSU539+Pd/UyS25Oc2LXmRJJPTn7+QpI3V1XNbpsAcyfrgE0g64CVME1RvTTJEzsun5lct+ea7n42yU+SvGT3HVXVjVV1anL8a5Jap6Oq/mLZezDPZs20bvNMZroxyyHrNvT3bt3mWceZ1m2eyUyybvBj3X7v1m2edZxp3eaZzHTgrFvoH1Pq7q3uvrK7r0zy6kU+9oIs638687Ju8yTrN9O6zZOswUyybuWs2zzJ+s20bvMkazCTrFs56zZPsn4zrds8yQXMNE1RfTLJkR2XD0+u23NNVR1K8uIkTx90UwBLIOuATSDrgJUwTVG9P8nRqrqsqi5Ocl2Sk7vWnEzyZ5Of/zjJf3R3z26bAHMn64BNIOuAlXBovwXd/WxV3ZTkriQXJflEdz9UVbckOdXdJ5P8S5JPV9XpJD/MdujtZ+sC9j2qdZtp3eZJ1m+mdZsnWdJMsu68rNtM6zZPsn4zrds8iaxbBes207rNk6zfTOs2T3IBM5UXyAAAABjJQv+YEgAAAOxHUQUAAGAocy+qVXW8qh6tqtNVdfMetz+/qj43uf1rVfXKee/pQkwxz/uq6uGqerCqvlxVr1jGPs/HfjPtWPe2quqqunKR+ztf08xTVW+fPE8PVdVnFr3H8zXF793Lq+ruqnpg8rt3zTL2Oa2q+kRVPVVV3z7L7VVVH53M+2BVXbHoPZ4vWSfrFk3WybplkHWybhnWLe9k3ZRZ191zO7J9kv53krwqycVJvpnk2K41f5nk45Ofr0vyuXnuaQHz/H6SX5/8/J6R55l2psm6Fya5N8l9Sa5c9r4v8Dk6muSBJL81ufzSZe97BjNtJXnP5OdjSb637H3vM9PvJbkiybfPcvs1Sb6U7S+Lfn2Sry17zzN4jmTd4DNN1sm6sWeSdeM/R7Ju8Jkm61Yi687jeVqZvJN102fdvN9RvSrJ6e5+vLufSXJ7khO71pxI8snJz19I8uaqqjnv66D2nae77+7un04u3pft7ycb2TTPUZJ8MMmHkvxskZs7gGnmeVeSW7v7R0nS3U8teI/na5qZOsmLJj+/OMn3F7i/89bd92b7L0mezYkkn+pt9yX5zap62WJ2dyCyTtYtmqyTdcsg62TdMqxb3sm6KbNu3kX10iRP7Lh8ZnLdnmu6+9kkP0nykjnv66CmmWenG7L96sHI9p1p8vb8ke7+4iI3dkDTPEeXJ7m8qr5SVfdV1fGF7e5gppnpA0neUVVnktyZ5L2L2drcnO9/a8sm62Tdosk6WbcMsk7WLcO65Z2smzLr9v0eVQ6mqt6R5Mokb1z2Xi5EVT0vyUeSvHPJW5mlQ9n+iMibsv3K6L1V9dru/vFSd3Vhrk9yW3d/uKrekO3vv3tNd/9y2Rtjvcm6ock6mBFZN7x1yztZl/m/o/pkkiM7Lh+eXLfnmqo6lO23t5+e874Oapp5UlVvSfL+JNd2988XtLeD2m+mFyZ5TZJ7qup72f5c+cmBT7yf5jk6k+Rkd/+iu7+b5LFsh9uoppnphiR3JEl3fzXJC5JcspDdzcdU/60NRNbJukWTdbJuGWSdrFuGdcs7WTdt1s35xNpDSR5Pcln+72Th39m15q/yqyfd3zHPPS1gntdl+wTpo8ve76xm2rX+ngx80v2Uz9HxJJ+c/HxJtj+K8JJl7/0CZ/pSkndOfn51ts9lqGXvfZ+5Xpmzn3T/R/nVk+6/vuz9zuA5knWDz7RrvawbcyZZN/5zJOsGn2nX+qGz7jyep5XJO1k3fdYtYtPXZPtVje8kef/kuluy/apUsv0KweeTnE7y9SSvWva/6Auc59+T/FeSb0yOk8ve84XOtGvtKgTafs9RZftjLw8n+VaS65a95xnMdCzJVyZh940kf7jsPe8zz2eT/CDJL7L9KugNSd6d5N07nqNbJ/N+a/TfuSmfI1k3+Ey71sq6MWeSdcufSdYNsO8LmWnX2uGzbsrnaaXyTtZN9ztXk38YAAAAhjDvc1QBAADgvCiqAAAADEVRBQAAYCiKKgAAAENRVAEAABiKogoAAMBQFFUAAACG8j/GqW1HqrhsowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a69e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "spines_off(axes)\n",
    "ax1, ax2, ax3 = axes\n",
    "vlines = [per.update_target_every * i for i in range(1, per.num_episodes//per.update_target_every + 1)]\n",
    "\n",
    "ax1.set_title('Episode rewards')\n",
    "ax1.plot(np.array(per.episode_rewards))\n",
    "\n",
    "\n",
    "ax2.set_title('Estimated Q values')\n",
    "ax2.plot(per.target_net_q_values)\n",
    "ax2.hlines(100, 0, len(per.target_net_q_values), linestyles='dotted', linewidth=0.75)\n",
    "ax2.vlines(vlines, 0, max(per.target_net_q_values), linestyles='dotted', linewidth=0.5, alpha=0.5);\n",
    "\n",
    "ax3.set_title('Temporal difference errors')\n",
    "ax3.plot(np.array(per.td_errors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs: \n",
    "\n",
    "- OpenAI baseline comparison\n",
    "- error and reward clipping!\n",
    "- test the vectorized implementation vs non-vectorized\n",
    "- check # of dead neurons\n",
    "- Look at what gets a high TD error/priority in PER. Can simply sample minibatches from the PriorityMemory to see what comes up. Use state2image to visualize. \n",
    "\n",
    "altho Distributional paper says:\n",
    "\n",
    "\"In this work we used distributions with support bounded in [VMIN,VMAX]. Treating this support as a hyperparameter allows us to change the optimization problem by treating all extremal returns (e.g. greater than VMAX ) as equivalent. Surprisingly, a similar value clipping in DQN significantly degrades per- formance in most games. To take another example: in- terpreting the discount factor Î³ as a proper probability, as some authors have argued, leads to a different algorithm.\" ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to begin presenting my work\n",
    "## Outline\n",
    "- What have I done?\n",
    "    - (Vectorized) DQN\n",
    "    - DDQN\n",
    "    - PER (leave for future update)\n",
    "    - Distributional DQN (part 2)\n",
    "- Why did I do it?\n",
    "    - Amodi/80k, Sutton, Silver, DeepMind, AI Safety\n",
    "- Key insights\n",
    "    - Replay buffer\n",
    "    - Target network\n",
    "    - Reward clipping/error clipping\n",
    "    - \n",
    "- Background material\n",
    "    - Prereqs\n",
    "    - Papers, videos, other tutorials, minimal theory, massimo, jaromiru.com\n",
    "\n",
    "- Verification steps/tests\n",
    "- Questions\n",
    "    - How does Dist DQN interact with mostly deterministic envs?\n",
    "    - Ways to verify/understand what's going on (amidfish)\n",
    "    - Best workflows for developing/prototyping?\n",
    "    - Challenges: time, memory, GPU, slow, data, statistical nature of results.\n",
    "- Proof read\n",
    "    - rename vars\n",
    "    - refactor as needed\n",
    "    - grammer/spelling\n",
    "    - content\n",
    "\n",
    "- Then: figure out styling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
