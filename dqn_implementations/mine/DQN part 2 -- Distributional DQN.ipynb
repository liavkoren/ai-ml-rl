{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro/References\" data-toc-modified-id=\"Intro/References-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro/References</a></span></li><li><span><a href=\"#Scaling-not-clipping?\" data-toc-modified-id=\"Scaling-not-clipping?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scaling not clipping?</a></span></li><li><span><a href=\"#Pytorch\" data-toc-modified-id=\"Pytorch-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pytorch</a></span></li><li><span><a href=\"#Pytorch-monitor\" data-toc-modified-id=\"Pytorch-monitor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pytorch monitor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vectorizing-is-terrible/awesome.\" data-toc-modified-id=\"Vectorizing-is-terrible/awesome.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vectorizing is terrible/awesome.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/References\n",
    "\n",
    "A Distributional Perspective on Reinforcement Learning\n",
    "https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\n",
    "https://arxiv.org/abs/1707.06887\n",
    "\n",
    "Distributional Reinforcement Learning with Quantile Regression\n",
    "https://arxiv.org/abs/1710.10044\n",
    "\n",
    "Distributional RL\n",
    "https://mtomassoli.github.io/2017/12/08/distributional_rl/\n",
    "\n",
    "An Analysis of Categorical Distributional Reinforcement Learning\n",
    "https://arxiv.org/abs/1802.08163\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Scaling not clipping?\n",
    "\n",
    "# Pytorch\n",
    "# Pytorch monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T21:46:08.807465Z",
     "start_time": "2018-07-19T21:46:08.766220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import math\n",
    "import random\n",
    "\n",
    "import attr\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_monitor import init_experiment, monitor_module\n",
    "# from smooth import smooth  # timeseries smoothing function\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cartpole = gym.make('CartPole-v1')\n",
    "lunarlander = gym.make('LunarLander-v2')\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T12:57:30.206978Z",
     "start_time": "2018-07-20T12:57:29.541715Z"
    },
    "code_folding": [
     1,
     22
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Memory(deque):\n",
    "    \"\"\" Experience Replay Memory class. \"\"\"\n",
    "    size = attr.ib()\n",
    "    minibatch_size = attr.ib()\n",
    "\n",
    "    def append(self, thing):\n",
    "        if len(self) > self.size - 1:\n",
    "            self.popleft()\n",
    "        return super().append(thing)\n",
    "\n",
    "    def sample(self):\n",
    "        batch_size = min(len(self), self.minibatch_size)\n",
    "        data = random.sample(self, batch_size)\n",
    "        states = torch.stack([record[0] for record in data])\n",
    "        actions = torch.tensor([record[1] for record in data], dtype=torch.long)\n",
    "        rewards = torch.tensor([record[2] for record in data], dtype=torch.float)\n",
    "        states_ = torch.stack([record[3] for record in data])\n",
    "        dones = torch.tensor([record[4] for record in data], dtype=torch.long)\n",
    "        return (states, actions, rewards, states_, dones)\n",
    "\n",
    "\n",
    "class ValueDistributionDeep(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64, num_hidden2_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.linear2 = nn.Linear(num_hidden1_units, num_hidden2_units)\n",
    "        self.linear3 = nn.Linear(num_hidden2_units, num_hidden2_units)\n",
    "        self.linear4 = nn.Linear(num_hidden2_units, self.action_shape * self.num_atoms)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        x1 = F.selu(self.linear1(x))\n",
    "        x2 = F.selu(self.linear2(x1))\n",
    "        x3 = F.selu(self.linear3(x2))\n",
    "        x4 = self.linear4(x3).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x4, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x1', x1, track_data=True, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=True, track_grad=True)\n",
    "            self.monitor('x3', x3, track_data=True, track_grad=True)\n",
    "            self.monitor('x4', x4, track_data=True, track_grad=True)\n",
    "            self.monitor('out', out, track_data=True, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "class ValueDistribution(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64, num_hidden2_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.linear2 = nn.Linear(num_hidden1_units, self.action_shape * self.num_atoms)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        x1 = F.relu(self.linear1(x))\n",
    "        x2 = self.linear2(x1).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x2, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x1', x1, track_data=True, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=True, track_grad=True)\n",
    "            self.monitor('out', out, track_data=True, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T19:19:40.302183Z",
     "start_time": "2018-07-19T19:19:40.113459Z"
    }
   },
   "source": [
    "## Vectorizing is terrible/awesome.\n",
    "\n",
    "This is the algo we'll be implementing. \n",
    "\n",
    "![The C51 algorithm](assets/C51-algo.png)\n",
    "\n",
    "Vectorizing this code is *very* important. Even on my lowely macbook, the fully vectorized version of this algorithm that accepts a minibatch runs about *30 times* faster than a naive implementation that's called inside a loop. This cashes out to 1000 training episodes of LunarLander in about 10 minutes, verses five hours. \n",
    "\n",
    "My process for vectoring this code was.. to sort of squint at it. \n",
    "Seriously. I wasn't even sure if I should first generalize it to accept minibatch tensors and then remove the loop, or vice versa. \n",
    "\n",
    "Squinting at it, thought, (and stepping through it line by line in Jupyter a few times), it became clear that the would actually tricky to vectorize: \n",
    "\n",
    "![The bastard lines]](assets/C51-algo-large.png)\n",
    "\n",
    "I decided to take the easy wins first, and first converted the `categorical_loss` function to accept minibatches first. This is straighforward, mostly just reshaping and expanding tensors. Pytorch's `squeeze` and `unsqueeze` methods have fun names and are great for this. \n",
    "\n",
    "Those two lines, though, were bloody horrible. \n",
    "\n",
    "They ended up cashing out into the following dense six lines of python:\n",
    "\n",
    "```python\n",
    "offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "m += lo_component.resize_as(m) + hi_component.resize_as(m)       \n",
    "```\n",
    "\n",
    "The main insight is that the `lo` and `hi` tensors contain routing information. They tend look like this:\n",
    "\n",
    "```\n",
    "lo:\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 10],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]])\n",
    "hi:\n",
    "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 4, 5, 6, 7, 7, 8, 9, 10, 10, 10],\n",
    "        [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10]])\n",
    "```\n",
    "\n",
    "This is for a three-transition test minibatch. The values are the target indicies for where probability needs to accumulate inside `m` our new probability-mass tensor.\n",
    "\n",
    "Eventually after squinting a lot at the PyTorch docs, I figured out that PyTorch's [`index_add`](https://pytorch.org/docs/stable/tensors.html?highlight=index_add#torch.Tensor.index_add_) method would do the trick. \n",
    "\n",
    "Usings `index_add` requires that all the tensors be unrolled, which is why we need index-offsets. Put it together, and you're done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T12:57:33.601401Z",
     "start_time": "2018-07-20T12:57:33.472565Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def categorical_vectorized_loss(online_net, target_net, transitions, discount): \n",
    "    states, actions, rewards, states_, dones = transitions\n",
    "    not_dones = (1 - dones).type(torch.FloatTensor)\n",
    "    atoms = target_net.atoms\n",
    "    probabilities = target_net.forward(states_)\n",
    "    Q_x_ = (probabilities * atoms).sum(2)\n",
    "    batch_size = states.shape[0]\n",
    "    assert Q_x_.shape == torch.Size((batch_size, target_net.action_shape)), f'Got: {Q_x_.shape}, expected: {(batch_size, target_net.action_shape)}'\n",
    "    a_star = Q_x_.argmax(dim=1) \n",
    "    assert a_star.shape == torch.Size((batch_size,)), f'Got {a_star.shape}, expected: ((batch_size,))'\n",
    "    \n",
    "    # compute the projected probability:\n",
    "    delta_z = (target_net.vmax - target_net.vmin)/(target_net.num_atoms - 1)    \n",
    "    # select only the probabilities distributions for the a_star actions:\n",
    "    probabilities = probabilities[range(batch_size), a_star]\n",
    "    T_zj = rewards.unsqueeze(1) + discount * atoms * not_dones.unsqueeze(1)\n",
    "    b_j = (T_zj.clamp(target_net.vmin, target_net.vmax) - target_net.vmin) / delta_z  # correct    \n",
    "    lo = b_j.floor()        \n",
    "    hi = b_j.ceil()\n",
    "    m = torch.zeros(batch_size, target_net.num_atoms, dtype=torch.float)\n",
    "    lo_component = torch.zeros_like(m.view(-1))\n",
    "    hi_component = torch.zeros_like(m.view(-1))\n",
    "    # offset will be used for indexing when we flatten the tensors into vectors:\n",
    "    offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "    idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "    lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "    hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "    lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "    hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "    m += lo_component.reshape(batch_size, target_net.num_atoms) + hi_component.reshape(batch_size, target_net.num_atoms)\n",
    "    # cross enthropy is Sigma <true> log <unnatural>, so for us is: target log(online)\n",
    "    online_distribution = online_net.forward(states)[range(batch_size), actions]\n",
    "    return -( m * online_distribution.log() ).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T12:57:35.621063Z",
     "start_time": "2018-07-20T12:57:34.985146Z"
    },
    "code_folding": [
     101
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class CategoricalAgent:\n",
    "    env = attr.ib()\n",
    "    discount = attr.ib(default=0.99)\n",
    "    epsilon_max = attr.ib(default=1.0)\n",
    "    epsilon_min = attr.ib(default=0.01)\n",
    "    annealing_const = attr.ib(default=.001)  # aka Lambda\n",
    "    minibatch_size = attr.ib(default=32)\n",
    "    memory_size = attr.ib(default=int(1e6))\n",
    "    num_episodes = attr.ib(default=1000)  # num of episodes in a training epoch\n",
    "    render_every = attr.ib(default=20)  # set to zero to turn off rendering\n",
    "    update_target_every = attr.ib(default=200)\n",
    "    vmin = attr.ib(default=-10)\n",
    "    vmax = attr.ib(default=10)\n",
    "    num_atoms = attr.ib(default=51)\n",
    "    learning_rate = attr.ib(default=0.000001)\n",
    "    monitor_total = attr.ib(default=10)\n",
    "    logger = attr.ib(default=None)\n",
    "    reward_scaling = attr.ib(default=1.0)\n",
    "    xavier = attr.ib(default=True)\n",
    "    weight_decay = attr.ib(default=0)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        self.steps = 0\n",
    "        state_shape = self.env.observation_space.shape[0]\n",
    "        self.memory = Memory(self.memory_size, self.minibatch_size)\n",
    "        self.action_shape = self.env.action_space.n\n",
    "        self.online_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        self.target_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        if self.xavier:\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "            for param in self.online_net.parameters():\n",
    "                if param.dim() < 2:\n",
    "                    continue\n",
    "                nn.init.xavier_uniform_(param, gain=gain)        \n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.steps = 0\n",
    "        self.target_net_q_values = []\n",
    "        self.episode_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.monitor_every = self.num_episodes//self.monitor_total\n",
    "\n",
    "        \n",
    "        \n",
    "    def render(self, episode):\n",
    "        if self.render_every and episode % self.render_every == 0:\n",
    "            self.env.render()\n",
    "\n",
    "    def training_progress_report(self, episode):\n",
    "        last_ep = self.episode_rewards[-1]\n",
    "        ten_ep_mean = sum(self.episode_rewards[-10:])/len(self.episode_rewards[-10:])\n",
    "        hundred_ep_mean = sum(self.episode_rewards[-100:])/len(self.episode_rewards[-100:])\n",
    "        return f'Ep: {episode} // steps: {self.steps} // last ep reward: {last_ep:.2f} // {min(10, len(self.episode_rewards[-10:]))}-ep mean: {ten_ep_mean:.2f} // {min(100, len(self.episode_rewards[-100:]))}-ep mean: {hundred_ep_mean:.2f}'\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.sample()\n",
    "        loss = categorical_vectorized_loss(self.online_net, self.target_net, batch, self.discount)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()/self.minibatch_size\n",
    "\n",
    "    def monitor(self):\n",
    "        if not hasattr(self.target_net, 'monitoring'):\n",
    "            return\n",
    "        if self.monitor_every and self.steps % self.monitor_every == 0:\n",
    "            self.target_net.monitoring(True)\n",
    "        else:\n",
    "            self.target_net.monitoring(False)\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_loss = 0\n",
    "            state = torch.tensor(self.env.reset(), dtype=torch.float)\n",
    "            self.target_net_q_values.append(self.target_net.predict_action_values(state).max().item())\n",
    "            if self.steps == 0 and self.logger:\n",
    "                self.logger.add_graph(self.target_net, state)\n",
    "            self.logger.add_scalar('Target net Q values', self.target_net_q_values[-1], episode)                \n",
    "            while not episode_done:\n",
    "                self.monitor()\n",
    "                epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * math.exp(-self.annealing_const * self.steps)\n",
    "                self.steps += 1                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, self.action_shape-1)\n",
    "                else:\n",
    "                    action = self.online_net.get_action(state).item()\n",
    "                self.render(episode)\n",
    "                self.monitor()\n",
    "                state_, reward, episode_done, _ = self.env.step(action)\n",
    "                reward *= self.reward_scaling\n",
    "                state_ = torch.tensor(state_, dtype=torch.float)\n",
    "                episode_reward += reward\n",
    "                self.memory.append((state, action, reward, state_, episode_done))\n",
    "                state = state_\n",
    "                if self.steps < 2:\n",
    "                    continue\n",
    "                episode_loss += self.replay()\n",
    "\n",
    "                if self.steps % self.update_target_every == 0:\n",
    "                    self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "                if episode_done:\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.training_loss.append(episode_loss)\n",
    "                    print(self.training_progress_report(episode), end='\\r', flush=True)\n",
    "                    self.logger.add_scalar('train loss', episode_loss, episode)\n",
    "                    self.logger.add_scalar('episode reward', episode_reward, episode)                    \n",
    "        self.env.close()\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T13:06:43.541629Z",
     "start_time": "2018-07-20T13:02:45.522467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_dir': 'tuning-categorical',\n",
       " 'random_seed': 0,\n",
       " 'run_dir': 'tuning-categorical/Jul-20-18@09:02:45-DaydreamNation.local',\n",
       " 'run_name': 'Jul-20-18@09:02:45-DaydreamNation.local',\n",
       " 'tag': 'Experiment Config: Test Monitor :: Jul-20-18@09:02:45\\n',\n",
       " 'title': 'Test Monitor'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liavkoren/Envs/ai-gym/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator reshape because torch.onnx.symbolic.reshape does not exist\n",
      "  .format(op_name, op_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 295 // steps: 24338 // last ep reward: -127.53 // 10-ep mean: -124.19 // 100-ep mean: -124.09\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-03ceb1568d85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                track_update_ratio=True)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-5a4c2fe3a5fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mepisode_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-5a4c2fe3a5fe>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_vectorized_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Enable monitoring\n",
    "\n",
    "config = {\n",
    "    'title':'Test Monitor',\n",
    "    'log_dir':'tuning-categorical',\n",
    "    'random_seed':0\n",
    "}\n",
    "logger, config = init_experiment(config)\n",
    "config\n",
    "agent = CategoricalAgent(\n",
    "    lunarlander, \n",
    "    learning_rate=0.0001, \n",
    "    num_episodes=1000,\n",
    "    update_target_every=100,  \n",
    "    reward_scaling=.3,\n",
    "    logger=logger,\n",
    "    weight_decay=0.1,\n",
    "    xavier=True\n",
    ")\n",
    "monitor_module(agent.target_net, logger, \n",
    "               track_data=True,\n",
    "               track_grad=True,\n",
    "               track_update=True,\n",
    "               track_update_ratio=True)\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T13:07:24.319604Z",
     "start_time": "2018-07-20T13:07:24.300995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1398e-01, -8.2857e-01,  5.9383e-01,  4.0346e-01, -3.7418e-01,\n",
       "         -1.2082e-01,  2.0179e-03,  3.7760e-03],\n",
       "        [-1.3022e+00, -5.8852e+00, -4.9629e+00,  6.2778e+00,  3.2038e+00,\n",
       "          2.8829e+00,  1.0750e-01,  5.5886e-03],\n",
       "        [-8.3385e-01, -9.5882e-01, -2.9007e+00,  1.6320e+00,  1.4147e+00,\n",
       "          1.3444e+00,  2.6426e-02, -4.2750e-04],\n",
       "        [ 1.1271e-02, -1.8971e-01,  1.0665e-01, -1.6903e-02, -2.9136e-02,\n",
       "         -6.1655e-02,  1.0358e-04, -2.0072e-06],\n",
       "        [-4.6333e-01, -1.0190e-01, -1.2102e+00,  3.3122e+00,  5.3648e+00,\n",
       "          3.3791e+00, -9.7408e-02, -6.5690e-02],\n",
       "        [-1.1823e+00, -4.0934e+00, -3.7119e+00,  4.8772e+00,  7.7181e-01,\n",
       "          1.5361e+00, -8.8831e-03,  1.2926e-01],\n",
       "        [ 8.9995e-01,  6.7709e+00,  3.1788e+00, -8.5975e+00, -6.5749e+00,\n",
       "         -5.5166e+00,  2.0583e-01,  1.1149e-03],\n",
       "        [-3.4027e-01, -4.3643e+00, -2.2935e+00,  1.8236e+00,  4.5032e-01,\n",
       "          7.4433e-01,  2.3704e-03,  4.4034e-03],\n",
       "        [-5.7308e-01, -2.7521e+00, -2.2157e+00,  1.9209e+00,  5.4586e-01,\n",
       "          6.3628e-01,  1.1329e-04, -3.5219e-06],\n",
       "        [ 1.0878e-01, -5.2209e-01,  2.2576e-01, -7.4467e-02, -3.9908e-02,\n",
       "         -2.6405e-02, -1.8532e-02, -6.4060e-04],\n",
       "        [ 4.6986e-01, -1.0847e+00,  9.3698e-01,  3.8638e-01, -4.4489e-01,\n",
       "         -2.2079e-01,  9.9353e-08,  1.1028e-02],\n",
       "        [ 1.8497e-02,  1.0829e-02, -9.2957e-04, -5.4461e-02,  3.9086e-02,\n",
       "          1.9114e-01,  5.4098e-02,  5.4012e-02],\n",
       "        [ 2.5476e-01,  1.1259e-01,  5.6697e-01, -1.1821e+00, -1.2442e+00,\n",
       "         -5.4963e-01,  1.4536e-01,  1.3570e-01],\n",
       "        [-1.4575e-01, -2.1697e+00, -1.2462e+00,  5.1779e-01,  2.7947e-01,\n",
       "          4.4717e-01,  7.8230e-05,  8.4148e-05],\n",
       "        [-3.1607e-01, -1.0508e+00, -1.1596e+00,  2.3085e+00,  3.3146e+00,\n",
       "          1.8587e+00,  1.5273e-01,  2.9660e-02],\n",
       "        [-1.2876e+00, -2.8631e+00, -3.3939e+00,  5.1045e+00,  3.6382e+00,\n",
       "          2.3654e+00, -9.3101e-02, -6.6957e-02],\n",
       "        [ 7.0354e-01,  4.3077e-01,  1.5466e+00, -2.3896e+00, -3.4413e+00,\n",
       "         -1.5153e+00, -4.6426e-04,  4.7838e-04],\n",
       "        [-1.3055e-02,  1.8621e-02, -2.4495e-02, -6.1333e-02,  6.7362e-02,\n",
       "          5.0595e-02,  8.6686e-05,  2.7816e-05],\n",
       "        [ 6.9758e-01,  6.7122e-01,  1.5968e+00, -1.3796e+00, -1.6315e-01,\n",
       "         -1.6695e-01, -5.2138e-02, -6.9156e-02],\n",
       "        [ 1.3475e+00,  3.9489e+00,  3.9924e+00, -5.5769e+00, -3.0487e+00,\n",
       "         -2.3544e+00, -1.6370e-01, -7.1739e-02],\n",
       "        [ 1.3057e-01,  3.9145e-02,  2.7138e-01, -3.2091e-01, -4.4272e-01,\n",
       "         -1.7163e-01,  3.4383e-04,  3.1214e-05],\n",
       "        [ 2.1317e-01,  6.2868e-02,  5.0731e-01, -7.4373e-01, -9.9787e-01,\n",
       "         -1.1924e+00,  6.3565e-02,  4.9963e-03],\n",
       "        [ 2.3259e-01, -4.4075e-01,  4.1511e-01,  2.1707e-01, -2.8931e-01,\n",
       "         -9.6879e-02, -4.1248e-07,  2.4699e-05],\n",
       "        [-4.0689e+00, -8.6513e+00, -1.1303e+01,  1.3354e+01,  7.2170e+00,\n",
       "          5.9751e+00,  1.1960e-01,  2.2193e-02],\n",
       "        [-4.2878e-01,  9.4365e-01, -7.5305e-01, -8.4243e-01, -4.8783e-01,\n",
       "         -2.2776e-01,  2.4288e-03,  4.6497e-04],\n",
       "        [-1.9593e+00, -4.1918e+00, -5.4686e+00,  7.5582e+00,  4.0037e+00,\n",
       "          3.3793e+00,  4.3393e-03, -8.6126e-04],\n",
       "        [ 9.5806e-01, -3.2511e+00,  1.1741e+00,  3.2375e+00,  1.7532e+00,\n",
       "          1.3593e+00,  9.5074e-03,  9.8297e-03],\n",
       "        [-3.4852e-01, -4.1316e-02, -1.0244e+00,  2.9269e-01,  7.4124e-01,\n",
       "          5.9810e-01, -1.6737e-02,  2.8170e-02],\n",
       "        [ 3.0977e-02, -1.1998e-03,  3.9974e-02, -1.6734e-01,  9.2600e-02,\n",
       "          5.2226e-02,  1.7431e-04,  6.3917e-03],\n",
       "        [ 1.0435e+00,  3.0162e+00,  3.1064e+00, -3.7277e+00, -5.0309e+00,\n",
       "         -3.6736e+00,  1.0421e-01, -2.2777e-06],\n",
       "        [-4.9721e-01, -1.1014e+00, -1.4966e+00,  2.3224e+00,  2.8446e+00,\n",
       "          2.4770e+00,  9.6966e-02,  1.2126e-01],\n",
       "        [-1.1842e-01,  8.2578e-02, -1.5970e-01, -1.0705e-01, -7.3674e-03,\n",
       "          9.8507e-04,  1.0631e-02,  4.1539e-03],\n",
       "        [ 1.0636e-01, -1.0961e+00,  1.7555e-02, -2.5378e-01, -9.6100e-01,\n",
       "         -6.7849e-01,  1.3676e-01,  1.0326e-01],\n",
       "        [ 2.8609e+00,  5.8831e+00,  7.6523e+00, -7.4652e+00, -2.3970e+00,\n",
       "         -2.7882e+00, -1.5652e-02, -5.3996e-02],\n",
       "        [-5.2261e-01,  7.0994e-01, -8.4417e-01, -3.0234e-01,  1.1599e+00,\n",
       "          4.3552e-01,  2.5644e-03,  9.0597e-03],\n",
       "        [ 7.9224e-01,  1.8784e+00,  2.4257e+00, -3.4237e+00, -2.6572e+00,\n",
       "         -1.9760e+00,  5.6701e-02,  4.1817e-05],\n",
       "        [-6.8843e-02,  1.4998e-01, -1.4532e-01, -1.7371e-02,  1.4343e-01,\n",
       "          6.1579e-02,  4.5110e-04, -1.4389e-03],\n",
       "        [-4.2155e-01, -1.8719e+00, -1.3599e+00,  3.4383e+00,  1.4269e+00,\n",
       "          1.5565e+00, -1.3312e-01, -1.4135e-01],\n",
       "        [ 9.3904e-04, -1.1496e-02,  8.9247e-03, -1.3386e-02,  7.0239e-03,\n",
       "         -1.9837e-03,  2.0457e-03,  3.7252e-03],\n",
       "        [ 4.3676e-02,  2.1509e-01,  2.1986e-01, -5.9333e-02,  3.6875e-01,\n",
       "         -9.3037e-03,  9.4005e-03,  9.5401e-03],\n",
       "        [-1.7766e+00, -4.1040e+00, -4.7304e+00,  3.9253e+00,  1.1607e+00,\n",
       "          1.0059e+00,  7.8469e-02,  4.2342e-02],\n",
       "        [ 3.2780e-01, -3.6161e+00,  7.1353e-02,  2.4019e+00,  1.3926e-01,\n",
       "          6.9270e-01,  5.5043e-02,  7.4404e-03],\n",
       "        [ 2.0438e-01,  2.7077e-01,  4.5392e-01,  7.8799e-01,  8.7230e-01,\n",
       "          7.3786e-01, -9.9351e-02, -1.0877e-01],\n",
       "        [-6.4340e-01,  2.9985e+00, -8.7430e-01, -2.7267e+00, -2.3799e-01,\n",
       "         -4.5922e-01,  2.2397e-01,  5.6120e-04],\n",
       "        [-2.2826e-01, -8.9185e-02, -7.4256e-01,  4.7799e-01,  7.2238e-01,\n",
       "          1.2219e+00, -1.3864e-01, -5.7443e-02],\n",
       "        [-1.0288e+00,  2.3496e+00, -1.9054e+00, -7.5869e-02,  6.8861e-01,\n",
       "          2.2010e-01,  1.5420e-03, -7.3801e-03],\n",
       "        [-2.1420e-01, -1.0362e-01, -8.6435e-01,  3.2253e-01,  1.3043e-01,\n",
       "         -1.2108e-01,  5.6725e-02,  9.0198e-05],\n",
       "        [ 3.8837e-01,  9.2690e-01,  1.1955e+00, -1.1693e+00, -1.4627e+00,\n",
       "         -1.3783e+00,  2.2529e-02,  2.0982e-03],\n",
       "        [ 7.6802e-01,  7.7352e-01,  1.9113e+00, -2.2936e+00,  1.2493e-01,\n",
       "         -1.1453e+00,  1.1055e-01, -5.9651e-02],\n",
       "        [ 1.3002e+00,  3.0777e+00,  2.4265e+00, -4.0322e+00,  7.0181e-02,\n",
       "         -1.2576e-01,  9.0147e-02,  6.5967e-02],\n",
       "        [ 2.0892e-01,  4.6533e-01,  4.0756e-01, -8.6146e-01,  1.7436e-01,\n",
       "          8.2047e-02,  9.4618e-02,  8.4119e-02],\n",
       "        [ 3.3467e-01,  1.4630e+00,  7.7506e-01, -1.9731e+00, -4.3710e-01,\n",
       "         -2.0662e-01,  8.1787e-02,  1.7341e-02],\n",
       "        [ 4.9895e-01, -3.2599e+00,  5.4871e-02,  3.5192e+00,  2.2497e+00,\n",
       "          1.8033e+00,  2.5253e-02,  2.5024e-02],\n",
       "        [ 8.0421e-01,  1.0486e+00,  2.1779e+00, -1.3487e+00, -3.3869e-01,\n",
       "         -4.3812e-01, -2.0869e-01, -1.6797e-01],\n",
       "        [ 2.1033e-01,  1.6800e-01,  4.1698e-01, -4.0958e-01,  7.1438e-02,\n",
       "          1.2984e-01,  1.5101e-01,  6.8279e-02],\n",
       "        [ 3.6456e-01, -1.1845e+00,  2.8829e-02, -7.0286e-01, -2.1267e+00,\n",
       "         -1.2739e+00,  2.0954e-01,  4.5617e-02],\n",
       "        [ 3.0262e+00,  2.9787e+00,  8.3989e+00, -7.5783e+00, -4.1432e+00,\n",
       "         -4.1259e+00, -1.2542e-01, -3.6910e-02],\n",
       "        [ 1.4856e+00,  1.1890e+00,  3.1112e+00, -1.8275e+00, -1.3845e-01,\n",
       "         -6.0126e-01,  8.5574e-03, -1.7623e-06],\n",
       "        [ 1.7379e+00,  6.1363e+00,  5.1916e+00, -6.0706e+00, -3.7550e+00,\n",
       "         -3.4181e+00, -1.1360e-02, -6.1607e-02],\n",
       "        [ 4.4449e-01,  1.0864e+00,  1.2222e+00, -2.3986e+00, -1.5035e+00,\n",
       "         -1.0359e+00,  6.4580e-02, -3.2781e-02],\n",
       "        [-1.0238e+00, -5.8721e-01, -2.3465e+00,  2.1852e+00,  2.2700e+00,\n",
       "          1.1154e+00,  1.6075e-01,  3.5013e-02],\n",
       "        [ 6.0884e-01, -1.8562e+00,  1.1957e+00,  4.8691e-01, -3.7440e-01,\n",
       "         -2.8749e-02, -9.6427e-03,  3.0378e-04],\n",
       "        [ 1.9934e+00,  2.9263e+00,  5.1724e+00, -8.1110e+00, -9.2824e+00,\n",
       "         -5.9933e+00, -3.3593e-02, -7.0368e-02],\n",
       "        [-8.6251e-01, -2.7323e+00, -2.1335e+00,  3.3581e+00,  1.5033e+00,\n",
       "          7.8530e-01, -8.6111e-02, -3.1677e-02]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.target_net.linear1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T02:45:28.547085Z",
     "start_time": "2018-07-20T02:45:28.291122Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(smooth(np.array(agent.target_net_q_values)))\n",
    "# plt.plot(smooth(np.array(agent.episode_rewards)))\n",
    "plt.plot(smooth(np.array(agent.training_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T21:27:58.741267Z",
     "start_time": "2018-07-19T21:27:58.636187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        ...,\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.target_net.linear4.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "Getting this working took a *lot* of debugging time. I went fairly far down a blind alley \n",
    "\n",
    "    - not the exploding gradients: small step size was a blind alley\n",
    "    - not the layer init\n",
    "    - Debugging w. three-armed bandit\n",
    "    - comparing against other implementations?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "266px",
    "width": "268px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
