{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro/References\" data-toc-modified-id=\"Intro/References-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro/References</a></span></li><li><span><a href=\"#Scaling-not-clipping?\" data-toc-modified-id=\"Scaling-not-clipping?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scaling not clipping?</a></span></li><li><span><a href=\"#Pytorch\" data-toc-modified-id=\"Pytorch-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pytorch</a></span></li><li><span><a href=\"#Pytorch-monitor\" data-toc-modified-id=\"Pytorch-monitor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pytorch monitor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vectorizing-is-terrible/awesome.\" data-toc-modified-id=\"Vectorizing-is-terrible/awesome.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vectorizing is terrible/awesome.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/References\n",
    "\n",
    "A Distributional Perspective on Reinforcement Learning\n",
    "https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\n",
    "https://arxiv.org/abs/1707.06887\n",
    "\n",
    "Distributional Reinforcement Learning with Quantile Regression\n",
    "https://arxiv.org/abs/1710.10044\n",
    "\n",
    "Distributional RL\n",
    "https://mtomassoli.github.io/2017/12/08/distributional_rl/\n",
    "\n",
    "An Analysis of Categorical Distributional Reinforcement Learning\n",
    "https://arxiv.org/abs/1802.08163\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Scaling not clipping?\n",
    "\n",
    "# Pytorch\n",
    "# Pytorch monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:50:00.189869Z",
     "start_time": "2018-08-01T16:49:59.159782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import (deque, defaultdict)\n",
    "from functools import partial\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import attr\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_monitor import init_experiment, monitor_module\n",
    "from running_stats import RunningStats\n",
    "from smooth import smooth  # timeseries smoothing function\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cartpole = gym.make('CartPole-v1')\n",
    "lunarlander = gym.make('LunarLander-v2')\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:50:02.620920Z",
     "start_time": "2018-08-01T16:50:02.362162Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Memory(deque):\n",
    "    \"\"\" Experience Replay Memory class. \"\"\"\n",
    "    size = attr.ib()\n",
    "    minibatch_size = attr.ib()\n",
    "\n",
    "    def append(self, thing):\n",
    "        if len(self) > self.size - 1:\n",
    "            self.popleft()\n",
    "        return super().append(thing)\n",
    "\n",
    "    def sample(self):\n",
    "        batch_size = min(len(self), self.minibatch_size)\n",
    "        data = random.sample(self, batch_size)\n",
    "        states = torch.stack([record[0] for record in data])\n",
    "        actions = torch.tensor([record[1] for record in data], dtype=torch.long)\n",
    "        rewards = torch.tensor([record[2] for record in data], dtype=torch.float)\n",
    "        states_ = torch.stack([record[3] for record in data])\n",
    "        dones = torch.tensor([record[4] for record in data], dtype=torch.long)\n",
    "        return (states, actions, rewards, states_, dones)\n",
    "\n",
    "    \n",
    "class ValueDistribution(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.linear1 = nn.utils.weight_norm(nn.Linear(self.state_shape, num_hidden1_units))\n",
    "        self.linear2 = nn.utils.weight_norm(nn.Linear(num_hidden1_units, self.action_shape * self.num_atoms))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        x1 = F.leaky_relu(self.linear1(x))\n",
    "        x2 = self.linear2(x1).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x2, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x1', x1, track_data=False, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=False, track_grad=True)\n",
    "            self.monitor('out', out, track_data=False, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T19:19:40.302183Z",
     "start_time": "2018-07-19T19:19:40.113459Z"
    }
   },
   "source": [
    "## Vectorizing is terrible/awesome.\n",
    "\n",
    "This is the algo we'll be implementing. \n",
    "\n",
    "![The C51 algorithm](assets/C51-algo.png)\n",
    "\n",
    "Vectorizing this code is *very* important. Even on my lowely macbook, the fully vectorized version of this algorithm that accepts a minibatch runs about *30 times* faster than a naive implementation that's called inside a loop. This cashes out to 1000 training episodes of LunarLander in about 10 minutes, verses five hours. \n",
    "\n",
    "My process for vectoring this code was.. to sort of squint at it. \n",
    "Seriously. I wasn't even sure if I should first generalize it to accept minibatch tensors and then remove the loop, or vice versa. \n",
    "\n",
    "Squinting at it, thought, (and stepping through it line by line in Jupyter a few times), it became clear that the would actually tricky to vectorize: \n",
    "\n",
    "![The bastard lines]](assets/C51-algo-large.png)\n",
    "\n",
    "I decided to take the easy wins first, and first converted the `categorical_loss` function to accept minibatches first. This is straighforward, mostly just reshaping and expanding tensors. Pytorch's `squeeze` and `unsqueeze` methods have fun names and are great for this. \n",
    "\n",
    "Those two lines, though, were bloody horrible. \n",
    "\n",
    "They ended up cashing out into the following dense six lines of python:\n",
    "\n",
    "```python\n",
    "offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "m += lo_component.resize_as(m) + hi_component.resize_as(m)       \n",
    "```\n",
    "\n",
    "The main insight is that the `lo` and `hi` tensors contain routing information. They tend look like this:\n",
    "\n",
    "```\n",
    "lo:\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 10],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]])\n",
    "hi:\n",
    "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 4, 5, 6, 7, 7, 8, 9, 10, 10, 10],\n",
    "        [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10]])\n",
    "```\n",
    "\n",
    "This is for a three-transition test minibatch. The values are the target indicies for where probability needs to accumulate inside `m` our new probability-mass tensor.\n",
    "\n",
    "Eventually after squinting a lot at the PyTorch docs, I figured out that PyTorch's [`index_add`](https://pytorch.org/docs/stable/tensors.html?highlight=index_add#torch.Tensor.index_add_) method would do the trick. \n",
    "\n",
    "Usings `index_add` requires that all the tensors be unrolled, which is why we need index-offsets. Put it together, and you're done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:50:06.902032Z",
     "start_time": "2018-08-01T16:50:06.793653Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def categorical_vectorized_loss(online_net, target_net, transitions, discount): \n",
    "    states, actions, rewards, states_, dones = transitions\n",
    "    not_dones = (1 - dones).type(torch.FloatTensor)\n",
    "    atoms = target_net.atoms\n",
    "    probabilities = target_net.forward(states_)\n",
    "    Q_x_ = (probabilities * atoms).sum(2)\n",
    "    batch_size = states.shape[0]\n",
    "    assert Q_x_.shape == torch.Size((batch_size, target_net.action_shape)), f'Got: {Q_x_.shape}, expected: {(batch_size, target_net.action_shape)}'\n",
    "    a_star = Q_x_.argmax(dim=1) \n",
    "    assert a_star.shape == torch.Size((batch_size,)), f'Got {a_star.shape}, expected: ((batch_size,))'\n",
    "    \n",
    "    # compute the projected probability:\n",
    "    delta_z = (target_net.vmax - target_net.vmin)/(target_net.num_atoms - 1)    \n",
    "    # select only the probabilities distributions for the a_star actions:\n",
    "    probabilities = probabilities[range(batch_size), a_star]\n",
    "    T_zj = rewards.unsqueeze(1) + discount * atoms * not_dones.unsqueeze(1)\n",
    "    b_j = (T_zj.clamp(target_net.vmin, target_net.vmax) - target_net.vmin) / delta_z  # correct    \n",
    "    lo = b_j.floor()        \n",
    "    hi = b_j.ceil()\n",
    "    m = torch.zeros(batch_size, target_net.num_atoms, dtype=torch.float)\n",
    "    lo_component = torch.zeros_like(m.view(-1))\n",
    "    hi_component = torch.zeros_like(m.view(-1))\n",
    "    # offset will be used for indexing when we flatten the tensors into vectors:\n",
    "    offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "    idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "    lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "    hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "    lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "    hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "    m += lo_component.reshape(batch_size, target_net.num_atoms) + hi_component.reshape(batch_size, target_net.num_atoms)\n",
    "    # cross enthropy is Sigma <true> log <unnatural>, so for us is: target log(online)\n",
    "    online_distribution = online_net.forward(states)[range(batch_size), actions]\n",
    "    return -( m * online_distribution.log() ).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:50:33.285011Z",
     "start_time": "2018-08-01T16:50:31.703171Z"
    },
    "code_folding": [
     103
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class CategoricalAgent:\n",
    "    env = attr.ib()\n",
    "    discount = attr.ib(default=0.99)\n",
    "    epsilon_max = attr.ib(default=1.0)\n",
    "    epsilon_min = attr.ib(default=0.01)\n",
    "    annealing_const = attr.ib(default=.001)  # aka Lambda\n",
    "    minibatch_size = attr.ib(default=32)\n",
    "    memory_size = attr.ib(default=int(1e6))\n",
    "    num_episodes = attr.ib(default=1000)  # num of episodes in a training epoch\n",
    "    render_every = attr.ib(default=20)  # set to zero to turn off rendering\n",
    "    update_target_every = attr.ib(default=200)\n",
    "    vmin = attr.ib(default=-10)\n",
    "    vmax = attr.ib(default=10)\n",
    "    num_atoms = attr.ib(default=51)\n",
    "    learning_rate = attr.ib(default=0.000001)\n",
    "    monitor_total = attr.ib(default=10)\n",
    "    logger = attr.ib(default=None)\n",
    "    init_func = attr.ib(default=None)\n",
    "    weight_decay = attr.ib(default=0)\n",
    "    use_lr_scheduler = attr.ib(default=True)\n",
    "    online_net = attr.ib(default=None)\n",
    "    target_net = attr.ib(default=None)\n",
    "    max_gradient_norm = attr.ib(default=1)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        self.steps = 0\n",
    "        state_shape = self.env.observation_space.shape[0]\n",
    "        self.memory = Memory(self.memory_size, self.minibatch_size)\n",
    "        self.action_shape = self.env.action_space.n\n",
    "        if self.online_net is None or self.target_net is None:\n",
    "            print('using default ValDist class')\n",
    "            self.online_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "            self.target_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        if self.init_func:\n",
    "            gain = nn.init.calculate_gain('leaky_relu')\n",
    "            for param in self.online_net.parameters():\n",
    "                if param.dim() < 2:\n",
    "                    continue\n",
    "                self.init_func(param)        \n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.steps = 0\n",
    "        self.target_net_q_values = []\n",
    "        self.episode_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.layer1_grad_ratio = []\n",
    "        self.layer2_grad_ratio = []        \n",
    "        self.monitor_every = self.num_episodes//self.monitor_total\n",
    "        self.setup_lr_scheduler()\n",
    "        self.reward_normalizer = RunningStats((),)\n",
    "        self.filter_reward_buffer = 0\n",
    "    \n",
    "    def setup_lr_scheduler(self):\n",
    "        if not self.use_lr_scheduler:\n",
    "            return        \n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, \n",
    "            'max', \n",
    "            factor=.5, \n",
    "            patience=50,\n",
    "            cooldown=50,\n",
    "            min_lr=2e-06,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def step_learning_rate(self, episode):\n",
    "        smoothing_window = 11\n",
    "        if self.use_lr_scheduler and episode > smoothing_window:                                \n",
    "            smoothed_reward = smooth(np.array(self.episode_rewards[-20:]), window_len=smoothing_window).mean()\n",
    "            self.scheduler.step(smoothed_reward)\n",
    "        \n",
    "        \n",
    "    def render(self, episode):\n",
    "        if self.render_every and episode % self.render_every == 0:\n",
    "            self.env.render()\n",
    "\n",
    "    def training_progress_report(self, episode):\n",
    "        last_ep = self.episode_rewards[-1]\n",
    "        ten_ep_mean = sum(self.episode_rewards[-10:])/len(self.episode_rewards[-10:])\n",
    "        hundred_ep_mean = sum(self.episode_rewards[-100:])/len(self.episode_rewards[-100:])\n",
    "        try:\n",
    "            lin1_grad = self.online_net.linear1.weight.grad.norm()\n",
    "            lin2_grad = self.online_net.linear2.weight.grad.norm()\n",
    "        except:\n",
    "            lin1_grad =-666\n",
    "            lin2_grad = -666\n",
    "        return f'Ep: {episode} // steps: {self.steps} // last ep reward: {last_ep:.2f} // {min(10, len(self.episode_rewards[-10:]))}-ep mean: {ten_ep_mean:.2f} // {min(100, len(self.episode_rewards[-100:]))}-ep mean: {hundred_ep_mean:.2f}, layer1 grad: {lin1_grad:.2f}, layer2 grad: {lin2_grad:.2f}'\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.sample()\n",
    "        loss = categorical_vectorized_loss(self.online_net, self.target_net, batch, self.discount)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.online_net.parameters(), self.max_gradient_norm)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()/self.minibatch_size\n",
    "\n",
    "    def monitor(self):\n",
    "        if not hasattr(self.target_net, 'monitoring'):\n",
    "            return\n",
    "        if self.monitor_every and self.steps % self.monitor_every == 0:\n",
    "            self.target_net.monitoring(True)\n",
    "        else:\n",
    "            self.target_net.monitoring(False)\n",
    "\n",
    "    def filter_reward(self, reward):\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        return reward\n",
    "                \n",
    "    def train(self, start=0):\n",
    "        for episode in range(start, start + self.num_episodes):\n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_loss = 0\n",
    "            state = torch.tensor(self.env.reset(), dtype=torch.float)\n",
    "            self.target_net_q_values.append(self.target_net.predict_action_values(state).max().item())\n",
    "            if self.logger:\n",
    "                if self.steps == 0:\n",
    "                    self.logger.add_graph(self.target_net, state)            \n",
    "                self.logger.add_scalar('Target net Q values', self.target_net_q_values[-1], episode)                \n",
    "\n",
    "            layer1_size = self.target_net.linear1.weight.norm().item()\n",
    "            layer2_size = self.target_net.linear2.weight.norm().item()                \n",
    "            while not episode_done:\n",
    "                self.monitor()\n",
    "                epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * math.exp(-self.annealing_const * self.steps)\n",
    "                self.steps += 1                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, self.action_shape-1)\n",
    "                else:\n",
    "                    action = self.online_net.get_action(state).item()\n",
    "                self.render(episode)\n",
    "                self.monitor()\n",
    "                state_, reward, episode_done, _ = self.env.step(action)\n",
    "                state_ = torch.tensor(state_, dtype=torch.float)\n",
    "                reward = self.filter_reward(reward)\n",
    "                episode_reward += reward\n",
    "                self.memory.append((state, action, reward, state_, episode_done))\n",
    "                state = state_\n",
    "                if self.steps < 2:\n",
    "                    continue\n",
    "                episode_loss += self.replay()\n",
    "                \n",
    "                if self.steps % self.update_target_every == 0:\n",
    "                    self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "                if episode_done:\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.training_loss.append(episode_loss)\n",
    "                    print(self.training_progress_report(episode), end='\\r', flush=True)\n",
    "\n",
    "                    layer1_ratio = self.target_net.linear1.weight.norm().item() / (layer1_size + 1e-5)\n",
    "                    layer2_ratio = self.target_net.linear2.weight.norm().item() / (layer2_size + 1e-5)\n",
    "                    self.layer1_grad_ratio.append(layer1_ratio)\n",
    "                    self.layer2_grad_ratio.append(layer2_ratio)\n",
    "                                        \n",
    "                    self.log_params(episode)\n",
    "                    self.step_learning_rate(episode)\n",
    "        \n",
    "    def log_params(self, episode):\n",
    "        if not self.logger:\n",
    "            return     \n",
    "        if episode % 5 != 0:\n",
    "            return \n",
    "        self.logger.add_scalar('train loss', self.training_loss[-1], episode)\n",
    "        self.logger.add_scalar('episode reward', self.episode_rewards[-1], episode)  \n",
    "        self.logger.add_scalar('L1 size ratio', self.layer1_grad_ratio[-1], episode)\n",
    "        self.logger.add_scalar('L2 size ratio', self.layer2_grad_ratio[-1], episode)        \n",
    "        for idx, param in enumerate(self.target_net.parameters()):\n",
    "            if param.dim() == 1:\n",
    "                continue\n",
    "            if not hasattr(param, 'grad'):\n",
    "                continue\n",
    "            self.logger.add_scalar(f'layer {idx//2 + 1} gradient', param.grad.norm(), episode)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "Getting this working took a *lot* of debugging time. I went fairly far down a blind alley \n",
    "\n",
    "    - not the exploding gradients: small step size was a blind alley\n",
    "    - not the layer init\n",
    "    - Debugging w. three-armed bandit\n",
    "    - comparing against other implementations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T17:42:58.323620Z",
     "start_time": "2018-08-01T17:23:35.723135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Run 0: LR: 0.001 / L2 weight_decay: 0.000 / Xavier: True, LR-scheduler: True', 'log_dir': 'tensorboard-data/tuning-categorical', 'random_seed': 0, 'run_name': 'Aug-01-18@13:23:35-DaydreamNation.local', 'run_dir': 'tensorboard-data/tuning-categorical/Aug-01-18@13:23:35-DaydreamNation.local', 'tag': 'Experiment Config: Run 0: LR: 0.001 / L2 weight_decay: 0.000 / Xavier: True, LR-scheduler: True :: Aug-01-18@13:23:35\\n'}\n",
      "using default ValDist class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liavkoren/Envs/ai-gym/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator norm because torch.onnx.symbolic.norm does not exist\n",
      "  .format(op_name, op_name))\n",
      "/Users/liavkoren/Envs/ai-gym/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator reshape because torch.onnx.symbolic.reshape does not exist\n",
      "  .format(op_name, op_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    57: reducing learning rate of group 0 to 5.0000e-04. 12.90 // 70-ep mean: 15.53, layer1 grad: -666.00, layer2 grad: -666.00\n",
      "Epoch   213: reducing learning rate of group 0 to 2.5000e-04.an: 107.20 // 100-ep mean: 193.70, layer1 grad: -666.00, layer2 grad: -666.00\n",
      "Epoch   314: reducing learning rate of group 0 to 1.2500e-04.an: 126.20 // 100-ep mean: 159.60, layer1 grad: -666.00, layer2 grad: -666.00\n",
      "Epoch   415: reducing learning rate of group 0 to 6.2500e-05.n: 94.30 // 100-ep mean: 159.77, layer1 grad: -666.00, layer2 grad: -666.0000\n",
      "Epoch   516: reducing learning rate of group 0 to 3.1250e-05.n: 190.20 // 100-ep mean: 204.27, layer1 grad: -666.00, layer2 grad: -666.000\n",
      "Ep: 616 // steps: 87463 // last ep reward: 94.00 // 10-ep mean: 201.00 // 100-ep mean: 146.82, layer1 grad: -666.00, layer2 grad: -666.000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-af7f61b9f75c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-af7f61b9f75c>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(agent, logger, title)\u001b[0m\n\u001b[1;32m      8\u001b[0m                    track_update_ratio=True)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mleaky_relu_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-49f27b32e3c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_shape\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ec6dff367d5f>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_action_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ec6dff367d5f>\u001b[0m in \u001b[0;36mpredict_action_values\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_action_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;34m\"\"\" Return (batch-size x actions). \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mweighted_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch-size x actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ec6dff367d5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m\"\"\" Return (actions x atoms). \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_atoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (actions x atoms)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# these settings work okayish.\n",
    "\n",
    "def run_experiment(agent, logger, title):\n",
    "    monitor_module(agent.target_net, logger, \n",
    "                   track_data=True,\n",
    "                   track_grad=True,\n",
    "                   track_update=True,\n",
    "                   track_update_ratio=True)\n",
    "\n",
    "    agent.train()\n",
    "\n",
    "leaky_relu_value = 0.01\n",
    "kaiming = partial(nn.init.kaiming_normal_, a=leaky_relu_value)\n",
    "    \n",
    "# try:\n",
    "# lr = 0.01\n",
    "# weight_decay = 0.00001\n",
    "    \n",
    "    \n",
    "for idx in range(2):\n",
    "    lr = 0.001\n",
    "    weight_decay = 0.0002\n",
    "    xavier = True\n",
    "    lr_scheduler = True\n",
    "    title = f'Run {idx}: LR: {lr:.3f} / L2 weight_decay: {weight_decay:.3f} / Xavier: {xavier}, LR-scheduler: {lr_scheduler}'\n",
    "    config = {\n",
    "        'title':title,\n",
    "        'log_dir':'tensorboard-data/tuning-categorical',\n",
    "        'random_seed':idx\n",
    "    }\n",
    "    logger, config = init_experiment(config)\n",
    "    print(config)        \n",
    "    agent = CategoricalAgent(\n",
    "        cartpole, \n",
    "        learning_rate=lr, \n",
    "        weight_decay=weight_decay, \n",
    "        use_lr_scheduler=True, \n",
    "        logger=logger,\n",
    "        num_episodes=1500,\n",
    "        max_gradient_norm=.1,\n",
    "        monitor_total=5,\n",
    "        init_func=kaiming,\n",
    "        minibatch_size=32\n",
    "        \n",
    "    )\n",
    "    run_experiment(agent, logger, title=title)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.linear1 = nn.utils.weight_norm(nn.Linear(self.state_shape, num_hidden1_units))\n",
    "        self.linear2 = nn.utils.weight_norm(nn.Linear(num_hidden1_units, self.action_shape * self.num_atoms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T18:05:34.194265Z",
     "start_time": "2018-08-01T18:05:34.188919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 4])\n",
      "torch.Size([102])\n",
      "torch.Size([102, 1])\n",
      "torch.Size([102, 64])\n"
     ]
    }
   ],
   "source": [
    "for param in agent.target_net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T18:13:15.822269Z",
     "start_time": "2018-08-01T18:13:15.797901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear1.bias', Parameter containing:\n",
       "  tensor([ 2.2837e-02,  8.9836e-01, -3.0284e-07,  6.1729e-01,  6.6527e-01,\n",
       "           1.6942e-05,  1.6843e+00,  8.6162e-01,  1.5570e-01, -1.2134e-06,\n",
       "           1.2219e-01,  5.2946e-01,  8.0753e-01, -1.8360e-06, -4.7695e-06,\n",
       "          -1.9192e-06, -8.0315e-07, -4.9914e-02, -2.0311e-07,  3.0241e-02,\n",
       "           9.8359e-01,  8.9038e-02,  1.0886e-03,  1.0273e+00,  6.3867e-01,\n",
       "           2.9062e-04, -1.0312e-06,  1.0760e+00,  1.2311e-01,  1.0204e+00,\n",
       "           5.6503e-03,  1.7655e-02,  9.0944e-04,  3.5699e-04, -7.5461e-07,\n",
       "           3.7333e-01,  6.0606e-01, -1.2319e-07,  4.9548e-01,  1.5889e-03,\n",
       "           8.6313e-01,  5.3546e-04,  6.0167e-02,  5.3135e-01,  3.6015e-04,\n",
       "           8.2631e-01, -8.6295e-02,  9.0215e-01,  9.5349e-04, -5.3209e-06,\n",
       "           7.2453e-01,  5.1707e-01,  9.6492e-01,  4.2404e-02,  5.2171e-04,\n",
       "          -5.9406e-02,  6.4306e-01,  6.0597e-02,  1.0313e+00, -7.6999e-03,\n",
       "          -4.0177e-02,  6.0704e-02,  8.5172e-01,  2.5030e-01])),\n",
       " ('linear1.weight_g', Parameter containing:\n",
       "  tensor([[ 2.3071e-01],\n",
       "          [ 5.5824e-02],\n",
       "          [ 4.7959e-08],\n",
       "          [ 2.7382e-02],\n",
       "          [ 3.1721e-02],\n",
       "          [ 1.7113e-05],\n",
       "          [-2.0724e-01],\n",
       "          [ 5.5219e-02],\n",
       "          [-5.3891e-01],\n",
       "          [-2.2549e-06],\n",
       "          [-7.1741e-03],\n",
       "          [ 2.2521e-01],\n",
       "          [-5.9863e-02],\n",
       "          [-4.5545e-08],\n",
       "          [ 1.9220e-07],\n",
       "          [-1.2695e-07],\n",
       "          [-2.5570e-07],\n",
       "          [ 9.9016e-01],\n",
       "          [ 1.3991e-08],\n",
       "          [ 2.5577e-01],\n",
       "          [-6.2222e-02],\n",
       "          [-6.2717e-01],\n",
       "          [ 1.0101e-02],\n",
       "          [ 1.1234e-01],\n",
       "          [ 2.6139e-02],\n",
       "          [-1.8812e-05],\n",
       "          [ 4.6160e-08],\n",
       "          [ 9.7600e-02],\n",
       "          [ 3.9503e-01],\n",
       "          [ 7.1402e-02],\n",
       "          [ 1.2658e-01],\n",
       "          [-1.4245e-01],\n",
       "          [-3.6435e-04],\n",
       "          [ 6.3574e-04],\n",
       "          [ 7.7345e-07],\n",
       "          [-7.0758e-03],\n",
       "          [-4.3385e-02],\n",
       "          [ 7.6600e-08],\n",
       "          [ 1.2095e-01],\n",
       "          [-1.0644e-03],\n",
       "          [ 3.7792e-02],\n",
       "          [ 4.4338e-05],\n",
       "          [ 4.7618e-01],\n",
       "          [-7.2635e-02],\n",
       "          [ 2.4694e-05],\n",
       "          [ 6.3961e-02],\n",
       "          [ 7.0620e-01],\n",
       "          [-5.0677e-02],\n",
       "          [-3.7141e-04],\n",
       "          [-4.6882e-06],\n",
       "          [-3.6434e-02],\n",
       "          [-1.8726e-01],\n",
       "          [ 5.4505e-02],\n",
       "          [-3.4373e-01],\n",
       "          [ 6.5913e-05],\n",
       "          [ 5.5701e-01],\n",
       "          [-2.9256e-02],\n",
       "          [ 4.7784e-01],\n",
       "          [-6.3521e-02],\n",
       "          [-1.0546e+00],\n",
       "          [-4.4469e-01],\n",
       "          [ 4.7878e-01],\n",
       "          [-1.0619e-01],\n",
       "          [ 7.4301e-03]])),\n",
       " ('linear1.weight_v', Parameter containing:\n",
       "  tensor([[-6.6296e-03,  5.8946e-03,  1.1573e-02,  3.2117e-02],\n",
       "          [-1.5465e-02, -7.8470e-03, -6.0204e-03, -8.2249e-03],\n",
       "          [-2.4400e-08, -5.4112e-09, -4.8302e-11,  4.3377e-07],\n",
       "          [-8.5063e-03, -4.1595e-03, -3.3333e-03, -3.5384e-03],\n",
       "          [-9.3860e-03, -4.8330e-03, -3.7224e-03, -4.0260e-03],\n",
       "          [ 2.8015e-05,  3.1404e-05,  1.6357e-05,  2.4490e-07],\n",
       "          [ 4.4800e-02,  1.5039e-02,  1.1299e-02,  1.1345e-02],\n",
       "          [-1.2514e-02, -5.9718e-03, -3.7389e-03, -2.0517e-03],\n",
       "          [-7.2007e-02, -1.2714e-01, -2.0091e-02,  1.6615e-02],\n",
       "          [ 1.2293e-06,  4.4008e-07,  6.2501e-07, -3.4951e-07],\n",
       "          [ 3.4502e-03,  7.3236e-04,  6.3572e-04,  9.7610e-04],\n",
       "          [-3.5162e-02, -1.1709e-02,  5.6871e-03,  2.1475e-02],\n",
       "          [ 1.8314e-02,  4.3880e-03,  5.9821e-03,  1.1751e-02],\n",
       "          [ 7.0518e-08,  4.4240e-07,  2.2805e-07, -1.1871e-06],\n",
       "          [ 1.4621e-06, -4.0653e-07,  9.0213e-07, -3.5155e-06],\n",
       "          [ 2.9884e-07,  7.0994e-08, -1.6355e-08,  6.9850e-08],\n",
       "          [ 2.5625e-08, -7.4610e-08, -1.7117e-08, -3.9327e-07],\n",
       "          [ 4.2064e-02, -2.7587e-02, -7.6357e-02, -2.5340e-01],\n",
       "          [ 1.2364e-08,  1.2308e-08,  2.8834e-08,  9.9226e-10],\n",
       "          [-8.6278e-03,  5.6298e-03,  1.2479e-02,  3.4867e-02],\n",
       "          [ 1.7511e-02,  8.2188e-03,  6.8122e-03,  1.0611e-02],\n",
       "          [-9.0956e-03, -3.5992e-02, -3.7114e-02, -9.6974e-02],\n",
       "          [-2.8320e-03, -1.3690e-03, -5.9765e-04, -3.7976e-03],\n",
       "          [-2.8933e-02, -9.3994e-03, -1.2953e-02, -2.7944e-02],\n",
       "          [-8.4567e-03, -3.9010e-03, -3.2324e-03, -3.4325e-03],\n",
       "          [ 1.1424e-04, -3.1645e-05, -5.6686e-05,  1.2545e-06],\n",
       "          [ 1.7745e-07, -9.3796e-08,  3.0178e-09,  2.9242e-08],\n",
       "          [-3.3702e-02, -7.8087e-04, -9.4158e-03, -2.2022e-02],\n",
       "          [ 2.4759e-02,  4.4380e-02,  8.4647e-03,  4.6867e-03],\n",
       "          [-1.8071e-02, -9.5704e-03, -6.5413e-03, -7.0714e-03],\n",
       "          [-3.1213e-02, -1.0080e-02, -6.3608e-03, -4.0761e-02],\n",
       "          [ 2.9880e-02,  9.4773e-03,  5.9869e-03,  3.8813e-02],\n",
       "          [-2.0380e-04, -2.3829e-04,  4.5086e-06,  1.1347e-04],\n",
       "          [ 2.2981e-05,  4.1878e-04,  1.1577e-04, -4.9237e-04],\n",
       "          [-8.8925e-07, -8.7484e-07, -3.8293e-07,  9.8978e-08],\n",
       "          [ 4.4353e-03,  1.1644e-03,  1.7471e-03,  2.1996e-03],\n",
       "          [ 1.2393e-02,  4.3195e-03,  4.1819e-03,  7.8252e-03],\n",
       "          [-6.7447e-08,  2.1376e-08, -6.0090e-09,  6.5073e-09],\n",
       "          [-2.0969e-02, -2.4644e-03,  3.5760e-03,  9.6517e-03],\n",
       "          [-6.9321e-05, -7.2938e-04, -2.8640e-05,  6.3314e-04],\n",
       "          [-1.1358e-02, -4.9817e-03, -3.8989e-03, -4.4120e-03],\n",
       "          [-6.3028e-05,  5.0664e-05, -4.3268e-05,  2.2618e-04],\n",
       "          [ 5.3315e-03,  2.3646e-02,  2.4954e-02,  6.5056e-02],\n",
       "          [ 3.6246e-02, -1.8410e-02,  1.5201e-03,  1.7729e-02],\n",
       "          [-9.0632e-05,  3.3560e-05, -5.5559e-06,  5.2187e-05],\n",
       "          [-1.7796e-02, -7.5373e-03, -6.9329e-03, -1.1935e-02],\n",
       "          [ 1.4487e-02, -3.1611e-03, -3.8480e-02, -1.1937e-01],\n",
       "          [ 1.3694e-02,  7.1134e-03,  5.1247e-03,  5.5788e-03],\n",
       "          [-2.2824e-04, -2.1572e-04, -1.3013e-06,  1.4918e-04],\n",
       "          [-2.6091e-06,  1.0178e-06,  3.1341e-07, -1.3856e-06],\n",
       "          [ 1.0252e-02,  5.3963e-03,  4.0085e-03,  3.8986e-03],\n",
       "          [ 3.2335e-02,  7.3791e-03, -5.6069e-03, -1.8286e-02],\n",
       "          [-1.5088e-02, -7.1670e-03, -5.4008e-03, -6.6801e-03],\n",
       "          [ 1.1743e-02, -7.7612e-03, -1.6748e-02, -4.6621e-02],\n",
       "          [ 3.5833e-05,  3.2752e-05,  3.5059e-05, -2.0934e-04],\n",
       "          [ 1.5304e-02, -5.6423e-03, -3.4913e-02, -1.1022e-01],\n",
       "          [ 9.1441e-03,  3.8850e-03,  3.0408e-03,  3.8871e-03],\n",
       "          [ 5.4090e-03,  2.3972e-02,  2.5280e-02,  6.5924e-02],\n",
       "          [ 1.7345e-02,  6.6245e-03,  5.4573e-03,  7.1671e-03],\n",
       "          [ 1.3653e-01,  1.5108e-01,  8.1436e-02,  4.0398e-01],\n",
       "          [-5.1873e-03,  1.7949e-03,  1.7288e-02,  5.3480e-02],\n",
       "          [ 5.3832e-03,  2.3834e-02,  2.5150e-02,  6.5567e-02],\n",
       "          [ 2.7694e-02,  3.4818e-04,  1.0721e-02,  2.7490e-02],\n",
       "          [-4.1152e-03, -1.1050e-03, -1.3894e-03, -1.8714e-03]])),\n",
       " ('linear2.bias', Parameter containing:\n",
       "  tensor([-0.1857, -0.1857, -0.1738, -0.1471, -0.1450, -0.1339, -0.1206,\n",
       "          -0.1169, -0.1074, -0.0991, -0.0942, -0.0863, -0.0800, -0.0745,\n",
       "          -0.0677, -0.0620, -0.0561, -0.0498, -0.0440, -0.0378, -0.0317,\n",
       "          -0.0256, -0.0196, -0.0139, -0.0077, -0.0010,  0.0050,  0.0046,\n",
       "           0.0197,  0.0233,  0.0237,  0.0369,  0.0490,  0.0570,  0.0667,\n",
       "           0.0745,  0.0810,  0.0874,  0.0939,  0.0997,  0.1062,  0.1129,\n",
       "           0.1196,  0.1264,  0.1329,  0.1393,  0.1456,  0.1522,  0.1584,\n",
       "           0.1628,  0.0763, -0.1858, -0.1858, -0.1734, -0.1462, -0.1442,\n",
       "          -0.1333, -0.1201, -0.1163, -0.1067, -0.0982, -0.0930, -0.0848,\n",
       "          -0.0781, -0.0723, -0.0652, -0.0592, -0.0531, -0.0467, -0.0408,\n",
       "          -0.0346, -0.0285, -0.0228, -0.0170, -0.0111, -0.0051,  0.0011,\n",
       "           0.0069,  0.0190,  0.0126,  0.0161,  0.0228,  0.0278,  0.0534,\n",
       "           0.0609,  0.0700,  0.0772,  0.0844,  0.0907,  0.0967,  0.1019,\n",
       "           0.1078,  0.1139,  0.1197,  0.1256,  0.1308,  0.1359,  0.1406,\n",
       "           0.1451,  0.1493,  0.1516,  0.0744])),\n",
       " ('linear2.weight_g', Parameter containing:\n",
       "  tensor([[ 0.7990],\n",
       "          [-0.7990],\n",
       "          [-0.7480],\n",
       "          [-0.6352],\n",
       "          [ 0.6266],\n",
       "          [ 0.5793],\n",
       "          [ 0.5225],\n",
       "          [-0.5062],\n",
       "          [ 0.4654],\n",
       "          [-0.4293],\n",
       "          [ 0.4074],\n",
       "          [ 0.3724],\n",
       "          [-0.3443],\n",
       "          [ 0.3193],\n",
       "          [-0.2889],\n",
       "          [ 0.2632],\n",
       "          [-0.2369],\n",
       "          [-0.2092],\n",
       "          [ 0.1837],\n",
       "          [ 0.1571],\n",
       "          [-0.1307],\n",
       "          [-0.1052],\n",
       "          [-0.0798],\n",
       "          [ 0.0560],\n",
       "          [ 0.0363],\n",
       "          [-0.0318],\n",
       "          [-0.0463],\n",
       "          [ 0.9032],\n",
       "          [ 0.8584],\n",
       "          [ 0.2289],\n",
       "          [-0.4057],\n",
       "          [-0.2743],\n",
       "          [-0.2656],\n",
       "          [-0.2867],\n",
       "          [-0.2909],\n",
       "          [ 0.3185],\n",
       "          [-0.3415],\n",
       "          [-0.3685],\n",
       "          [-0.3958],\n",
       "          [-0.4236],\n",
       "          [ 0.4527],\n",
       "          [ 0.4821],\n",
       "          [-0.5123],\n",
       "          [-0.5429],\n",
       "          [-0.5737],\n",
       "          [-0.6048],\n",
       "          [ 0.6364],\n",
       "          [-0.6687],\n",
       "          [-0.7017],\n",
       "          [ 0.7355],\n",
       "          [-0.4920],\n",
       "          [ 0.7998],\n",
       "          [-0.7998],\n",
       "          [-0.7463],\n",
       "          [ 0.6298],\n",
       "          [-0.6211],\n",
       "          [ 0.5740],\n",
       "          [-0.5174],\n",
       "          [-0.5009],\n",
       "          [-0.4596],\n",
       "          [ 0.4231],\n",
       "          [ 0.4010],\n",
       "          [-0.3656],\n",
       "          [ 0.3371],\n",
       "          [-0.3117],\n",
       "          [-0.2809],\n",
       "          [-0.2550],\n",
       "          [ 0.2285],\n",
       "          [-0.2007],\n",
       "          [ 0.1752],\n",
       "          [-0.1489],\n",
       "          [ 0.1230],\n",
       "          [ 0.0985],\n",
       "          [ 0.0746],\n",
       "          [-0.0535],\n",
       "          [-0.0403],\n",
       "          [-0.0424],\n",
       "          [-0.0593],\n",
       "          [ 1.0301],\n",
       "          [ 0.9984],\n",
       "          [-0.3722],\n",
       "          [-0.5658],\n",
       "          [ 0.3539],\n",
       "          [-0.2748],\n",
       "          [-0.2965],\n",
       "          [ 0.3008],\n",
       "          [ 0.3276],\n",
       "          [-0.3519],\n",
       "          [ 0.3806],\n",
       "          [-0.4083],\n",
       "          [ 0.4361],\n",
       "          [-0.4656],\n",
       "          [ 0.4958],\n",
       "          [-0.5267],\n",
       "          [-0.5575],\n",
       "          [ 0.5886],\n",
       "          [-0.6199],\n",
       "          [ 0.6519],\n",
       "          [-0.6846],\n",
       "          [-0.7179],\n",
       "          [ 0.7522],\n",
       "          [-0.5125]])),\n",
       " ('linear2.weight_v', Parameter containing:\n",
       "  tensor([[-8.3128e-05, -2.4691e-03,  2.4583e-06,  ..., -1.6411e-04,\n",
       "           -2.3936e-03, -6.9054e-04],\n",
       "          [ 8.3156e-05,  2.4704e-03, -2.6400e-11,  ...,  1.6414e-04,\n",
       "            2.3948e-03,  6.9088e-04],\n",
       "          [ 8.0749e-05,  2.3542e-03, -6.6770e-08,  ...,  1.6260e-04,\n",
       "            2.2823e-03,  6.5847e-04],\n",
       "          ...,\n",
       "          [ 1.0109e-03, -1.5245e-02,  2.3226e-11,  ...,  2.8441e-03,\n",
       "           -1.4116e-02, -4.1302e-03],\n",
       "          [-1.1074e-03,  1.7162e-02, -3.5408e-11,  ..., -3.3408e-03,\n",
       "            1.5842e-02,  4.6666e-03],\n",
       "          [-7.7601e-04, -7.8063e-03,  2.7716e-10,  ...,  3.9884e-03,\n",
       "           -8.5618e-03, -1.9119e-03]]))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(agent.target_net.named_parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "266px",
    "width": "268px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
