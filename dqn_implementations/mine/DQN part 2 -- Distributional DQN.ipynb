{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro/References\" data-toc-modified-id=\"Intro/References-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro/References</a></span></li><li><span><a href=\"#Scaling-not-clipping?\" data-toc-modified-id=\"Scaling-not-clipping?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scaling not clipping?</a></span></li><li><span><a href=\"#Pytorch\" data-toc-modified-id=\"Pytorch-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pytorch</a></span></li><li><span><a href=\"#Pytorch-monitor\" data-toc-modified-id=\"Pytorch-monitor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pytorch monitor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vectorizing-is-terrible/awesome.\" data-toc-modified-id=\"Vectorizing-is-terrible/awesome.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vectorizing is terrible/awesome.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/References\n",
    "\n",
    "A Distributional Perspective on Reinforcement Learning\n",
    "https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\n",
    "https://arxiv.org/abs/1707.06887\n",
    "\n",
    "Distributional Reinforcement Learning with Quantile Regression\n",
    "https://arxiv.org/abs/1710.10044\n",
    "\n",
    "Distributional RL\n",
    "https://mtomassoli.github.io/2017/12/08/distributional_rl/\n",
    "\n",
    "An Analysis of Categorical Distributional Reinforcement Learning\n",
    "https://arxiv.org/abs/1802.08163\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Scaling not clipping?\n",
    "\n",
    "# Pytorch\n",
    "# Pytorch monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T14:44:18.836141Z",
     "start_time": "2018-07-20T14:44:18.780517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import attr\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_monitor import init_experiment, monitor_module\n",
    "# from smooth import smooth  # timeseries smoothing function\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cartpole = gym.make('CartPole-v1')\n",
    "lunarlander = gym.make('LunarLander-v2')\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T12:57:30.206978Z",
     "start_time": "2018-07-20T12:57:29.541715Z"
    },
    "code_folding": [
     1,
     22
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Memory(deque):\n",
    "    \"\"\" Experience Replay Memory class. \"\"\"\n",
    "    size = attr.ib()\n",
    "    minibatch_size = attr.ib()\n",
    "\n",
    "    def append(self, thing):\n",
    "        if len(self) > self.size - 1:\n",
    "            self.popleft()\n",
    "        return super().append(thing)\n",
    "\n",
    "    def sample(self):\n",
    "        batch_size = min(len(self), self.minibatch_size)\n",
    "        data = random.sample(self, batch_size)\n",
    "        states = torch.stack([record[0] for record in data])\n",
    "        actions = torch.tensor([record[1] for record in data], dtype=torch.long)\n",
    "        rewards = torch.tensor([record[2] for record in data], dtype=torch.float)\n",
    "        states_ = torch.stack([record[3] for record in data])\n",
    "        dones = torch.tensor([record[4] for record in data], dtype=torch.long)\n",
    "        return (states, actions, rewards, states_, dones)\n",
    "\n",
    "\n",
    "class ValueDistributionDeep(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64, num_hidden2_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.linear2 = nn.Linear(num_hidden1_units, num_hidden2_units)\n",
    "        self.linear3 = nn.Linear(num_hidden2_units, num_hidden2_units)\n",
    "        self.linear4 = nn.Linear(num_hidden2_units, self.action_shape * self.num_atoms)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        x1 = F.selu(self.linear1(x))\n",
    "        x2 = F.selu(self.linear2(x1))\n",
    "        x3 = F.selu(self.linear3(x2))\n",
    "        x4 = self.linear4(x3).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x4, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x1', x1, track_data=True, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=True, track_grad=True)\n",
    "            self.monitor('x3', x3, track_data=True, track_grad=True)\n",
    "            self.monitor('x4', x4, track_data=True, track_grad=True)\n",
    "            self.monitor('out', out, track_data=True, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "class ValueDistribution(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64, num_hidden2_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.linear2 = nn.Linear(num_hidden1_units, self.action_shape * self.num_atoms)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        x1 = F.relu(self.linear1(x))\n",
    "        x2 = self.linear2(x1).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x2, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x1', x1, track_data=True, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=True, track_grad=True)\n",
    "            self.monitor('out', out, track_data=True, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T19:19:40.302183Z",
     "start_time": "2018-07-19T19:19:40.113459Z"
    }
   },
   "source": [
    "## Vectorizing is terrible/awesome.\n",
    "\n",
    "This is the algo we'll be implementing. \n",
    "\n",
    "![The C51 algorithm](assets/C51-algo.png)\n",
    "\n",
    "Vectorizing this code is *very* important. Even on my lowely macbook, the fully vectorized version of this algorithm that accepts a minibatch runs about *30 times* faster than a naive implementation that's called inside a loop. This cashes out to 1000 training episodes of LunarLander in about 10 minutes, verses five hours. \n",
    "\n",
    "My process for vectoring this code was.. to sort of squint at it. \n",
    "Seriously. I wasn't even sure if I should first generalize it to accept minibatch tensors and then remove the loop, or vice versa. \n",
    "\n",
    "Squinting at it, thought, (and stepping through it line by line in Jupyter a few times), it became clear that the would actually tricky to vectorize: \n",
    "\n",
    "![The bastard lines]](assets/C51-algo-large.png)\n",
    "\n",
    "I decided to take the easy wins first, and first converted the `categorical_loss` function to accept minibatches first. This is straighforward, mostly just reshaping and expanding tensors. Pytorch's `squeeze` and `unsqueeze` methods have fun names and are great for this. \n",
    "\n",
    "Those two lines, though, were bloody horrible. \n",
    "\n",
    "They ended up cashing out into the following dense six lines of python:\n",
    "\n",
    "```python\n",
    "offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "m += lo_component.resize_as(m) + hi_component.resize_as(m)       \n",
    "```\n",
    "\n",
    "The main insight is that the `lo` and `hi` tensors contain routing information. They tend look like this:\n",
    "\n",
    "```\n",
    "lo:\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 10],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]])\n",
    "hi:\n",
    "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 4, 5, 6, 7, 7, 8, 9, 10, 10, 10],\n",
    "        [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10]])\n",
    "```\n",
    "\n",
    "This is for a three-transition test minibatch. The values are the target indicies for where probability needs to accumulate inside `m` our new probability-mass tensor.\n",
    "\n",
    "Eventually after squinting a lot at the PyTorch docs, I figured out that PyTorch's [`index_add`](https://pytorch.org/docs/stable/tensors.html?highlight=index_add#torch.Tensor.index_add_) method would do the trick. \n",
    "\n",
    "Usings `index_add` requires that all the tensors be unrolled, which is why we need index-offsets. Put it together, and you're done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T12:57:33.601401Z",
     "start_time": "2018-07-20T12:57:33.472565Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def categorical_vectorized_loss(online_net, target_net, transitions, discount): \n",
    "    states, actions, rewards, states_, dones = transitions\n",
    "    not_dones = (1 - dones).type(torch.FloatTensor)\n",
    "    atoms = target_net.atoms\n",
    "    probabilities = target_net.forward(states_)\n",
    "    Q_x_ = (probabilities * atoms).sum(2)\n",
    "    batch_size = states.shape[0]\n",
    "    assert Q_x_.shape == torch.Size((batch_size, target_net.action_shape)), f'Got: {Q_x_.shape}, expected: {(batch_size, target_net.action_shape)}'\n",
    "    a_star = Q_x_.argmax(dim=1) \n",
    "    assert a_star.shape == torch.Size((batch_size,)), f'Got {a_star.shape}, expected: ((batch_size,))'\n",
    "    \n",
    "    # compute the projected probability:\n",
    "    delta_z = (target_net.vmax - target_net.vmin)/(target_net.num_atoms - 1)    \n",
    "    # select only the probabilities distributions for the a_star actions:\n",
    "    probabilities = probabilities[range(batch_size), a_star]\n",
    "    T_zj = rewards.unsqueeze(1) + discount * atoms * not_dones.unsqueeze(1)\n",
    "    b_j = (T_zj.clamp(target_net.vmin, target_net.vmax) - target_net.vmin) / delta_z  # correct    \n",
    "    lo = b_j.floor()        \n",
    "    hi = b_j.ceil()\n",
    "    m = torch.zeros(batch_size, target_net.num_atoms, dtype=torch.float)\n",
    "    lo_component = torch.zeros_like(m.view(-1))\n",
    "    hi_component = torch.zeros_like(m.view(-1))\n",
    "    # offset will be used for indexing when we flatten the tensors into vectors:\n",
    "    offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "    idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "    lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "    hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "    lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "    hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "    m += lo_component.reshape(batch_size, target_net.num_atoms) + hi_component.reshape(batch_size, target_net.num_atoms)\n",
    "    # cross enthropy is Sigma <true> log <unnatural>, so for us is: target log(online)\n",
    "    online_distribution = online_net.forward(states)[range(batch_size), actions]\n",
    "    return -( m * online_distribution.log() ).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:07:09.975723Z",
     "start_time": "2018-07-20T15:07:09.247055Z"
    },
    "code_folding": [
     101
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class CategoricalAgent:\n",
    "    env = attr.ib()\n",
    "    discount = attr.ib(default=0.99)\n",
    "    epsilon_max = attr.ib(default=1.0)\n",
    "    epsilon_min = attr.ib(default=0.01)\n",
    "    annealing_const = attr.ib(default=.001)  # aka Lambda\n",
    "    minibatch_size = attr.ib(default=32)\n",
    "    memory_size = attr.ib(default=int(1e6))\n",
    "    num_episodes = attr.ib(default=1000)  # num of episodes in a training epoch\n",
    "    render_every = attr.ib(default=20)  # set to zero to turn off rendering\n",
    "    update_target_every = attr.ib(default=200)\n",
    "    vmin = attr.ib(default=-10)\n",
    "    vmax = attr.ib(default=10)\n",
    "    num_atoms = attr.ib(default=51)\n",
    "    learning_rate = attr.ib(default=0.000001)\n",
    "    monitor_total = attr.ib(default=10)\n",
    "    logger = attr.ib(default=None)\n",
    "    reward_scaling = attr.ib(default=1.0)\n",
    "    xavier = attr.ib(default=True)\n",
    "    weight_decay = attr.ib(default=0)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        self.steps = 0\n",
    "        state_shape = self.env.observation_space.shape[0]\n",
    "        self.memory = Memory(self.memory_size, self.minibatch_size)\n",
    "        self.action_shape = self.env.action_space.n\n",
    "        self.online_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        self.target_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        if self.xavier:\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "            for param in self.online_net.parameters():\n",
    "                if param.dim() < 2:\n",
    "                    continue\n",
    "                nn.init.xavier_uniform_(param, gain=gain)        \n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.steps = 0\n",
    "        self.target_net_q_values = []\n",
    "        self.episode_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.monitor_every = self.num_episodes//self.monitor_total\n",
    "\n",
    "        \n",
    "        \n",
    "    def render(self, episode):\n",
    "        if self.render_every and episode % self.render_every == 0:\n",
    "            self.env.render()\n",
    "\n",
    "    def training_progress_report(self, episode):\n",
    "        last_ep = self.episode_rewards[-1]\n",
    "        ten_ep_mean = sum(self.episode_rewards[-10:])/len(self.episode_rewards[-10:])\n",
    "        hundred_ep_mean = sum(self.episode_rewards[-100:])/len(self.episode_rewards[-100:])\n",
    "        lin1_grad = self.online_net.linear1.weight.grad.norm()\n",
    "        lin2_grad = self.online_net.linear2.weight.grad.norm()\n",
    "        return f'Ep: {episode} // steps: {self.steps} // last ep reward: {last_ep:.2f} // {min(10, len(self.episode_rewards[-10:]))}-ep mean: {ten_ep_mean:.2f} // {min(100, len(self.episode_rewards[-100:]))}-ep mean: {hundred_ep_mean:.2f}, layer1 grad: {lin1_grad:.2f}, layer2 grad: {lin2_grad}'\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.sample()\n",
    "        loss = categorical_vectorized_loss(self.online_net, self.target_net, batch, self.discount)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()/self.minibatch_size\n",
    "\n",
    "    def monitor(self):\n",
    "        if not hasattr(self.target_net, 'monitoring'):\n",
    "            return\n",
    "        if self.monitor_every and self.steps % self.monitor_every == 0:\n",
    "            self.target_net.monitoring(True)\n",
    "        else:\n",
    "            self.target_net.monitoring(False)\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_loss = 0\n",
    "            state = torch.tensor(self.env.reset(), dtype=torch.float)\n",
    "            self.target_net_q_values.append(self.target_net.predict_action_values(state).max().item())\n",
    "            if self.steps == 0 and self.logger:\n",
    "                self.logger.add_graph(self.target_net, state)\n",
    "            self.logger.add_scalar('Target net Q values', self.target_net_q_values[-1], episode)                \n",
    "            while not episode_done:\n",
    "                self.monitor()\n",
    "                epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * math.exp(-self.annealing_const * self.steps)\n",
    "                self.steps += 1                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, self.action_shape-1)\n",
    "                else:\n",
    "                    action = self.online_net.get_action(state).item()\n",
    "                self.render(episode)\n",
    "                self.monitor()\n",
    "                state_, reward, episode_done, _ = self.env.step(action)\n",
    "                reward *= self.reward_scaling\n",
    "                state_ = torch.tensor(state_, dtype=torch.float)\n",
    "                episode_reward += reward\n",
    "                self.memory.append((state, action, reward, state_, episode_done))\n",
    "                state = state_\n",
    "                if self.steps < 2:\n",
    "                    continue\n",
    "                episode_loss += self.replay()\n",
    "\n",
    "                if self.steps % self.update_target_every == 0:\n",
    "                    self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "                if episode_done:\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.training_loss.append(episode_loss)\n",
    "                    print(self.training_progress_report(episode), end='\\r', flush=True)\n",
    "                    self.logger.add_scalar('train loss', episode_loss, episode)\n",
    "                    self.logger.add_scalar('episode reward', episode_reward, episode)                    \n",
    "        self.env.close()\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T14:49:04.451580Z",
     "start_time": "2018-07-20T14:49:04.430012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enable monitoring\n",
    "def run_experiment(learning_rate, weight_decay, xavier, title):\n",
    "    try:\n",
    "        config = {\n",
    "            'title':title,\n",
    "            'log_dir':'tuning-categorical',\n",
    "            'random_seed':0\n",
    "        }\n",
    "        logger, config = init_experiment(config)\n",
    "        agent = CategoricalAgent(\n",
    "            lunarlander, \n",
    "            #learning_rate=0.0001, \n",
    "            learning_rate=learning_rate, \n",
    "            num_episodes=2000,\n",
    "            update_target_every=200,  \n",
    "            reward_scaling=.3,\n",
    "            logger=logger,\n",
    "            # weight_decay=0.01,\n",
    "            weight_decay=weight_decay,\n",
    "            xavier=True\n",
    "        )\n",
    "        monitor_module(agent.target_net, logger, \n",
    "                       track_data=True,\n",
    "                       track_grad=True,\n",
    "                       track_update=True,\n",
    "                       track_update_ratio=True)\n",
    "\n",
    "        agent.train()\n",
    "    except:\n",
    "        print(sys.exc_info()[0])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:07:20.366470Z",
     "start_time": "2018-07-20T15:07:14.467630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liavkoren/Envs/ai-gym/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator reshape because torch.onnx.symbolic.reshape does not exist\n",
      "  .format(op_name, op_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'KeyboardInterrupt'> ep reward: -63.85 // 6-ep mean: -77.25 // 6-ep mean: -77.25, layer1 grad: 0.0030973798129707575, layer2 grad: 0.005117109045386314\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "l2 = 0.001\n",
    "xavier = True\n",
    "title = f'LR: {lr} / L2 reg: {l2} / Xavier: {xavier}'\n",
    "run_experiment(learning_rate=lr, weight_decay=l2, xavier=xavier, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T02:45:28.547085Z",
     "start_time": "2018-07-20T02:45:28.291122Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(smooth(np.array(agent.target_net_q_values)))\n",
    "# plt.plot(smooth(np.array(agent.episode_rewards)))\n",
    "plt.plot(smooth(np.array(agent.training_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T21:27:58.741267Z",
     "start_time": "2018-07-19T21:27:58.636187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        ...,\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
       "        [nan., nan., nan.,  ..., nan., nan., nan.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.target_net.linear4.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "Getting this working took a *lot* of debugging time. I went fairly far down a blind alley \n",
    "\n",
    "    - not the exploding gradients: small step size was a blind alley\n",
    "    - not the layer init\n",
    "    - Debugging w. three-armed bandit\n",
    "    - comparing against other implementations?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "266px",
    "width": "268px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
