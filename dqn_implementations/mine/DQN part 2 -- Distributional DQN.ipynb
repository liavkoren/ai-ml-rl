{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro/References\" data-toc-modified-id=\"Intro/References-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro/References</a></span></li><li><span><a href=\"#Scaling-not-clipping?\" data-toc-modified-id=\"Scaling-not-clipping?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scaling not clipping?</a></span></li><li><span><a href=\"#Pytorch\" data-toc-modified-id=\"Pytorch-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pytorch</a></span></li><li><span><a href=\"#Pytorch-monitor\" data-toc-modified-id=\"Pytorch-monitor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pytorch monitor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vectorizing-is-terrible/awesome.\" data-toc-modified-id=\"Vectorizing-is-terrible/awesome.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vectorizing is terrible/awesome.</a></span></li></ul></li><li><span><a href=\"#Works-beautifully-for-CartPole\" data-toc-modified-id=\"Works-beautifully-for-CartPole-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Works beautifully for CartPole</a></span></li><li><span><a href=\"#Debugging\" data-toc-modified-id=\"Debugging-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Debugging</a></span><ul class=\"toc-item\"><li><span><a href=\"#Things-I-tried\" data-toc-modified-id=\"Things-I-tried-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Things I tried</a></span></li></ul></li><li><span><a href=\"#Lunar-Lander\" data-toc-modified-id=\"Lunar-Lander-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Lunar Lander</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/References\n",
    "\n",
    "A Distributional Perspective on Reinforcement Learning\n",
    "https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\n",
    "https://arxiv.org/abs/1707.06887\n",
    "\n",
    "Distributional Reinforcement Learning with Quantile Regression\n",
    "https://arxiv.org/abs/1710.10044\n",
    "\n",
    "Distributional RL\n",
    "https://mtomassoli.github.io/2017/12/08/distributional_rl/\n",
    "\n",
    "An Analysis of Categorical Distributional Reinforcement Learning\n",
    "https://arxiv.org/abs/1802.08163\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Scaling not clipping?\n",
    "\n",
    "# Pytorch\n",
    "# Pytorch monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T22:47:38.377841Z",
     "start_time": "2018-08-19T22:47:36.693613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import (deque, defaultdict)\n",
    "from functools import partial\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import attr\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_monitor import init_experiment, monitor_module\n",
    "from running_stats import RunningStats\n",
    "from smooth import smooth  # timeseries smoothing function\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cartpole = gym.make('CartPole-v1')\n",
    "lunarlander = gym.make('LunarLander-v2')\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T22:47:38.676610Z",
     "start_time": "2018-08-19T22:47:38.381436Z"
    },
    "code_folding": [
     1,
     22
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Memory(deque):\n",
    "    \"\"\" Experience Replay Memory class. \"\"\"\n",
    "    size = attr.ib()\n",
    "    minibatch_size = attr.ib()\n",
    "\n",
    "    def append(self, thing):\n",
    "        if len(self) > self.size - 1:\n",
    "            self.popleft()\n",
    "        return super().append(thing)\n",
    "\n",
    "    def sample(self):\n",
    "        batch_size = min(len(self), self.minibatch_size)\n",
    "        data = random.sample(self, batch_size)\n",
    "        states = torch.stack([record[0] for record in data])\n",
    "        actions = torch.tensor([record[1] for record in data], dtype=torch.long)\n",
    "        rewards = torch.tensor([record[2] for record in data], dtype=torch.float)\n",
    "        states_ = torch.stack([record[3] for record in data])\n",
    "        dones = torch.tensor([record[4] for record in data], dtype=torch.long)\n",
    "        return (states, actions, rewards, states_, dones)\n",
    "\n",
    "    \n",
    "class ValueDistribution(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.linear2 = nn.Linear(num_hidden1_units, self.action_shape * self.num_atoms)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        x1 = F.leaky_relu(self.linear1(x))\n",
    "        x2 = self.linear2(x1).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x2, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x1', x1, track_data=False, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=False, track_grad=True)\n",
    "            self.monitor('out', out, track_data=False, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T19:19:40.302183Z",
     "start_time": "2018-07-19T19:19:40.113459Z"
    }
   },
   "source": [
    "## Vectorizing is terrible/awesome.\n",
    "\n",
    "This is the algo we'll be implementing. \n",
    "\n",
    "![The C51 algorithm](assets/C51-algo.png)\n",
    "\n",
    "Vectorizing this code is *very* important. Even on my lowely macbook, the fully vectorized version of this algorithm that accepts a minibatch runs about *30 times* faster than a naive implementation that's called inside a loop. This cashes out to 1000 training episodes of LunarLander in about 10 minutes, verses five hours. \n",
    "\n",
    "My process for vectoring this code was.. to sort of squint at it. \n",
    "Seriously. I wasn't even sure if I should first generalize it to accept minibatch tensors and then remove the loop, or vice versa. \n",
    "\n",
    "Squinting at it, thought, (and stepping through it line by line in Jupyter a few times), it became clear that the would actually tricky to vectorize: \n",
    "\n",
    "![The bastard lines]](assets/C51-algo-large.png)\n",
    "\n",
    "I decided to take the easy wins first, and first converted the `categorical_loss` function to accept minibatches first. This is straighforward, mostly just reshaping and expanding tensors. Pytorch's `squeeze` and `unsqueeze` methods have fun names and are great for this. \n",
    "\n",
    "Those two lines, though, were bloody horrible. \n",
    "\n",
    "They ended up cashing out into the following dense six lines of python:\n",
    "\n",
    "```python\n",
    "offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "m += lo_component.resize_as(m) + hi_component.resize_as(m)       \n",
    "```\n",
    "\n",
    "The main insight is that the `lo` and `hi` tensors contain routing information. They tend look like this:\n",
    "\n",
    "```\n",
    "lo:\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 10],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]])\n",
    "hi:\n",
    "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 4, 5, 6, 7, 7, 8, 9, 10, 10, 10],\n",
    "        [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10]])\n",
    "```\n",
    "\n",
    "This is for a three-transition test minibatch. The values are the target indicies for where probability needs to accumulate inside `m` our new probability-mass tensor.\n",
    "\n",
    "Eventually after squinting a lot at the PyTorch docs, I figured out that PyTorch's [`index_add`](https://pytorch.org/docs/stable/tensors.html?highlight=index_add#torch.Tensor.index_add_) method would do the trick. \n",
    "\n",
    "Usings `index_add` requires that all the tensors be unrolled, which is why we need index-offsets. Put it together, and you're done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T22:47:38.835199Z",
     "start_time": "2018-08-19T22:47:38.679300Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def categorical_vectorized_loss(online_net, target_net, transitions, discount): \n",
    "    states, actions, rewards, states_, dones = transitions\n",
    "    not_dones = (1 - dones).type(torch.FloatTensor)\n",
    "    atoms = target_net.atoms\n",
    "    probabilities = target_net.forward(states_)\n",
    "    Q_x_ = (probabilities * atoms).sum(2)\n",
    "    batch_size = states.shape[0]\n",
    "    assert Q_x_.shape == torch.Size((batch_size, target_net.action_shape)), f'Got: {Q_x_.shape}, expected: {(batch_size, target_net.action_shape)}'\n",
    "    a_star = Q_x_.argmax(dim=1) \n",
    "    assert a_star.shape == torch.Size((batch_size,)), f'Got {a_star.shape}, expected: ((batch_size,))'\n",
    "    \n",
    "    # compute the projected probability:\n",
    "    delta_z = (target_net.vmax - target_net.vmin)/(target_net.num_atoms - 1)    \n",
    "    # select only the probabilities distributions for the a_star actions:\n",
    "    probabilities = probabilities[range(batch_size), a_star]\n",
    "    T_zj = rewards.unsqueeze(1) + discount * atoms * not_dones.unsqueeze(1)\n",
    "    b_j = (T_zj.clamp(target_net.vmin, target_net.vmax) - target_net.vmin) / delta_z  # correct    \n",
    "    lo = b_j.floor()        \n",
    "    hi = b_j.ceil()\n",
    "    m = torch.zeros(batch_size, target_net.num_atoms, dtype=torch.float)\n",
    "    lo_component = torch.zeros_like(m.view(-1))\n",
    "    hi_component = torch.zeros_like(m.view(-1))\n",
    "    # offset will be used for indexing when we flatten the tensors into vectors:\n",
    "    offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "    idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "    lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "    hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "    lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "    hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "    m += lo_component.reshape(batch_size, target_net.num_atoms) + hi_component.reshape(batch_size, target_net.num_atoms)\n",
    "    # cross enthropy is Sigma <true> log <unnatural>, so for us is: target log(online)\n",
    "    online_distribution = online_net.forward(states)[range(batch_size), actions]\n",
    "    return -( m * online_distribution.log() ).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T22:47:41.081030Z",
     "start_time": "2018-08-19T22:47:38.839267Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-16dedb5778c4>, line 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-16dedb5778c4>\"\u001b[0;36m, line \u001b[0;32m76\u001b[0m\n\u001b[0;31m    smoothed_reward = smooth(rewards), window_len=window_len).mean()\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@attr.s\n",
    "class CategoricalAgent:\n",
    "    env = attr.ib()\n",
    "    discount = attr.ib(default=0.99)\n",
    "    epsilon_max = attr.ib(default=1.0)\n",
    "    epsilon_min = attr.ib(default=0.01)\n",
    "    annealing_const = attr.ib(default=.001)  # aka Lambda\n",
    "    minibatch_size = attr.ib(default=32)\n",
    "    memory_size = attr.ib(default=int(1e6))\n",
    "    num_episodes = attr.ib(default=1000)  # num of episodes in a training epoch\n",
    "    render_every = attr.ib(default=20)  # set to zero to turn off rendering\n",
    "    update_target_every = attr.ib(default=200)\n",
    "    vmin = attr.ib(default=-10)\n",
    "    vmax = attr.ib(default=10)\n",
    "    num_atoms = attr.ib(default=51)\n",
    "    learning_rate = attr.ib(default=0.000001)\n",
    "    monitor_total = attr.ib(default=10)\n",
    "    logger = attr.ib(default=None)\n",
    "    use_kaiming_init = attr.ib(default=True)\n",
    "    weight_decay = attr.ib(default=0)\n",
    "    use_lr_scheduler = attr.ib(default=True)\n",
    "    max_gradient_norm = attr.ib(default=1)\n",
    "    learning_rate_floor = attr.ib(default=2e-06)\n",
    "    learning_rate_patience = attr.ib(default=50)\n",
    "    learning_rate_cooldown = attr.ib(default=50)\n",
    "    learning_rate_smoothing_window = attr.ib(default=20)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        self.steps = 0\n",
    "        state_shape = self.env.observation_space.shape[0]\n",
    "        self.memory = Memory(self.memory_size, self.minibatch_size)\n",
    "        self.action_shape = self.env.action_space.n\n",
    "        self.online_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        self.target_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        self.init_layers()\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.steps = 0\n",
    "        self.target_net_q_values = deque([], 1000)\n",
    "        self.episode_rewards = deque([], 1000)\n",
    "        self.training_loss = deque([], 1000)\n",
    "        self.layer1_grad_ratio = deque([], 1000)\n",
    "        self.layer2_grad_ratio = deque([]  , 1000)\n",
    "        self.monitor_every = self.num_episodes//self.monitor_total\n",
    "        self.init_learning_rate_scheduler()\n",
    "        self.reward_normalizer = RunningStats((),)\n",
    "        self.filter_reward_buffer = 0\n",
    "    \n",
    "    def init_layers(self):\n",
    "        if not self.use_kaiming_init:\n",
    "            return                \n",
    "        for param in self.online_net.parameters():\n",
    "            if param.dim() < 2:\n",
    "                continue\n",
    "            leaky_relu_value = 0.01\n",
    "            nn.init.kaiming_normal_(param, a=leaky_relu_value)\n",
    "        \n",
    "    \n",
    "    def init_learning_rate_scheduler(self):\n",
    "        if not self.use_lr_scheduler:\n",
    "            return        \n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, \n",
    "            'max', \n",
    "            factor=.5, \n",
    "            patience=self.learning_rate_patience,\n",
    "            cooldown=self.learning_rate_cooldown,\n",
    "            min_lr=self.learning_rate_floor,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def step_learning_rate(self, episode):\n",
    "        window_len = 11\n",
    "        if self.use_lr_scheduler and episode > window_len:    \n",
    "            rewards = np.array(self.episode_rewards[-self.learning_rate_smoothing_window:])\n",
    "            smoothed_reward = smooth(rewards), window_len=window_len).mean()\n",
    "            self.scheduler.step(smoothed_reward)\n",
    "                \n",
    "    def render(self, episode):\n",
    "        if self.render_every and episode % self.render_every == 0:\n",
    "            self.env.render()\n",
    "\n",
    "    def training_progress_report(self, episode):\n",
    "        last_ep = self.episode_rewards[-1]\n",
    "        ten_ep_mean = sum(self.episode_rewards[-10:])/len(self.episode_rewards[-10:])\n",
    "        hundred_ep_mean = sum(self.episode_rewards[-100:])/len(self.episode_rewards[-100:])\n",
    "        try:\n",
    "            lin1_grad = self.online_net.linear1.weight.grad.norm()\n",
    "            lin2_grad = self.online_net.linear2.weight.grad.norm()\n",
    "        except:\n",
    "            lin1_grad =-666\n",
    "            lin2_grad = -666\n",
    "        return f'Ep: {episode} // steps: {self.steps} // last ep reward: {last_ep:.2f} // {min(10, len(self.episode_rewards[-10:]))}-ep mean: {ten_ep_mean:.2f} // {min(100, len(self.episode_rewards[-100:]))}-ep mean: {hundred_ep_mean:.2f}, layer1 grad: {lin1_grad:.2f}, layer2 grad: {lin2_grad:.2f}'\n",
    "\n",
    "    def monitor(self):\n",
    "        if not hasattr(self.target_net, 'monitoring'):\n",
    "            return\n",
    "        if self.monitor_every and self.steps % self.monitor_every == 0:\n",
    "            self.target_net.monitoring(True)\n",
    "        else:\n",
    "            self.target_net.monitoring(False)\n",
    "            \n",
    "    def log_params(self, episode):\n",
    "        if not self.logger:\n",
    "            return     \n",
    "        if episode % 5 != 0:\n",
    "            return \n",
    "        self.logger.add_scalar('train loss', self.training_loss[-1], episode)\n",
    "        self.logger.add_scalar('episode reward', self.episode_rewards[-1], episode)  \n",
    "        self.logger.add_scalar('L1 size ratio', self.layer1_grad_ratio[-1], episode)\n",
    "        self.logger.add_scalar('L2 size ratio', self.layer2_grad_ratio[-1], episode)        \n",
    "        for idx, param in enumerate(self.target_net.parameters()):\n",
    "            if param.dim() == 1:\n",
    "                continue\n",
    "            if not hasattr(param, 'grad'):\n",
    "                continue\n",
    "            self.logger.add_scalar(f'layer {idx//2 + 1} gradient', param.grad.norm(), episode)            \n",
    "\n",
    "    def filter_reward(self, reward):\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        return reward\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.sample()\n",
    "        loss = categorical_vectorized_loss(self.online_net, self.target_net, batch, self.discount)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.online_net.parameters(), self.max_gradient_norm)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()/self.minibatch_size\n",
    "\n",
    "    def train(self, start=0):\n",
    "        for episode in range(start, start + self.num_episodes):\n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_loss = 0\n",
    "            state = torch.tensor(self.env.reset(), dtype=torch.float)\n",
    "            self.target_net_q_values.append(self.target_net.predict_action_values(state).max().item())\n",
    "            if self.logger:\n",
    "                if self.steps == 0:\n",
    "                    self.logger.add_graph(self.target_net, state)            \n",
    "                self.logger.add_scalar('Target net Q values', self.target_net_q_values[-1], episode)                \n",
    "\n",
    "            layer1_size = self.target_net.linear1.weight.norm().item()\n",
    "            layer2_size = self.target_net.linear2.weight.norm().item()                \n",
    "            while not episode_done:\n",
    "                self.monitor()\n",
    "                epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * math.exp(-self.annealing_const * self.steps)\n",
    "                self.steps += 1                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, self.action_shape-1)\n",
    "                else:\n",
    "                    action = self.online_net.get_action(state).item()\n",
    "                self.render(episode)\n",
    "                self.monitor()\n",
    "                state_, reward, episode_done, _ = self.env.step(action)\n",
    "                state_ = torch.tensor(state_, dtype=torch.float)\n",
    "                reward = self.filter_reward(reward)\n",
    "                episode_reward += reward\n",
    "                self.memory.append((state, action, reward, state_, episode_done))\n",
    "                state = state_\n",
    "                if self.steps < 2:\n",
    "                    continue\n",
    "                episode_loss += self.replay()\n",
    "                \n",
    "                if self.steps % self.update_target_every == 0:\n",
    "                    self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "                if episode_done:\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.training_loss.append(episode_loss)\n",
    "                    print(self.training_progress_report(episode), end='\\r', flush=True)\n",
    "\n",
    "                    layer1_ratio = self.target_net.linear1.weight.norm().item() / (layer1_size + 1e-5)\n",
    "                    layer2_ratio = self.target_net.linear2.weight.norm().item() / (layer2_size + 1e-5)\n",
    "                    self.layer1_grad_ratio.append(layer1_ratio)\n",
    "                    self.layer2_grad_ratio.append(layer2_ratio)\n",
    "                                        \n",
    "                    self.log_params(episode)\n",
    "                    self.step_learning_rate(episode)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Works beautifully for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T22:47:23.801618Z",
     "start_time": "2018-08-19T22:47:23.591619Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dcd44c5a6572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m'random_seed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     }\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_experiment' is not defined"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    lr = 0.001\n",
    "    weight_decay = 0.0001\n",
    "    title = f'Run {idx}: LR: {lr:.3f} / L2 weight_decay: {weight_decay:.3f}'\n",
    "    config = {\n",
    "        'title':title,\n",
    "        'log_dir':'tensorboard-data/tuning-categorical',\n",
    "        'random_seed':idx\n",
    "    }\n",
    "    logger, config = init_experiment(config)\n",
    "    print(title)        \n",
    "    print(config['log_dir'])\n",
    "    agent = CategoricalAgent(\n",
    "        cartpole, \n",
    "        learning_rate=lr, \n",
    "        weight_decay=weight_decay, \n",
    "        use_lr_scheduler=True, \n",
    "        logger=logger,\n",
    "        num_episodes=1500,\n",
    "        max_gradient_norm=.5,\n",
    "        monitor_total=5,\n",
    "        use_kaiming_init=True,\n",
    "        minibatch_size=32\n",
    "        \n",
    "    )\n",
    "    monitor_module(\n",
    "        agent.target_net, \n",
    "        logger, \n",
    "        track_data=False,\n",
    "        track_grad=True,\n",
    "        track_update=True,\n",
    "        track_update_ratio=True\n",
    "    )\n",
    "    # agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "However, it's quite unstable in the LunarLander environment. I'm still not sure why -- it may be bugs, it may be hyper-parameters. \n",
    "\n",
    "![Q values](assets/q values.png)\n",
    "![Episode rewards](assets/episode rewards.png)\n",
    "\n",
    "These images show what three typical runs (Different number of epochs, but otherwise similar hyperparameters): the agent's performance is hugely variable, sometimes doing quite well, sometimes doing terribly, with the (heavily) smoothed trend line more or less flat. The Q values seem to converge, but also have high variance. \n",
    "\n",
    "The gradients through the ReLu/LeakyReLu layers tend to be *ridiculously* large. I'm not convinced this is the source of the performance problem in LunarLander, as L2 and gradient-norm clipping are successful at keeping the norm of the weights constant during training, and learning is stable with similarly large gradients in CartPole. \n",
    "\n",
    "I've spent over a week attempting to debug the LunarLander performance problems -- for now I'm putting this project on the back-burner. \n",
    "\n",
    "\n",
    "## Things I tried\n",
    "\n",
    "Singlely and in combinations:\n",
    "\n",
    "- Simple three-armed bandit test to confirm that the agent can learn arbitrary distributions\n",
    "- Careful step-through and analysis of my implementation of the Categorical Algorithm, plus comparing against the output of other people's Categorical Algorithm implementations.\n",
    "- ReLU, PreReLU and LeakyReLU activation functions\n",
    "- No layer initialization, Xavier Normal, Xavier Uniform and Kaiming Normal init\n",
    "- Batch norm layers -- I later found a reference in the [Weight Normalization](https://arxiv.org/abs/1602.07868) paper suggesting that \"the noise introduced by estimating the minibatch statistics destabilizes the [DQN] learning process\".\n",
    "- Weight normalization\n",
    "- Reward clipping (either to [-1, 1], or arbitrary values that seemed reasonable given the histogram of rewards, to to [Vmin, Vmax]).\n",
    "- Exponential reward smoothing and normalization\n",
    "- Different layer architetures\n",
    "- Using a learning rate annealing schedule with several differ hyperparameter settings\n",
    "- Random hyperparameter searches \n",
    "- Probably other things I forgot to include in my log file\n",
    "\n",
    "\n",
    "Additional things I'd *like* to do at some point:\n",
    "- track the magnitude of the policy changes during training. If the policy is changing too much, it probably suggests some kind of learning instability\n",
    "- Visualizing the reward distribution for specific states -- even if this doesn't help with debugging, it'll be **cool**\n",
    "- Bodging in DoubleDQN -- perhaps the problem is simply that the Categorical agent is too optimistic? \n",
    "- Talking with more people about this. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T23:25:28.316093Z",
     "start_time": "2018-08-03T18:40:24.711721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0: LR: 0.001 / L2 weight_decay: 0.000\n",
      "tensorboard-data/tuning-categorical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liavkoren/Envs/ai-gym/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator reshape because torch.onnx.symbolic.reshape does not exist\n",
      "  .format(op_name, op_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   257: reducing learning rate of group 0 to 5.0000e-04.an: -32.08 // 100-ep mean: -26.76, layer1 grad: 0.02, layer2 grad: 0.052\n",
      "Epoch   558: reducing learning rate of group 0 to 2.5000e-04.ean: -38.05 // 100-ep mean: -24.71, layer1 grad: 0.03, layer2 grad: 0.08\n",
      "Epoch   859: reducing learning rate of group 0 to 1.2500e-04.ean: -25.02 // 100-ep mean: -28.37, layer1 grad: 0.02, layer2 grad: 0.037\n",
      "Epoch  1160: reducing learning rate of group 0 to 6.2500e-05.mean: -41.12 // 100-ep mean: -27.48, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "Epoch  1461: reducing learning rate of group 0 to 3.1250e-05.mean: -16.30 // 100-ep mean: -32.09, layer1 grad: 0.01, layer2 grad: 0.025\n",
      "Epoch  1762: reducing learning rate of group 0 to 1.5625e-05.mean: -28.45 // 100-ep mean: -29.21, layer1 grad: 0.02, layer2 grad: 0.04\n",
      "Epoch  2063: reducing learning rate of group 0 to 7.8125e-06.mean: -26.11 // 100-ep mean: -29.50, layer1 grad: 0.02, layer2 grad: 0.04\n",
      "Epoch  2364: reducing learning rate of group 0 to 7.0000e-06.mean: -20.80 // 100-ep mean: -28.30, layer1 grad: 0.02, layer2 grad: 0.052\n",
      "Ep: 2769 // steps: 1001053 // last ep reward: -38.48 // 10-ep mean: -62.89 // 100-ep mean: -55.23, layer1 grad: 0.02, layer2 grad: 0.02\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8e1f9211aa28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrack_update_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-526963fc0efa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mepisode_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-526963fc0efa>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_vectorized_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5e453c1d84cd>\u001b[0m in \u001b[0;36mcategorical_vectorized_loss\u001b[0;34m(online_net, target_net, transitions, discount)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# select only the probabilities distributions for the a_star actions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_star\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mT_zj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0matoms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnot_dones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mb_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT_zj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdelta_z\u001b[0m  \u001b[0;31m# correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    lr = 0.001\n",
    "    weight_decay = 0.0001\n",
    "    title = f'Run {idx}: LR: {lr:.3f} / L2 weight_decay: {weight_decay:.3f}'\n",
    "    config = {\n",
    "        'title':title,\n",
    "        'log_dir':'tensorboard-data/tuning-categorical',\n",
    "        'random_seed':idx\n",
    "    }\n",
    "    logger, config = init_experiment(config)\n",
    "    print(title)        \n",
    "    print(config['log_dir'])\n",
    "    agent = CategoricalAgent(\n",
    "        lunarlander, \n",
    "        learning_rate=lr, \n",
    "        weight_decay=weight_decay, \n",
    "        use_lr_scheduler=True, \n",
    "        logger=logger,\n",
    "        num_episodes=4500,\n",
    "        max_gradient_norm=.5,\n",
    "        monitor_total=5,\n",
    "        use_kaiming_init=True,\n",
    "        minibatch_size=32,\n",
    "        learning_rate_floor=7e-6,\n",
    "        learning_rate_patience=150,\n",
    "        learning_rate_cooldown=150,\n",
    "        update_target_every=500,\n",
    "        learning_rate_smoothing_window=50\n",
    "    )\n",
    "    monitor_module(\n",
    "        agent.target_net, \n",
    "        logger, \n",
    "        track_data=False,\n",
    "        track_grad=True,\n",
    "        track_update=True,\n",
    "        track_update_ratio=True\n",
    "    )\n",
    "    agent.train()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "266px",
    "width": "268px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
