{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro/References\" data-toc-modified-id=\"Intro/References-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro/References</a></span></li><li><span><a href=\"#Scaling-not-clipping?\" data-toc-modified-id=\"Scaling-not-clipping?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scaling not clipping?</a></span></li><li><span><a href=\"#Pytorch\" data-toc-modified-id=\"Pytorch-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pytorch</a></span></li><li><span><a href=\"#Pytorch-monitor\" data-toc-modified-id=\"Pytorch-monitor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pytorch monitor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vectorizing-is-terrible/awesome.\" data-toc-modified-id=\"Vectorizing-is-terrible/awesome.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vectorizing is terrible/awesome.</a></span></li></ul></li><li><span><a href=\"#works-beautifully:\" data-toc-modified-id=\"works-beautifully:-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>works beautifully:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/References\n",
    "\n",
    "A Distributional Perspective on Reinforcement Learning\n",
    "https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\n",
    "https://arxiv.org/abs/1707.06887\n",
    "\n",
    "Distributional Reinforcement Learning with Quantile Regression\n",
    "https://arxiv.org/abs/1710.10044\n",
    "\n",
    "Distributional RL\n",
    "https://mtomassoli.github.io/2017/12/08/distributional_rl/\n",
    "\n",
    "An Analysis of Categorical Distributional Reinforcement Learning\n",
    "https://arxiv.org/abs/1802.08163\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Scaling not clipping?\n",
    "\n",
    "# Pytorch\n",
    "# Pytorch monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T21:32:08.176881Z",
     "start_time": "2018-07-22T21:32:08.090150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import (deque, defaultdict)\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import attr\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_monitor import init_experiment, monitor_module\n",
    "from running_stats import RunningStats\n",
    "from smooth import smooth  # timeseries smoothing function\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cartpole = gym.make('CartPole-v1')\n",
    "lunarlander = gym.make('LunarLander-v2')\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T21:46:43.202898Z",
     "start_time": "2018-07-22T21:46:42.400367Z"
    },
    "code_folding": [
     1,
     22
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Memory(deque):\n",
    "    \"\"\" Experience Replay Memory class. \"\"\"\n",
    "    size = attr.ib()\n",
    "    minibatch_size = attr.ib()\n",
    "\n",
    "    def append(self, thing):\n",
    "        if len(self) > self.size - 1:\n",
    "            self.popleft()\n",
    "        return super().append(thing)\n",
    "\n",
    "    def sample(self):\n",
    "        batch_size = min(len(self), self.minibatch_size)\n",
    "        data = random.sample(self, batch_size)\n",
    "        states = torch.stack([record[0] for record in data])\n",
    "        actions = torch.tensor([record[1] for record in data], dtype=torch.long)\n",
    "        rewards = torch.tensor([record[2] for record in data], dtype=torch.float)\n",
    "        states_ = torch.stack([record[3] for record in data])\n",
    "        dones = torch.tensor([record[4] for record in data], dtype=torch.long)\n",
    "        return (states, actions, rewards, states_, dones)\n",
    "\n",
    "\n",
    "class ValueDistributionDeep(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64, num_hidden2_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(self.state_shape)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.linear2 = nn.Linear(num_hidden1_units, num_hidden2_units)\n",
    "        self.linear3 = nn.Linear(num_hidden2_units, num_hidden2_units)\n",
    "        self.linear4 = nn.Linear(num_hidden2_units, self.action_shape * self.num_atoms)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        if x.dim() > 1:            \n",
    "            x0 = self.batch_norm1(x)\n",
    "        else:\n",
    "            x0 = x            \n",
    "        x1 = F.selu(self.linear1(x0))\n",
    "        x2 = F.selu(self.linear2(x1))\n",
    "        x3 = F.selu(self.linear3(x2))\n",
    "        x4 = self.linear4(x3).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x4, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x0', x0, track_data=True, track_grad=True)\n",
    "            self.monitor('x1', x1, track_data=True, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=True, track_grad=True)\n",
    "            self.monitor('x3', x3, track_data=True, track_grad=True)\n",
    "            self.monitor('x4', x4, track_data=True, track_grad=True)\n",
    "            self.monitor('out', out, track_data=True, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "class ValueDistribution(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64, num_hidden2_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.batch_norm = nn.BatchNorm1d(state_shape)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.linear2 = nn.Linear(num_hidden1_units, self.action_shape * self.num_atoms)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        if x.dim() > 1:\n",
    "            x0 = self.batch_norm(x)\n",
    "        else:\n",
    "            x0 = x\n",
    "        x1 = F.relu(self.linear1(x0))\n",
    "        x2 = self.linear2(x1).reshape(-1, self.action_shape, self.num_atoms)\n",
    "        out = F.softmax(x2, dim=2)  # (actions x atoms)\n",
    "        if x.dim() == 1:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x0', x0, track_data=False, track_grad=True)\n",
    "            self.monitor('x1', x1, track_data=False, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=False, track_grad=True)\n",
    "            self.monitor('out', out, track_data=False, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T04:30:22.365128Z",
     "start_time": "2018-07-22T04:30:22.137054Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ValueDistributionDeepBatch(torch.nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, vmin, vmax, num_atoms=51, num_hidden1_units=64, num_hidden2_units=64):\n",
    "        super().__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.num_atoms = num_atoms\n",
    "        self.atoms = torch.linspace(self.vmin, self.vmax, self.num_atoms)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(self.state_shape)\n",
    "        self.linear1 = nn.Linear(self.state_shape, num_hidden1_units)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(num_hidden1_units)        \n",
    "        self.linear2 = nn.Linear(num_hidden1_units, num_hidden2_units)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(num_hidden2_units)        \n",
    "        self.linear3 = nn.Linear(num_hidden2_units, num_hidden2_units)        \n",
    "        self.linear4 = nn.Linear(num_hidden2_units, self.action_shape * self.num_atoms)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return (actions x atoms). \"\"\"\n",
    "        if x.dim() > 1:            \n",
    "            bn1 = self.batch_norm1(x)\n",
    "            x1 = F.relu(self.linear1(bn1))\n",
    "            bn2 = self.batch_norm2(x1)\n",
    "            x2 = F.relu(self.linear2(bn2))\n",
    "            bn3 = self.batch_norm3(x2)\n",
    "            x3 = F.relu(self.linear3(bn3))\n",
    "            x4 = self.linear4(x3).reshape(-1, self.action_shape, self.num_atoms)            \n",
    "            batch_size = x.size(0)\n",
    "        else:\n",
    "            x0 = x            \n",
    "            x1 = F.relu(self.linear1(x0))\n",
    "            x2 = F.relu(self.linear2(x1))\n",
    "            x3 = F.relu(self.linear3(x2))\n",
    "            x4 = self.linear4(x3).reshape(-1, self.action_shape, self.num_atoms)\n",
    "            batch_size = 1\n",
    "        out = F.softmax(x4, dim=2)  # (actions x atoms)\n",
    "        assert out.size() == torch.Size((batch_size, self.action_shape, self.num_atoms))\n",
    "        if hasattr(self, 'monitor'):\n",
    "            self.monitor('x1', x1, track_data=True, track_grad=True)\n",
    "            self.monitor('x2', x2, track_data=True, track_grad=True)\n",
    "            self.monitor('x3', x3, track_data=True, track_grad=True)\n",
    "            self.monitor('x4', x4, track_data=True, track_grad=True)\n",
    "            self.monitor('out', out, track_data=True, track_grad=True)\n",
    "        return out\n",
    "    \n",
    "    def predict_action_values(self, states):\n",
    "        \"\"\" Return (batch-size x actions). \"\"\"\n",
    "        distribution = self.forward(states)\n",
    "        weighted_distribution = distribution * self.atoms\n",
    "        out = weighted_distribution.sum(dim=2).squeeze()  # (batch-size x actions)\n",
    "        dims = states.dim()\n",
    "        assert out.size() == torch.Size((self.action_shape,))\n",
    "        return out\n",
    "        \n",
    "    def get_action(self, state):        \n",
    "        values = self.predict_action_values(state)\n",
    "        action = values.argmax()\n",
    "        return action\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T19:19:40.302183Z",
     "start_time": "2018-07-19T19:19:40.113459Z"
    }
   },
   "source": [
    "## Vectorizing is terrible/awesome.\n",
    "\n",
    "This is the algo we'll be implementing. \n",
    "\n",
    "![The C51 algorithm](assets/C51-algo.png)\n",
    "\n",
    "Vectorizing this code is *very* important. Even on my lowely macbook, the fully vectorized version of this algorithm that accepts a minibatch runs about *30 times* faster than a naive implementation that's called inside a loop. This cashes out to 1000 training episodes of LunarLander in about 10 minutes, verses five hours. \n",
    "\n",
    "My process for vectoring this code was.. to sort of squint at it. \n",
    "Seriously. I wasn't even sure if I should first generalize it to accept minibatch tensors and then remove the loop, or vice versa. \n",
    "\n",
    "Squinting at it, thought, (and stepping through it line by line in Jupyter a few times), it became clear that the would actually tricky to vectorize: \n",
    "\n",
    "![The bastard lines]](assets/C51-algo-large.png)\n",
    "\n",
    "I decided to take the easy wins first, and first converted the `categorical_loss` function to accept minibatches first. This is straighforward, mostly just reshaping and expanding tensors. Pytorch's `squeeze` and `unsqueeze` methods have fun names and are great for this. \n",
    "\n",
    "Those two lines, though, were bloody horrible. \n",
    "\n",
    "They ended up cashing out into the following dense six lines of python:\n",
    "\n",
    "```python\n",
    "offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "m += lo_component.resize_as(m) + hi_component.resize_as(m)       \n",
    "```\n",
    "\n",
    "The main insight is that the `lo` and `hi` tensors contain routing information. They tend look like this:\n",
    "\n",
    "```\n",
    "lo:\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 10],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]])\n",
    "hi:\n",
    "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 4, 5, 6, 7, 7, 8, 9, 10, 10, 10],\n",
    "        [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10]])\n",
    "```\n",
    "\n",
    "This is for a three-transition test minibatch. The values are the target indicies for where probability needs to accumulate inside `m` our new probability-mass tensor.\n",
    "\n",
    "Eventually after squinting a lot at the PyTorch docs, I figured out that PyTorch's [`index_add`](https://pytorch.org/docs/stable/tensors.html?highlight=index_add#torch.Tensor.index_add_) method would do the trick. \n",
    "\n",
    "Usings `index_add` requires that all the tensors be unrolled, which is why we need index-offsets. Put it together, and you're done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T21:55:25.305669Z",
     "start_time": "2018-07-22T21:55:25.208206Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def categorical_vectorized_loss(online_net, target_net, transitions, discount): \n",
    "    states, actions, rewards, states_, dones = transitions\n",
    "    not_dones = (1 - dones).type(torch.FloatTensor)\n",
    "    atoms = target_net.atoms\n",
    "    probabilities = target_net.forward(states_)\n",
    "    Q_x_ = (probabilities * atoms).sum(2)\n",
    "    batch_size = states.shape[0]\n",
    "    assert Q_x_.shape == torch.Size((batch_size, target_net.action_shape)), f'Got: {Q_x_.shape}, expected: {(batch_size, target_net.action_shape)}'\n",
    "    a_star = Q_x_.argmax(dim=1) \n",
    "    assert a_star.shape == torch.Size((batch_size,)), f'Got {a_star.shape}, expected: ((batch_size,))'\n",
    "    \n",
    "    # compute the projected probability:\n",
    "    delta_z = (target_net.vmax - target_net.vmin)/(target_net.num_atoms - 1)    \n",
    "    # select only the probabilities distributions for the a_star actions:\n",
    "    probabilities = probabilities[range(batch_size), a_star]\n",
    "    T_zj = rewards.unsqueeze(1) + discount * atoms * not_dones.unsqueeze(1)\n",
    "    b_j = (T_zj.clamp(target_net.vmin, target_net.vmax) - target_net.vmin) / delta_z  # correct    \n",
    "    lo = b_j.floor()        \n",
    "    hi = b_j.ceil()\n",
    "    m = torch.zeros(batch_size, target_net.num_atoms, dtype=torch.float)\n",
    "    lo_component = torch.zeros_like(m.view(-1))\n",
    "    hi_component = torch.zeros_like(m.view(-1))\n",
    "    # offset will be used for indexing when we flatten the tensors into vectors:\n",
    "    offset_bound = target_net.num_atoms * batch_size - target_net.num_atoms\n",
    "    idx_offset = torch.range(0, offset_bound, target_net.num_atoms).unsqueeze(1).expand_as(m)\n",
    "    lo_idx = (lo + idx_offset).view(-1).type(torch.long)\n",
    "    hi_idx = (hi + idx_offset).view(-1).type(torch.long)\n",
    "    lo_component = m.view(-1).index_add(0, lo_idx, (probabilities * (hi - b_j)).view(-1) )\n",
    "    hi_component = m.view(-1).index_add(0, hi_idx, (probabilities * (b_j - lo)).view(-1) )\n",
    "    m += lo_component.reshape(batch_size, target_net.num_atoms) + hi_component.reshape(batch_size, target_net.num_atoms)\n",
    "    # cross enthropy is Sigma <true> log <unnatural>, so for us is: target log(online)\n",
    "    online_distribution = online_net.forward(states)[range(batch_size), actions]\n",
    "    return -( m * online_distribution.log() ).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T03:13:33.751618Z",
     "start_time": "2018-07-23T03:13:32.565481Z"
    },
    "code_folding": [
     91
    ]
   },
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class CategoricalAgent:\n",
    "    env = attr.ib()\n",
    "    discount = attr.ib(default=0.99)\n",
    "    epsilon_max = attr.ib(default=1.0)\n",
    "    epsilon_min = attr.ib(default=0.01)\n",
    "    annealing_const = attr.ib(default=.001)  # aka Lambda\n",
    "    minibatch_size = attr.ib(default=32)\n",
    "    memory_size = attr.ib(default=int(1e6))\n",
    "    num_episodes = attr.ib(default=1000)  # num of episodes in a training epoch\n",
    "    render_every = attr.ib(default=20)  # set to zero to turn off rendering\n",
    "    update_target_every = attr.ib(default=200)\n",
    "    vmin = attr.ib(default=-10)\n",
    "    vmax = attr.ib(default=10)\n",
    "    num_atoms = attr.ib(default=51)\n",
    "    learning_rate = attr.ib(default=0.000001)\n",
    "    monitor_total = attr.ib(default=10)\n",
    "    logger = attr.ib(default=None)\n",
    "    xavier = attr.ib(default=True)\n",
    "    weight_decay = attr.ib(default=0)\n",
    "    use_lr_scheduler = attr.ib(default=True)\n",
    "    online_net = attr.ib(default=None)\n",
    "    target_net = attr.ib(default=None)\n",
    "    max_gradient_norm = attr.ib(default=10)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        self.steps = 0\n",
    "        state_shape = self.env.observation_space.shape[0]\n",
    "        self.memory = Memory(self.memory_size, self.minibatch_size)\n",
    "        self.action_shape = self.env.action_space.n\n",
    "        if self.online_net is None or self.target_net is None:\n",
    "            print('using default ValDist class')\n",
    "            self.online_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "            self.target_net = ValueDistribution(state_shape=state_shape, action_shape=self.action_shape, vmin=self.vmin, vmax=self.vmax, num_atoms=self.num_atoms)\n",
    "        if self.xavier:\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "            for param in self.online_net.parameters():\n",
    "                if param.dim() < 2:\n",
    "                    continue\n",
    "                nn.init.xavier_uniform_(param, gain=gain)        \n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.steps = 0\n",
    "        self.target_net_q_values = []\n",
    "        self.episode_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.monitor_every = self.num_episodes//self.monitor_total\n",
    "        self.setup_lr_scheduler()\n",
    "        self.reward_normalizer = RunningStats((),)\n",
    "        self.filter_reward_buffer = 0\n",
    "    \n",
    "    def setup_lr_scheduler(self):\n",
    "        if not self.use_lr_scheduler:\n",
    "            return        \n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, \n",
    "            'max', \n",
    "            factor=.5, \n",
    "            patience=150,\n",
    "            cooldown=150,\n",
    "            min_lr=2e-05,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def step_learning_rate(self, episode):\n",
    "        if self.use_lr_scheduler and episode > 11:                                \n",
    "            smoothed_reward = smooth(np.array(self.episode_rewards[-50:])).mean()\n",
    "            self.scheduler.step(smoothed_reward)\n",
    "        \n",
    "        \n",
    "    def render(self, episode):\n",
    "        if self.render_every and episode % self.render_every == 0:\n",
    "            self.env.render()\n",
    "\n",
    "    def training_progress_report(self, episode):\n",
    "        last_ep = self.episode_rewards[-1]\n",
    "        ten_ep_mean = sum(self.episode_rewards[-10:])/len(self.episode_rewards[-10:])\n",
    "        hundred_ep_mean = sum(self.episode_rewards[-100:])/len(self.episode_rewards[-100:])\n",
    "        lin1_grad = self.online_net.linear1.weight.grad.norm()\n",
    "        lin2_grad = self.online_net.linear2.weight.grad.norm()\n",
    "        return f'Ep: {episode} // steps: {self.steps} // last ep reward: {last_ep:.2f} // {min(10, len(self.episode_rewards[-10:]))}-ep mean: {ten_ep_mean:.2f} // {min(100, len(self.episode_rewards[-100:]))}-ep mean: {hundred_ep_mean:.2f}, layer1 grad: {lin1_grad:.2f}, layer2 grad: {lin2_grad:.2f}'\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.sample()\n",
    "        loss = categorical_vectorized_loss(self.online_net, self.target_net, batch, self.discount)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.online_net.parameters(), self.max_gradient_norm)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()/self.minibatch_size\n",
    "\n",
    "    def monitor(self):\n",
    "        if not hasattr(self.target_net, 'monitoring'):\n",
    "            return\n",
    "        if self.monitor_every and self.steps % self.monitor_every == 0:\n",
    "            self.target_net.monitoring(True)\n",
    "        else:\n",
    "            self.target_net.monitoring(False)\n",
    "\n",
    "    def filter_reward(self, reward):\n",
    "        if self.filter_reward_buffer == 0:\n",
    "            print('using filtered rewards')\n",
    "        reward = np.clip(reward, self.vmin, self.vmax)\n",
    "        gamma = 0.95\n",
    "        eps = 1e-5        \n",
    "        self.filter_reward_buffer *= gamma\n",
    "        self.filter_reward_buffer += reward\n",
    "        self.reward_normalizer.append(self.filter_reward_buffer)\n",
    "        return (reward - self.reward_normalizer.mean) / np.sqrt(self.reward_normalizer.var + eps)\n",
    "                \n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_loss = 0\n",
    "            state = torch.tensor(self.env.reset(), dtype=torch.float)\n",
    "            self.target_net_q_values.append(self.target_net.predict_action_values(state).max().item())\n",
    "            if self.logger:\n",
    "                if self.steps == 0:\n",
    "                    self.logger.add_graph(self.target_net, state)            \n",
    "                self.logger.add_scalar('Target net Q values', self.target_net_q_values[-1], episode)                \n",
    "            while not episode_done:\n",
    "                self.monitor()\n",
    "                epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * math.exp(-self.annealing_const * self.steps)\n",
    "                self.steps += 1                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, self.action_shape-1)\n",
    "                else:\n",
    "                    action = self.online_net.get_action(state).item()\n",
    "                self.render(episode)\n",
    "                self.monitor()\n",
    "                state_, reward, episode_done, _ = self.env.step(action)\n",
    "                state_ = torch.tensor(state_, dtype=torch.float)\n",
    "                reward = self.filter_reward(reward)\n",
    "                episode_reward += reward\n",
    "                self.memory.append((state, action, reward, state_, episode_done))\n",
    "                state = state_\n",
    "                if self.steps < 2:\n",
    "                    continue\n",
    "                episode_loss += self.replay()\n",
    "                \n",
    "                if self.steps % self.update_target_every == 0:\n",
    "                    self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "                if episode_done:\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.training_loss.append(episode_loss)\n",
    "                    print(self.training_progress_report(episode), end='\\r', flush=True)\n",
    "                    self.log_params(episode)\n",
    "                    self.step_learning_rate(episode)\n",
    "        #self.env.close()\n",
    "        \n",
    "    def log_params(self, episode):\n",
    "        if not self.logger:\n",
    "            return     \n",
    "        if episode % 5 != 0:\n",
    "            return \n",
    "        self.logger.add_scalar('train loss', self.training_loss[-1], episode)\n",
    "        self.logger.add_scalar('episode reward', self.episode_rewards[-1], episode)  \n",
    "        for idx, param in enumerate(self.target_net.parameters()):\n",
    "            if param.dim() == 1:\n",
    "                continue\n",
    "            if not hasattr(param, 'grad'):\n",
    "                continue\n",
    "            self.logger.add_scalar(f'layer {idx/2} gradient', param.grad.norm(), episode)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T13:32:56.803122Z",
     "start_time": "2018-07-21T02:09:39.002013Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.104 / L2 reg: 0.003 / Xavier: True\n",
      "\n",
      "Epoch   211: reducing learning rate of group 0 to 5.2020e-02.ean: -273.41 // 100-ep mean: -277.13, layer1 grad: 0.01, layer2 grad: 0.04\n",
      "Epoch   512: reducing learning rate of group 0 to 2.6010e-02.ean: -374.15 // 100-ep mean: -382.50, layer1 grad: 0.01, layer2 grad: 0.033\n",
      "Epoch   813: reducing learning rate of group 0 to 1.3005e-02.ean: -294.03 // 100-ep mean: -400.02, layer1 grad: 0.01, layer2 grad: 0.06\n",
      "Epoch  1114: reducing learning rate of group 0 to 6.5025e-03. mean: -465.89 // 100-ep mean: -424.55, layer1 grad: 0.00, layer2 grad: 0.03\n",
      "Epoch  1415: reducing learning rate of group 0 to 3.2512e-03. mean: -371.55 // 100-ep mean: -427.94, layer1 grad: 0.01, layer2 grad: 0.043\n",
      "LR: 0.337 / L2 reg: 0.275 / Xavier: Trueard: -674.70 // 10-ep mean: -471.48 // 100-ep mean: -491.38, layer1 grad: 0.01, layer2 grad: 0.033\n",
      "\n",
      "Epoch   235: reducing learning rate of group 0 to 1.6829e-01.ean: -220.77 // 100-ep mean: -243.58, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   536: reducing learning rate of group 0 to 8.4145e-02.ean: -280.56 // 100-ep mean: -247.75, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   837: reducing learning rate of group 0 to 4.2072e-02.ean: -279.78 // 100-ep mean: -246.48, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1138: reducing learning rate of group 0 to 2.1036e-02. mean: -296.09 // 100-ep mean: -250.54, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1439: reducing learning rate of group 0 to 1.0518e-02. mean: -300.46 // 100-ep mean: -277.06, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.002 / L2 reg: 0.000 / Xavier: Trueard: -341.12 // 10-ep mean: -276.30 // 100-ep mean: -300.19, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   206: reducing learning rate of group 0 to 1.1579e-03.ean: -361.55 // 100-ep mean: -354.07, layer1 grad: 0.07, layer2 grad: 0.10\n",
      "Epoch   507: reducing learning rate of group 0 to 5.7893e-04.mean: -381.58 // 100-ep mean: -296.21, layer1 grad: 0.09, layer2 grad: 0.10\n",
      "Epoch   892: reducing learning rate of group 0 to 2.8947e-04.mean: -186.93 // 100-ep mean: -240.26, layer1 grad: 0.04, layer2 grad: 0.07\n",
      "Epoch  1193: reducing learning rate of group 0 to 1.4473e-04. mean: -236.37 // 100-ep mean: -236.61, layer1 grad: 0.05, layer2 grad: 0.15\n",
      "LR: 0.028 / L2 reg: 8.641 / Xavier: Trueard: -311.17 // 10-ep mean: -325.97 // 100-ep mean: -299.85, layer1 grad: 0.05, layer2 grad: 0.06\n",
      "\n",
      "Epoch   360: reducing learning rate of group 0 to 1.4149e-02.ean: -233.18 // 100-ep mean: -238.47, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   661: reducing learning rate of group 0 to 7.0743e-03.ean: -273.40 // 100-ep mean: -247.38, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   962: reducing learning rate of group 0 to 3.5372e-03.mean: -314.93 // 100-ep mean: -257.33, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1263: reducing learning rate of group 0 to 1.7686e-03. mean: -293.15 // 100-ep mean: -241.76, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.011 / L2 reg: 0.001 / Xavier: Trueard: -110.80 // 10-ep mean: -227.34 // 100-ep mean: -283.47, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 5.3743e-03.ean: -315.43 // 100-ep mean: -285.61, layer1 grad: 0.03, layer2 grad: 0.06\n",
      "Epoch   614: reducing learning rate of group 0 to 2.6872e-03.ean: -257.83 // 100-ep mean: -280.97, layer1 grad: 0.02, layer2 grad: 0.04\n",
      "Epoch  1152: reducing learning rate of group 0 to 1.3436e-03. mean: -227.91 // 100-ep mean: -252.26, layer1 grad: 0.03, layer2 grad: 0.04\n",
      "Epoch  1453: reducing learning rate of group 0 to 6.7179e-04. mean: -262.22 // 100-ep mean: -270.04, layer1 grad: 0.05, layer2 grad: 0.06\n",
      "LR: 0.001 / L2 reg: 0.000 / Xavier: Trueard: -219.92 // 10-ep mean: -248.94 // 100-ep mean: -260.65, layer1 grad: 0.05, layer2 grad: 0.05\n",
      "\n",
      "Epoch   202: reducing learning rate of group 0 to 2.7806e-04.ean: -809.45 // 100-ep mean: -730.02, layer1 grad: 0.05, layer2 grad: 0.045\n",
      "Epoch   503: reducing learning rate of group 0 to 1.3903e-04.ean: -522.61 // 100-ep mean: -534.14, layer1 grad: 0.08, layer2 grad: 0.105\n",
      "Epoch   804: reducing learning rate of group 0 to 6.9516e-05.mean: -404.72 // 100-ep mean: -386.29, layer1 grad: 0.10, layer2 grad: 0.09\n",
      "Epoch  1105: reducing learning rate of group 0 to 3.4758e-05. mean: -349.64 // 100-ep mean: -369.47, layer1 grad: 0.15, layer2 grad: 0.12\n",
      "Epoch  1406: reducing learning rate of group 0 to 1.7379e-05. mean: -300.76 // 100-ep mean: -350.03, layer1 grad: 0.12, layer2 grad: 0.13\n",
      "LR: 0.000 / L2 reg: 0.064 / Xavier: Trueard: -322.63 // 10-ep mean: -350.15 // 100-ep mean: -340.82, layer1 grad: 0.07, layer2 grad: 0.19\n",
      "\n",
      "Epoch   205: reducing learning rate of group 0 to 2.1658e-04.ean: -374.16 // 100-ep mean: -427.14, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch   506: reducing learning rate of group 0 to 1.0829e-04.ean: -485.31 // 100-ep mean: -422.73, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch   807: reducing learning rate of group 0 to 5.4146e-05.ean: -248.14 // 100-ep mean: -432.08, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch  1108: reducing learning rate of group 0 to 2.7073e-05.mean: -565.76 // 100-ep mean: -430.35, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch  1409: reducing learning rate of group 0 to 1.3537e-05. mean: -515.10 // 100-ep mean: -419.97, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "LR: 0.004 / L2 reg: 0.000 / Xavier: Trueard: -299.46 // 10-ep mean: -364.22 // 100-ep mean: -448.85, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 2.2482e-03.ean: -598.55 // 100-ep mean: -682.54, layer1 grad: 0.02, layer2 grad: 0.023\n",
      "Epoch   502: reducing learning rate of group 0 to 1.1241e-03.ean: -679.49 // 100-ep mean: -719.41, layer1 grad: 0.05, layer2 grad: 0.084\n",
      "Epoch   803: reducing learning rate of group 0 to 5.6206e-04.mean: -860.30 // 100-ep mean: -704.42, layer1 grad: 0.06, layer2 grad: 0.05\n",
      "Epoch  1104: reducing learning rate of group 0 to 2.8103e-04.p mean: -836.88 // 100-ep mean: -708.90, layer1 grad: 0.05, layer2 grad: 0.11\n",
      "Epoch  1405: reducing learning rate of group 0 to 1.4052e-04. mean: -699.32 // 100-ep mean: -694.66, layer1 grad: 0.09, layer2 grad: 0.115\n",
      "LR: 0.006 / L2 reg: 0.082 / Xavier: Trueard: -795.93 // 10-ep mean: -752.22 // 100-ep mean: -694.59, layer1 grad: 0.07, layer2 grad: 0.086\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 3.1404e-03.ean: -440.70 // 100-ep mean: -330.40, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   502: reducing learning rate of group 0 to 1.5702e-03.ean: -321.48 // 100-ep mean: -335.73, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   803: reducing learning rate of group 0 to 7.8511e-04.ean: -345.37 // 100-ep mean: -335.81, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1104: reducing learning rate of group 0 to 3.9256e-04.mean: -384.43 // 100-ep mean: -408.97, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch  1405: reducing learning rate of group 0 to 1.9628e-04. mean: -364.90 // 100-ep mean: -432.72, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "LR: 0.080 / L2 reg: 0.000 / Xavier: Trueard: -388.48 // 10-ep mean: -479.86 // 100-ep mean: -405.68, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 4.0134e-02.ean: -626.09 // 100-ep mean: -580.23, layer1 grad: 0.05, layer2 grad: 0.033\n",
      "Epoch   502: reducing learning rate of group 0 to 2.0067e-02.ean: -226.10 // 100-ep mean: -383.09, layer1 grad: 0.01, layer2 grad: 0.0212\n",
      "Epoch   803: reducing learning rate of group 0 to 1.0033e-02.an: -434.03 // 100-ep mean: -331.40, layer1 grad: 0.05, layer2 grad: 0.0341\n",
      "Epoch  1104: reducing learning rate of group 0 to 5.0167e-03.mean: -325.53 // 100-ep mean: -348.19, layer1 grad: 0.05, layer2 grad: 0.03\n",
      "Epoch  1405: reducing learning rate of group 0 to 2.5083e-03. mean: -492.56 // 100-ep mean: -451.20, layer1 grad: 0.05, layer2 grad: 0.0544\n",
      "LR: 0.457 / L2 reg: 0.012 / Xavier: Trueard: -198.57 // 10-ep mean: -493.22 // 100-ep mean: -431.44, layer1 grad: 0.04, layer2 grad: 0.054\n",
      "\n",
      "Epoch   307: reducing learning rate of group 0 to 2.2842e-01.ean: -171.81 // 100-ep mean: -177.05, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   621: reducing learning rate of group 0 to 1.1421e-01.ean: -173.90 // 100-ep mean: -176.25, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   922: reducing learning rate of group 0 to 5.7105e-02.ean: -159.65 // 100-ep mean: -178.26, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1381: reducing learning rate of group 0 to 2.8552e-02.mean: -174.39 // 100-ep mean: -172.43, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.399 / L2 reg: 0.247 / Xavier: Trueard: -167.86 // 10-ep mean: -205.61 // 100-ep mean: -174.90, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   249: reducing learning rate of group 0 to 1.9935e-01.ean: -240.50 // 100-ep mean: -243.68, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   550: reducing learning rate of group 0 to 9.9677e-02.ean: -294.89 // 100-ep mean: -239.14, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   851: reducing learning rate of group 0 to 4.9839e-02.ean: -257.10 // 100-ep mean: -264.89, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1152: reducing learning rate of group 0 to 2.4919e-02. mean: -246.09 // 100-ep mean: -263.34, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1453: reducing learning rate of group 0 to 1.2460e-02. mean: -283.95 // 100-ep mean: -256.52, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.000 / L2 reg: 0.089 / Xavier: Trueard: -513.83 // 10-ep mean: -345.42 // 100-ep mean: -259.06, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   204: reducing learning rate of group 0 to 1.1088e-04.ean: -408.56 // 100-ep mean: -452.25, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch   505: reducing learning rate of group 0 to 5.5440e-05.ean: -519.47 // 100-ep mean: -440.11, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch   806: reducing learning rate of group 0 to 2.7720e-05.ean: -424.13 // 100-ep mean: -442.79, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch  1107: reducing learning rate of group 0 to 1.3860e-05.mean: -360.96 // 100-ep mean: -422.40, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch  1408: reducing learning rate of group 0 to 6.9300e-06. mean: -401.40 // 100-ep mean: -518.30, layer1 grad: 0.00, layer2 grad: 0.0000\n",
      "LR: 0.102 / L2 reg: 3.217 / Xavier: Trueard: -421.79 // 10-ep mean: -403.33 // 100-ep mean: -466.69, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "\n",
      "Epoch   214: reducing learning rate of group 0 to 5.0847e-02.ean: -233.68 // 100-ep mean: -230.70, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   515: reducing learning rate of group 0 to 2.5424e-02.ean: -270.09 // 100-ep mean: -240.26, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   816: reducing learning rate of group 0 to 1.2712e-02.ean: -266.28 // 100-ep mean: -243.47, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1117: reducing learning rate of group 0 to 6.3559e-03. mean: -217.02 // 100-ep mean: -250.83, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1418: reducing learning rate of group 0 to 3.1779e-03. mean: -218.47 // 100-ep mean: -260.98, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.013 / L2 reg: 0.002 / Xavier: Trueard: -440.83 // 10-ep mean: -344.51 // 100-ep mean: -273.54, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   259: reducing learning rate of group 0 to 6.4623e-03.ean: -249.67 // 100-ep mean: -277.06, layer1 grad: 0.05, layer2 grad: 0.05\n",
      "Epoch   586: reducing learning rate of group 0 to 3.2312e-03.ean: -243.15 // 100-ep mean: -278.28, layer1 grad: 0.03, layer2 grad: 0.04\n",
      "Epoch   887: reducing learning rate of group 0 to 1.6156e-03.ean: -315.05 // 100-ep mean: -273.75, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "Epoch  1323: reducing learning rate of group 0 to 8.0779e-04.mean: -238.28 // 100-ep mean: -284.86, layer1 grad: 0.02, layer2 grad: 0.023\n",
      "LR: 0.008 / L2 reg: 0.000 / Xavier: Trueard: -383.64 // 10-ep mean: -353.18 // 100-ep mean: -278.05, layer1 grad: 0.03, layer2 grad: 0.03\n",
      "\n",
      "Epoch   389: reducing learning rate of group 0 to 3.7689e-03.mean: -216.00 // 100-ep mean: -202.27, layer1 grad: 0.12, layer2 grad: 0.09\n",
      "Epoch   690: reducing learning rate of group 0 to 1.8844e-03.mean: -243.36 // 100-ep mean: -204.27, layer1 grad: 0.07, layer2 grad: 0.05\n",
      "Epoch   991: reducing learning rate of group 0 to 9.4222e-04. mean: -205.98 // 100-ep mean: -215.55, layer1 grad: 0.08, layer2 grad: 0.08\n",
      "Epoch  1292: reducing learning rate of group 0 to 4.7111e-04. mean: -247.27 // 100-ep mean: -243.13, layer1 grad: 0.08, layer2 grad: 0.08\n",
      "LR: 0.976 / L2 reg: 0.063 / Xavier: Trueward: -209.44 // 10-ep mean: -248.27 // 100-ep mean: -236.84, layer1 grad: 0.10, layer2 grad: 0.12\n",
      "\n",
      "Epoch   368: reducing learning rate of group 0 to 4.8776e-01.ean: -174.57 // 100-ep mean: -181.63, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   669: reducing learning rate of group 0 to 2.4388e-01.ean: -173.38 // 100-ep mean: -177.56, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   970: reducing learning rate of group 0 to 1.2194e-01.ean: -187.57 // 100-ep mean: -182.89, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1272: reducing learning rate of group 0 to 6.0969e-02.mean: -196.37 // 100-ep mean: -178.42, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.976 / L2 reg: 0.104 / Xavier: Trueard: -156.23 // 10-ep mean: -169.91 // 100-ep mean: -176.04, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 4.8795e-01.ean: -171.80 // 100-ep mean: -177.33, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   683: reducing learning rate of group 0 to 2.4398e-01.an: -161.77 // 100-ep mean: -179.43, layer1 grad: nan, layer2 grad: nann\n",
      "Epoch  1094: reducing learning rate of group 0 to 1.2199e-01.mean: -178.22 // 100-ep mean: -178.99, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1395: reducing learning rate of group 0 to 6.0994e-02.mean: -179.06 // 100-ep mean: -178.46, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.851 / L2 reg: 0.003 / Xavier: Trueard: -162.78 // 10-ep mean: -165.92 // 100-ep mean: -176.49, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   573: reducing learning rate of group 0 to 4.2549e-01.ean: -186.54 // 100-ep mean: -175.74, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   874: reducing learning rate of group 0 to 2.1274e-01.ean: -179.28 // 100-ep mean: -192.23, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1175: reducing learning rate of group 0 to 1.0637e-01.mean: -173.83 // 100-ep mean: -182.56, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1476: reducing learning rate of group 0 to 5.3186e-02. mean: -185.14 // 100-ep mean: -177.33, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.006 / L2 reg: 0.000 / Xavier: Trueard: -175.80 // 10-ep mean: -187.79 // 100-ep mean: -177.91, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 2.8933e-03.ean: -711.37 // 100-ep mean: -657.41, layer1 grad: 0.01, layer2 grad: 0.022\n",
      "Epoch   502: reducing learning rate of group 0 to 1.4466e-03.ean: -524.35 // 100-ep mean: -564.17, layer1 grad: 0.11, layer2 grad: 0.065\n",
      "Epoch   803: reducing learning rate of group 0 to 7.2332e-04.ean: -445.64 // 100-ep mean: -477.06, layer1 grad: 0.04, layer2 grad: 0.04\n",
      "Epoch  1217: reducing learning rate of group 0 to 3.6166e-04. mean: -595.56 // 100-ep mean: -485.07, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "LR: 0.023 / L2 reg: 0.000 / Xavier: Trueard: -330.19 // 10-ep mean: -456.46 // 100-ep mean: -469.08, layer1 grad: 0.03, layer2 grad: 0.03\n",
      "\n",
      "Epoch   372: reducing learning rate of group 0 to 1.1740e-02.mean: -350.90 // 100-ep mean: -287.91, layer1 grad: 0.04, layer2 grad: 0.05\n",
      "Epoch   673: reducing learning rate of group 0 to 5.8699e-03.mean: -480.61 // 100-ep mean: -449.25, layer1 grad: 0.05, layer2 grad: 0.090\n",
      "Epoch   974: reducing learning rate of group 0 to 2.9349e-03.ean: -154.41 // 100-ep mean: -237.58, layer1 grad: 0.09, layer2 grad: 0.1093\n",
      "Epoch  1275: reducing learning rate of group 0 to 1.4675e-03. mean: -297.01 // 100-ep mean: -246.62, layer1 grad: 0.08, layer2 grad: 0.11\n",
      "LR: 0.113 / L2 reg: 0.000 / Xavier: Trueard: -585.03 // 10-ep mean: -530.48 // 100-ep mean: -360.48, layer1 grad: 0.08, layer2 grad: 0.218\n",
      "\n",
      "Epoch   365: reducing learning rate of group 0 to 5.6417e-02.ean: -179.87 // 100-ep mean: -180.16, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   857: reducing learning rate of group 0 to 2.8209e-02.ean: -181.34 // 100-ep mean: -183.15, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1158: reducing learning rate of group 0 to 1.4104e-02.mean: -192.95 // 100-ep mean: -179.00, layer1 grad: nan, layer2 grad: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1459: reducing learning rate of group 0 to 7.0521e-03. mean: -191.72 // 100-ep mean: -187.94, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.003 / L2 reg: 0.004 / Xavier: Trueard: -142.80 // 10-ep mean: -164.62 // 100-ep mean: -183.56, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 1.4328e-03.mean: -510.53 // 100-ep mean: -528.55, layer1 grad: 0.01, layer2 grad: 0.011\n",
      "Epoch   502: reducing learning rate of group 0 to 7.1638e-04.ean: -395.94 // 100-ep mean: -364.88, layer1 grad: 0.00, layer2 grad: 0.033\n",
      "Epoch   803: reducing learning rate of group 0 to 3.5819e-04.ean: -369.99 // 100-ep mean: -486.23, layer1 grad: 0.01, layer2 grad: 0.043\n",
      "Epoch  1104: reducing learning rate of group 0 to 1.7909e-04.mean: -327.42 // 100-ep mean: -481.87, layer1 grad: 0.00, layer2 grad: 0.063\n",
      "Epoch  1405: reducing learning rate of group 0 to 8.9547e-05. mean: -382.91 // 100-ep mean: -512.59, layer1 grad: 0.01, layer2 grad: 0.035\n",
      "LR: 0.000 / L2 reg: 0.000 / Xavier: Trueard: -570.25 // 10-ep mean: -591.60 // 100-ep mean: -564.95, layer1 grad: 0.01, layer2 grad: 0.033\n",
      "\n",
      "Epoch   204: reducing learning rate of group 0 to 1.0363e-04.ean: -453.65 // 100-ep mean: -374.12, layer1 grad: 0.08, layer2 grad: 0.12\n",
      "Epoch   505: reducing learning rate of group 0 to 5.1814e-05.mean: -298.24 // 100-ep mean: -301.46, layer1 grad: 0.16, layer2 grad: 0.23\n",
      "Epoch   806: reducing learning rate of group 0 to 2.5907e-05.mean: -365.59 // 100-ep mean: -332.52, layer1 grad: 0.11, layer2 grad: 0.20\n",
      "Epoch  1107: reducing learning rate of group 0 to 1.2954e-05. mean: -342.25 // 100-ep mean: -341.71, layer1 grad: 0.16, layer2 grad: 0.25\n",
      "Epoch  1408: reducing learning rate of group 0 to 6.4768e-06. mean: -299.84 // 100-ep mean: -337.30, layer1 grad: 0.29, layer2 grad: 0.50\n",
      "LR: 0.253 / L2 reg: 4.835 / Xavier: Trueard: -509.25 // 10-ep mean: -335.37 // 100-ep mean: -348.64, layer1 grad: 0.15, layer2 grad: 0.33\n",
      "\n",
      "Epoch   206: reducing learning rate of group 0 to 1.2636e-01.an: -236.89 // 100-ep mean: -244.04, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch   507: reducing learning rate of group 0 to 6.3180e-02.ean: -255.04 // 100-ep mean: -246.16, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   808: reducing learning rate of group 0 to 3.1590e-02.ean: -244.53 // 100-ep mean: -235.17, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1109: reducing learning rate of group 0 to 1.5795e-02. mean: -299.77 // 100-ep mean: -265.56, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1410: reducing learning rate of group 0 to 7.8975e-03. mean: -280.87 // 100-ep mean: -270.78, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.486 / L2 reg: 0.001 / Xavier: Trueard: -207.19 // 10-ep mean: -221.86 // 100-ep mean: -244.79, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   504: reducing learning rate of group 0 to 2.4318e-01.ean: -167.13 // 100-ep mean: -182.51, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   805: reducing learning rate of group 0 to 1.2159e-01.ean: -180.33 // 100-ep mean: -182.19, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1141: reducing learning rate of group 0 to 6.0796e-02.mean: -230.41 // 100-ep mean: -187.46, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1442: reducing learning rate of group 0 to 3.0398e-02. mean: -186.47 // 100-ep mean: -179.10, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.000 / L2 reg: 0.011 / Xavier: Trueard: -193.37 // 10-ep mean: -188.61 // 100-ep mean: -178.84, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   202: reducing learning rate of group 0 to 9.2671e-05.ean: -306.47 // 100-ep mean: -388.21, layer1 grad: 0.02, layer2 grad: 0.033\n",
      "Epoch   503: reducing learning rate of group 0 to 4.6336e-05.ean: -612.28 // 100-ep mean: -493.69, layer1 grad: 0.02, layer2 grad: 0.033\n",
      "Epoch   804: reducing learning rate of group 0 to 2.3168e-05.ean: -409.00 // 100-ep mean: -455.55, layer1 grad: 0.01, layer2 grad: 0.024\n",
      "Epoch  1105: reducing learning rate of group 0 to 1.1584e-05.mean: -508.96 // 100-ep mean: -465.93, layer1 grad: 0.02, layer2 grad: 0.033\n",
      "Epoch  1406: reducing learning rate of group 0 to 5.7920e-06. mean: -478.95 // 100-ep mean: -498.97, layer1 grad: 0.02, layer2 grad: 0.033\n",
      "LR: 0.011 / L2 reg: 0.002 / Xavier: Trueard: -1432.31 // 10-ep mean: -667.62 // 100-ep mean: -450.03, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "\n",
      "Epoch   203: reducing learning rate of group 0 to 5.5466e-03.ean: -279.99 // 100-ep mean: -315.67, layer1 grad: 0.03, layer2 grad: 0.05\n",
      "Epoch   504: reducing learning rate of group 0 to 2.7733e-03.ean: -231.25 // 100-ep mean: -267.57, layer1 grad: 0.03, layer2 grad: 0.04\n",
      "Epoch   805: reducing learning rate of group 0 to 1.3866e-03.ean: -286.93 // 100-ep mean: -273.91, layer1 grad: 0.03, layer2 grad: 0.03\n",
      "Epoch  1106: reducing learning rate of group 0 to 6.9332e-04. mean: -310.24 // 100-ep mean: -285.20, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "Epoch  1407: reducing learning rate of group 0 to 3.4666e-04. mean: -244.63 // 100-ep mean: -261.89, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "LR: 0.000 / L2 reg: 9.033 / Xavier: Trueard: -160.29 // 10-ep mean: -255.54 // 100-ep mean: -241.91, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "\n",
      "Epoch   544: reducing learning rate of group 0 to 6.4454e-05.ean: -296.43 // 100-ep mean: -282.69, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   845: reducing learning rate of group 0 to 3.2227e-05.ean: -294.97 // 100-ep mean: -315.72, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1146: reducing learning rate of group 0 to 1.6114e-05. mean: -341.90 // 100-ep mean: -335.78, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1447: reducing learning rate of group 0 to 8.0568e-06. mean: -350.96 // 100-ep mean: -359.82, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.355 / L2 reg: 0.001 / Xavier: Trueard: -341.37 // 10-ep mean: -345.94 // 100-ep mean: -370.75, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   549: reducing learning rate of group 0 to 1.7757e-01.ean: -191.86 // 100-ep mean: -178.32, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   850: reducing learning rate of group 0 to 8.8787e-02.ean: -192.64 // 100-ep mean: -181.72, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1151: reducing learning rate of group 0 to 4.4393e-02.mean: -164.54 // 100-ep mean: -173.53, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1452: reducing learning rate of group 0 to 2.2197e-02. mean: -182.03 // 100-ep mean: -176.19, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.340 / L2 reg: 0.000 / Xavier: Trueard: -179.45 // 10-ep mean: -175.45 // 100-ep mean: -177.79, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   320: reducing learning rate of group 0 to 1.6983e-01.ean: -189.39 // 100-ep mean: -177.08, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   621: reducing learning rate of group 0 to 8.4914e-02.ean: -184.81 // 100-ep mean: -180.41, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   922: reducing learning rate of group 0 to 4.2457e-02.ean: -173.05 // 100-ep mean: -173.37, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1223: reducing learning rate of group 0 to 2.1229e-02.mean: -180.10 // 100-ep mean: -181.95, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.001 / L2 reg: 0.768 / Xavier: Trueard: -185.42 // 10-ep mean: -182.59 // 100-ep mean: -173.82, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   257: reducing learning rate of group 0 to 5.7334e-04.ean: -253.38 // 100-ep mean: -305.87, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   558: reducing learning rate of group 0 to 2.8667e-04.ean: -371.71 // 100-ep mean: -314.74, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   859: reducing learning rate of group 0 to 1.4333e-04.ean: -319.34 // 100-ep mean: -318.84, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1160: reducing learning rate of group 0 to 7.1667e-05. mean: -371.80 // 100-ep mean: -363.73, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1461: reducing learning rate of group 0 to 3.5834e-05. mean: -366.81 // 100-ep mean: -364.23, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "LR: 0.038 / L2 reg: 0.005 / Xavier: Trueard: -465.50 // 10-ep mean: -384.57 // 100-ep mean: -388.41, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "\n",
      "Epoch   617: reducing learning rate of group 0 to 1.8893e-02.ean: -264.12 // 100-ep mean: -326.51, layer1 grad: 0.02, layer2 grad: 0.03\n",
      "Epoch   918: reducing learning rate of group 0 to 9.4467e-03.mean: -432.00 // 100-ep mean: -386.06, layer1 grad: 0.01, layer2 grad: 0.02\n",
      "Epoch  1219: reducing learning rate of group 0 to 4.7233e-03. mean: -433.26 // 100-ep mean: -435.08, layer1 grad: 0.01, layer2 grad: 0.03\n",
      "LR: 0.007 / L2 reg: 0.054 / Xavier: Trueard: -290.34 // 10-ep mean: -425.83 // 100-ep mean: -447.29, layer1 grad: 0.01, layer2 grad: 0.03\n",
      "\n",
      "Epoch   311: reducing learning rate of group 0 to 3.7266e-03.ean: -286.33 // 100-ep mean: -307.12, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   612: reducing learning rate of group 0 to 1.8633e-03.ean: -313.84 // 100-ep mean: -337.71, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   913: reducing learning rate of group 0 to 9.3165e-04.ean: -352.90 // 100-ep mean: -360.83, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1214: reducing learning rate of group 0 to 4.6583e-04. mean: -409.52 // 100-ep mean: -408.82, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "LR: 0.335 / L2 reg: 0.009 / Xavier: Trueard: -452.44 // 10-ep mean: -529.97 // 100-ep mean: -442.75, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "\n",
      "Epoch   700: reducing learning rate of group 0 to 1.6775e-01.ean: -168.97 // 100-ep mean: -174.85, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1197: reducing learning rate of group 0 to 8.3874e-02.mean: -184.52 // 100-ep mean: -183.02, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.005 / L2 reg: 0.000 / Xavier: Trueard: -129.63 // 10-ep mean: -162.33 // 100-ep mean: -182.57, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   423: reducing learning rate of group 0 to 2.5521e-03.mean: -225.01 // 100-ep mean: -252.00, layer1 grad: 0.08, layer2 grad: 0.12\n",
      "Epoch   824: reducing learning rate of group 0 to 1.2761e-03.mean: -276.55 // 100-ep mean: -287.19, layer1 grad: 0.12, layer2 grad: 0.06\n",
      "LR: 0.029 / L2 reg: 0.094 / Xavier: Trueard: -176.84 // 10-ep mean: -161.79 // 100-ep mean: -170.40, layer1 grad: nan, layer2 grad: nan6\n",
      "\n",
      "Epoch   202: reducing learning rate of group 0 to 1.4572e-02.ean: -308.40 // 100-ep mean: -272.50, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   503: reducing learning rate of group 0 to 7.2858e-03.ean: -333.44 // 100-ep mean: -292.96, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   804: reducing learning rate of group 0 to 3.6429e-03.ean: -333.94 // 100-ep mean: -313.62, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1105: reducing learning rate of group 0 to 1.8214e-03. mean: -269.87 // 100-ep mean: -328.92, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1406: reducing learning rate of group 0 to 9.1072e-04. mean: -374.74 // 100-ep mean: -333.09, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.005 / L2 reg: 0.006 / Xavier: Trueard: -147.68 // 10-ep mean: -403.75 // 100-ep mean: -367.66, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   201: reducing learning rate of group 0 to 2.3444e-03.ean: -361.80 // 100-ep mean: -358.95, layer1 grad: 0.02, layer2 grad: 0.04\n",
      "Epoch   502: reducing learning rate of group 0 to 1.1722e-03.ean: -485.22 // 100-ep mean: -434.81, layer1 grad: 0.02, layer2 grad: 0.044\n",
      "Epoch   803: reducing learning rate of group 0 to 5.8610e-04.ean: -487.85 // 100-ep mean: -484.81, layer1 grad: 0.01, layer2 grad: 0.044\n",
      "Epoch  1104: reducing learning rate of group 0 to 2.9305e-04. mean: -546.81 // 100-ep mean: -517.54, layer1 grad: 0.01, layer2 grad: 0.04\n",
      "Epoch  1405: reducing learning rate of group 0 to 1.4652e-04. mean: -436.60 // 100-ep mean: -491.65, layer1 grad: 0.02, layer2 grad: 0.034\n",
      "LR: 0.002 / L2 reg: 0.740 / Xavier: Trueard: -755.74 // 10-ep mean: -479.78 // 100-ep mean: -483.24, layer1 grad: 0.00, layer2 grad: 0.04\n",
      "\n",
      "Epoch   364: reducing learning rate of group 0 to 1.1748e-03.an: -337.66 // 100-ep mean: -294.52, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch   665: reducing learning rate of group 0 to 5.8740e-04.ean: -297.29 // 100-ep mean: -288.18, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   966: reducing learning rate of group 0 to 2.9370e-04.ean: -289.84 // 100-ep mean: -330.38, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1267: reducing learning rate of group 0 to 1.4685e-04. mean: -286.92 // 100-ep mean: -327.45, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.690 / L2 reg: 0.000 / Xavier: Trueard: -309.59 // 10-ep mean: -325.65 // 100-ep mean: -362.44, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "\n",
      "Epoch   204: reducing learning rate of group 0 to 3.4509e-01.ean: -192.65 // 100-ep mean: -187.29, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   505: reducing learning rate of group 0 to 1.7255e-01.ean: -167.37 // 100-ep mean: -178.88, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1007: reducing learning rate of group 0 to 8.6273e-02.mean: -166.10 // 100-ep mean: -176.98, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1308: reducing learning rate of group 0 to 4.3136e-02.mean: -191.80 // 100-ep mean: -177.59, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.831 / L2 reg: 0.044 / Xavier: Trueard: -184.39 // 10-ep mean: -170.74 // 100-ep mean: -174.59, layer1 grad: nan, layer2 grad: nan\n",
      "\n",
      "Epoch   522: reducing learning rate of group 0 to 4.1552e-01.ean: -180.53 // 100-ep mean: -174.04, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch   823: reducing learning rate of group 0 to 2.0776e-01.ean: -176.51 // 100-ep mean: -179.60, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1124: reducing learning rate of group 0 to 1.0388e-01.mean: -184.87 // 100-ep mean: -180.54, layer1 grad: nan, layer2 grad: nan\n",
      "Epoch  1425: reducing learning rate of group 0 to 5.1940e-02. mean: -160.14 // 100-ep mean: -174.76, layer1 grad: nan, layer2 grad: nan\n",
      "LR: 0.003 / L2 reg: 1.418 / Xavier: Trueard: -65.31 // 10-ep mean: -168.95 // 100-ep mean: -174.23, layer1 grad: nan, layer2 grad: nann\n",
      "\n",
      "Epoch   204: reducing learning rate of group 0 to 1.5735e-03.ean: -146.56 // 100-ep mean: -245.94, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   505: reducing learning rate of group 0 to 7.8677e-04.ean: -297.04 // 100-ep mean: -278.49, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   806: reducing learning rate of group 0 to 3.9339e-04.ean: -283.48 // 100-ep mean: -275.34, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1107: reducing learning rate of group 0 to 1.9669e-04. mean: -329.13 // 100-ep mean: -303.38, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch  1408: reducing learning rate of group 0 to 9.8346e-05. mean: -292.67 // 100-ep mean: -318.13, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "LR: 0.001 / L2 reg: 0.100 / Xavier: Trueard: -318.75 // 10-ep mean: -311.71 // 100-ep mean: -339.71, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "\n",
      "Epoch   203: reducing learning rate of group 0 to 3.1938e-04.ean: -333.61 // 100-ep mean: -385.78, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Epoch   504: reducing learning rate of group 0 to 1.5969e-04.ean: -482.24 // 100-ep mean: -440.35, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch   805: reducing learning rate of group 0 to 7.9844e-05.ean: -279.02 // 100-ep mean: -431.38, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch  1106: reducing learning rate of group 0 to 3.9922e-05.mean: -589.97 // 100-ep mean: -376.07, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "Epoch  1407: reducing learning rate of group 0 to 1.9961e-05. mean: -412.72 // 100-ep mean: -490.11, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "LR: 0.022 / L2 reg: 4.009 / Xavier: Trueard: -232.88 // 10-ep mean: -392.69 // 100-ep mean: -446.68, layer1 grad: 0.00, layer2 grad: 0.000\n",
      "\n",
      "Epoch   243: reducing learning rate of group 0 to 1.0792e-02.ean: -228.06 // 100-ep mean: -265.02, layer1 grad: 0.00, layer2 grad: 0.00\n",
      "Ep: 500 // steps: 51124 // last ep reward: -260.10 // 10-ep mean: -291.88 // 100-ep mean: -254.52, layer1 grad: 0.00, layer2 grad: 0.00\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cdfef78de0cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# run_experiment(agent, logger, title=title)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mparam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-cdfef78de0cf>\u001b[0m in \u001b[0;36mparam_search\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m         \u001b[0mexperiments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxavier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# run_experiment(agent, logger, title=title)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mparam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8d99be2502b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mepisode_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8d99be2502b4>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_vectorized_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5e453c1d84cd>\u001b[0m in \u001b[0;36mcategorical_vectorized_loss\u001b[0;34m(online_net, target_net, transitions, discount)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlo_component\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_atoms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhi_component\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_atoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# cross enthropy is Sigma <true> log <unnatural>, so for us is: target log(online)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0monline_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0monline_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-883b0c2bc98f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;34m\"\"\" Return (actions x atoms). \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return F.batch_norm(\n\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1193\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m     )\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def param_search():\n",
    "    for _ in range(50):\n",
    "        lr = 10 ** np.random.uniform(-4, 0)\n",
    "        l2 = 10 ** np.random.uniform(-5, 1)\n",
    "        xavier = True\n",
    "        title = f'LR: {lr:.3f} / L2 reg: {l2:.3f} / Xavier: {xavier}'\n",
    "        '''\n",
    "        config = {\n",
    "            'title':title,\n",
    "            'log_dir':'tuning-categorical',\n",
    "            'random_seed':0\n",
    "        }\n",
    "        logger, config = init_experiment(config)\n",
    "        '''\n",
    "        print(title + '\\n')        \n",
    "        agent = CategoricalAgent(\n",
    "            lunarlander, \n",
    "            learning_rate=lr, \n",
    "            num_episodes=1500,\n",
    "            update_target_every=200,  \n",
    "            reward_scaling=1,\n",
    "            weight_decay=l2,\n",
    "            xavier=xavier\n",
    "        )\n",
    "        experiments[(lr, l2, xavier)] = agent\n",
    "        agent.train()\n",
    "        # run_experiment(agent, logger, title=title)\n",
    "param_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# works beautifully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-23T03:13:39.100Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Run 0: LR: 0.000 / L2 weight_decay: 0.001 / Xavier: True, LR-scheduler: True', 'log_dir': 'tensorboard-data/tuning-categorical', 'random_seed': 0, 'run_name': 'Jul-22-18@23:13:39-DaydreamNation.local', 'run_dir': 'tensorboard-data/tuning-categorical/Jul-22-18@23:13:39-DaydreamNation.local', 'tag': 'Experiment Config: Run 0: LR: 0.000 / L2 weight_decay: 0.001 / Xavier: True, LR-scheduler: True :: Jul-22-18@23:13:39\\n'}\n",
      "using default ValDist class\n",
      "using filtered rewards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liavkoren/Envs/ai-gym/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator reshape because torch.onnx.symbolic.reshape does not exist\n",
      "  .format(op_name, op_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 67 // steps: 8245 // last ep reward: 240.57 // 10-ep mean: 273.99 // 68-ep mean: 228.77, layer1 grad: 0.01, layer2 grad: 0.02\r"
     ]
    }
   ],
   "source": [
    "def run_experiment(agent, logger, title):\n",
    "    monitor_module(agent.target_net, logger, \n",
    "                   track_data=True,\n",
    "                   track_grad=True,\n",
    "                   track_update=True,\n",
    "                   track_update_ratio=True)\n",
    "\n",
    "    agent.train()\n",
    "\n",
    "for idx in range(1):\n",
    "    lr = 0.00003\n",
    "    weight_decay = 0.001\n",
    "    xavier = True\n",
    "    lr_scheduler = True\n",
    "    title = f'Run {idx}: LR: {lr:.3f} / L2 weight_decay: {weight_decay:.3f} / Xavier: {xavier}, LR-scheduler: {lr_scheduler}'\n",
    "    config = {\n",
    "        'title':title,\n",
    "        'log_dir':'tensorboard-data/tuning-categorical',\n",
    "        'random_seed':idx\n",
    "    }\n",
    "    logger, config = init_experiment(config)\n",
    "    print(config)        \n",
    "    # online_net = ValueDistributionDeepBatch(state_shape=8, action_shape=4, vmin=-10, vmax=10, num_atoms=51)\n",
    "    # target_net = ValueDistributionDeepBatch(state_shape=8, action_shape=4, vmin=-10, vmax=10, num_atoms=51)\n",
    "    agent = CategoricalAgent(\n",
    "        lunarlander, \n",
    "        learning_rate=lr, \n",
    "        weight_decay=weight_decay, \n",
    "        use_lr_scheduler=True, \n",
    "        logger=logger,\n",
    "        num_episodes=2000,\n",
    "        max_gradient_norm=.5,\n",
    "        monitor_total=5\n",
    "    )\n",
    "    run_experiment(agent, logger, title=title)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "Getting this working took a *lot* of debugging time. I went fairly far down a blind alley \n",
    "\n",
    "    - not the exploding gradients: small step size was a blind alley\n",
    "    - not the layer init\n",
    "    - Debugging w. three-armed bandit\n",
    "    - comparing against other implementations?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "266px",
    "width": "268px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
