{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liavkoren/Envs/ai-gym/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import math\n",
    "import random\n",
    "\n",
    "import keras\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name).env\n",
    "REPLAY_MEMORY_SIZE = 1e5\n",
    "OBSERVATION_SHAPE = env.observation_space.shape\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "DISCOUNT_RATE = 0.99\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = .01\n",
    "ANNEALING_CONST = .001  # aka Lambda\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "\n",
    "class Memory(deque):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def append(self, thing):\n",
    "        if len(self) > self.size - 1:\n",
    "            self.popleft()\n",
    "        return super().append(thing)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(len(self), batch_size)\n",
    "        return random.sample(self, batch_size)\n",
    "\n",
    "def make_network():\n",
    "    q_network = keras.Sequential()\n",
    "    q_network.add(keras.layers.Dense(64, activation='relu', input_shape=OBSERVATION_SHAPE))\n",
    "    q_network.add(keras.layers.Dense(NUM_ACTIONS, activation='linear'))    \n",
    "    q_network.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "    # q_network.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\n",
    "    return q_network\n",
    "\n",
    "q_network = make_network()\n",
    "replay_memory = Memory(REPLAY_MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting episode 0 Steps: 020.014.013.013.011.012.017.023.037.011.0Starting episode 10 Steps: 17123.018.050.017.013.015.024.010.08.013.0Starting episode 20 Steps: 36225.045.011.011.011.08.028.021.012.016.0Starting episode 30 Steps: 5509.09.017.09.012.011.022.013.017.015.0Starting episode 40 Steps: 68413.012.013.012.011.09.09.030.019.015.0Starting episode 50 Steps: 82710.011.09.011.013.011.012.014.011.016.0Starting episode 60 Steps: 94515.013.011.019.012.021.011.011.010.010.0Starting episode 70 Steps: 107818.019.014.019.014.011.013.011.013.010.0Starting episode 80 Steps: 122012.011.024.015.013.011.015.010.012.011.0Starting episode 90 Steps: 13549.012.012.013.016.020.025.017.029.010.0Starting episode 100 Steps: 151723.016.018.010.013.024.051.020.011.021.0Starting episode 110 Steps: 172413.017.024.021.016.044.024.018.082.018.0Starting episode 120 Steps: 200134.029.022.0136.028.075.033.044.062.0123.0Starting episode 130 Steps: 258740.061.037.066.095.047.060.0101.056.061.0Starting episode 140 Steps: 3211145.089.057.070.071.070.0225.0111.086.0210.0Starting episode 150 Steps: 4345"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6e7f627c4e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mdouble_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-6e7f627c4e69>\u001b[0m in \u001b[0;36mdouble_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-6e7f627c4e69>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mreplay_memory_with_ddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mreplay_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-6e7f627c4e69>\u001b[0m in \u001b[0;36mreplay_vectorized\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mnon_terminal_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_terminal_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_terminal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_terminal_actions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_terminal_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDISCOUNT_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_terminal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0monline_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMINIBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# REMEBER, Q is a func from (state, action) pairs to values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    113\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   4117\u001b[0m     \"\"\"\n\u001b[1;32m   4118\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 4119\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   4120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   4006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4007\u001b[0m     \"\"\"\n\u001b[0;32m-> 4008\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4009\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4010\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ai-gym/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = [] \n",
    "q_hist_target_net2 = []\n",
    "q_hist_primary_net2 = []\n",
    "steps = 0\n",
    "\n",
    "lunar = 'LunarLander-v2'\n",
    "cartpole = 'CartPole-v1'\n",
    "env = gym.make(cartpole).env\n",
    "REPLAY_MEMORY_SIZE = 1e5\n",
    "OBSERVATION_SHAPE = env.observation_space.shape\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "SAVE_EVERY = 2000  # steps\n",
    "RENDER_EVERY = 100\n",
    "\n",
    "\n",
    "def double_dqn():\n",
    "    \n",
    "    replay_memory_with_ddqn = Memory(REPLAY_MEMORY_SIZE)\n",
    "    RENDER = True\n",
    "    NUM_EPISODES = 500\n",
    "    REPORT_EVERY= 10\n",
    "    UPDATE_TARGET_EVERY = 100\n",
    "    MAX_EPISODE_LEN = 2000\n",
    "    USE_DDQN = True\n",
    "    online_net = make_network()\n",
    "    target_net = make_network()\n",
    "    online_net.load_weights(env_name + 'online-net.ht')\n",
    "    target_net.load_weights(env_name + 'target-net.ht')\n",
    "    replay_memory = Memory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "\n",
    "    def replay():\n",
    "        # Bec of the way we've set up the online_net, it gives us Vals for all actions available in one forward \n",
    "        # pass.    \n",
    "        batch = replay_memory_with_ddqn.sample(MINIBATCH_SIZE)  # (batchsize x observation_shape)\n",
    "\n",
    "        states = np.array([exp[0] for exp in batch])\n",
    "        y = target_net.predict(states)  # (batch x num-actions)\n",
    "\n",
    "        terminal_state = np.zeros(OBSERVATION_SHAPE)\n",
    "        states_ = np.array([exp[3] if exp[3] is not None else terminal_state for exp in batch])\n",
    "        online_predicted_actions = online_net.predict(states).argmax(axis=1)\n",
    "        y_ = target_net.predict(states_)\n",
    "\n",
    "        for idx, exp in enumerate(batch):\n",
    "            state, action, reward, state_, terminal = exp        \n",
    "            if state_ is None:\n",
    "                y[idx, action] = reward\n",
    "            else:                \n",
    "                if USE_DDQN:\n",
    "                    best_action = online_predicted_actions[idx]\n",
    "                    y[idx, action] = reward + DISCOUNT_RATE * y_[idx, best_action]\n",
    "                else:\n",
    "                    y[idx, action] = reward + DISCOUNT_RATE * np.amax(y_[idx])                    \n",
    "                # t[a] = r + GAMMA * pTarget_[i][ numpy.argmax(p_[i]) ]\n",
    "        online_net.fit(states, y, batch_size=64, epochs=1, verbose=0)  # REMEBER, Q is a func from (state, action) pairs to values. \n",
    "            \n",
    "    def replay_vectorized():\n",
    "        batch = replay_memory_with_ddqn.sample(MINIBATCH_SIZE)\n",
    "        # unpack all the replay memories into arrays:\n",
    "        states = np.array([transition[0] for transition in batch])  # (batch x state-size)\n",
    "        transition_actions = np.array([transition[1] for transition in batch])\n",
    "        rewards = np.array([transition[2] for transition in batch])  # (batch x 1)\n",
    "        terminal_mask = np.array([True if transition[3] is None else False for transition in batch])  # (batch x 1)\n",
    "        terminal_state = np.zeros(OBSERVATION_SHAPE)\n",
    "        states_ = np.array([transition[3] if transition[3] is not None else terminal_state for transition in batch])  # (batch x state-size)\n",
    "\n",
    "        y = target_net.predict(states)  # (batch x num-actions)            \n",
    "        online_predicted_actions = online_net.predict(states).argmax(axis=1)  # batch x num-action\n",
    "        y_ = target_net.predict(states_)  # (batch x num-actions)\n",
    "        \n",
    "        # set y = r for terminal states:\n",
    "        terminal_state_actions = transition_actions[terminal_mask]\n",
    "        y[terminal_mask, terminal_state_actions] = rewards[terminal_mask]\n",
    "\n",
    "        # DDQN update:\n",
    "        # set y = r + gamma * Q_hat(s', argmax Q(s', a'))\n",
    "        non_terminal_mask = ~terminal_mask\n",
    "        best_actions = online_predicted_actions[non_terminal_mask]\n",
    "        non_terminal_actions = transition_actions[non_terminal_mask]\n",
    "        y[non_terminal_mask, non_terminal_actions] = rewards[non_terminal_mask] + DISCOUNT_RATE * y_[non_terminal_mask, best_actions] \n",
    "        online_net.fit(states, y, batch_size=MINIBATCH_SIZE, epochs=1, verbose=0)  # REMEBER, Q is a func from (state, action) pairs to values.         \n",
    "\n",
    "\n",
    "    def Q_val_one(net, state):\n",
    "        return net.predict(state.reshape((1, OBSERVATION_SHAPE[0]))).flatten()\n",
    "\n",
    "\n",
    "    def main_loop():\n",
    "        \"\"\" \n",
    "        Target network + DDQN.\n",
    "        \"\"\"\n",
    "        global steps, reward_hist_target_net, q_hist_target_net2\n",
    "        \n",
    "        for episode_count in range(int(NUM_EPISODES)):\n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            state = env.reset()\n",
    "            q_hist_target_net2.append(Q_val_one(target_net, state).max())\n",
    "            q_hist_primary_net2.append(Q_val_one(online_net, state).max())\n",
    "            episode_trace = []\n",
    "\n",
    "            if episode_count % REPORT_EVERY == 0:\n",
    "                print('Starting episode %s ' % episode_count, end='', flush=False)\n",
    "                print('Steps: %d' % steps, end='', flush=False)\n",
    "            if steps % SAVE_EVERY == 0:                \n",
    "                online_net.save(env_name + 'online-net.ht')\n",
    "                target_net.save(env_name + 'target-net.ht')\n",
    "            episode_len = 0\n",
    "            while not episode_done:\n",
    "                episode_len += 1\n",
    "                EPSILON = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN) * math.exp(-ANNEALING_CONST * steps)\n",
    "                steps += 1\n",
    "\n",
    "                if random.random() < EPSILON:\n",
    "                    action = random.randint(0, NUM_ACTIONS-1)\n",
    "                else:\n",
    "                    action = Q_val_one(online_net, state).argmax()\n",
    "\n",
    "                state_, reward, episode_done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                if RENDER and episode_count % RENDER_EVERY == 0:\n",
    "                    env.render()\n",
    "                if episode_done:\n",
    "                    state_ = None\n",
    "\n",
    "                replay_memory_with_ddqn.append((state, action, reward, state_, episode_done))\n",
    "                replay_vectorized()\n",
    "                state = state_\n",
    "\n",
    "                if steps % UPDATE_TARGET_EVERY == 0:\n",
    "                    target_net.set_weights(online_net.get_weights())\n",
    "                if episode_len > MAX_EPISODE_LEN:\n",
    "                    episode_done = True\n",
    "\n",
    "                if episode_done:\n",
    "                    print(episode_reward, end='', flush=False)            \n",
    "                    episode_rewards.append(episode_reward)\n",
    "    main_loop()\n",
    "\n",
    "\n",
    "double_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
