{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CategoricalPolicyImprovement(object):\n",
    "    \"\"\" Deep Q-Learning training method. \"\"\"\n",
    "\n",
    "    def __init__(self, policy, target_policy, lr=0.00025, discount=0.95, v_min=-10, v_max=10, atoms_no=51, batch_size=32):\n",
    "        self.name = 'Categorical-PI'\n",
    "        self.policy = policy\n",
    "        self.target_policy = target_policy\n",
    "        self.lr = lr\n",
    "        self.gamma = discount\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        self.dtype = torch.float\n",
    "        self.v_min, self.v_max = v_min, v_max\n",
    "        self.atoms_no = atoms_no\n",
    "        self.support = torch.linspace(v_min, v_max, atoms_no)\n",
    "        self.support = self.support.type(self.dtype)\n",
    "        self.delta_z = (v_max - v_min) / (atoms_no - 1)\n",
    "        self.m = torch.zeros(batch_size, self.atoms_no).type(self.dtype)\n",
    "\n",
    "    def accumulate_gradient(self, batch_sz, states, actions, rewards,\n",
    "                            next_states, mask):\n",
    "        \"\"\" Compute the difference between the return distributions of Q(s,a)\n",
    "            and TQ(s_,a).\n",
    "        \"\"\"\n",
    "        states = Variable(states)\n",
    "        actions = Variable(actions)\n",
    "        next_states = Variable(next_states, volatile=True)\n",
    "\n",
    "        # Compute probabilities of Q(s,a*)\n",
    "        q_probs = self.policy(states)\n",
    "        actions = actions.view(batch_sz, 1, 1)\n",
    "        action_mask = actions.expand(batch_sz, 1, self.atoms_no)\n",
    "        qa_probs = q_probs.gather(1, action_mask).squeeze()\n",
    "\n",
    "        # Compute distribution of Q(s_,a)\n",
    "        target_qa_probs = self._get_categorical(next_states, rewards, mask)\n",
    "\n",
    "        # Compute the cross-entropy of phi(TZ(x_,a)) || Z(x,a)\n",
    "        qa_probs = qa_probs.clamp(min=1e-3)  # Tudor's trick for avoiding nans\n",
    "        loss = - torch.sum(target_qa_probs * torch.log(qa_probs))\n",
    "\n",
    "        # Accumulate gradients\n",
    "        loss.backward()\n",
    "\n",
    "    def update_model(self):\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def _get_categorical(self, next_states, rewards, mask):\n",
    "        batch_sz = next_states.size(0)\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # Compute probabilities p(x, a)\n",
    "        probs = self.target_policy(next_states).data\n",
    "        qs = torch.mul(probs, self.support.expand_as(probs))\n",
    "        argmax_a = qs.sum(2).max(1)[1].unsqueeze(1).unsqueeze(1)\n",
    "        action_mask = argmax_a.expand(batch_sz, 1, self.atoms_no)\n",
    "        qa_probs = probs.gather(1, action_mask).squeeze()\n",
    "\n",
    "        # Mask gamma and reshape it torgether with rewards to fit p(x,a).\n",
    "        rewards = rewards.expand_as(qa_probs)\n",
    "        gamma = (mask.float() * gamma).expand_as(qa_probs)\n",
    "\n",
    "        # Compute projection of the application of the Bellman operator.\n",
    "        bellman_op = rewards + gamma * self.support.unsqueeze(0).expand_as(rewards)\n",
    "        bellman_op = torch.clamp(bellman_op, self.v_min, self.v_max)\n",
    "\n",
    "        # Compute categorical indices for distributing the probability\n",
    "        m = self.m.fill_(0)\n",
    "        b = (bellman_op - self.v_min) / self.delta_z\n",
    "        l = b.floor().long()\n",
    "        u = b.ceil().long()\n",
    "        # Fix disappearing probability mass when l = b = u (b is int)\n",
    "        l[(u > 0) * (l == u)] -= 1\n",
    "        u[(l < (self.atoms_no - 1)) * (l == u)] += 1\n",
    "\n",
    "        # Distribute probability\n",
    "        \"\"\"\n",
    "        for i in range(batch_sz):\n",
    "            for j in range(self.atoms_no):\n",
    "                uidx = u[i][j]\n",
    "                lidx = l[i][j]\n",
    "                m[i][lidx] = m[i][lidx] + qa_probs[i][j] * (uidx - b[i][j])\n",
    "                m[i][uidx] = m[i][uidx] + qa_probs[i][j] * (b[i][j] - lidx)\n",
    "        for i in range(batch_sz):\n",
    "            m[i].index_add_(0, l[i], qa_probs[i] * (u[i].float() - b[i]))\n",
    "            m[i].index_add_(0, u[i], qa_probs[i] * (b[i] - l[i].float()))\n",
    "        \"\"\"\n",
    "        # Optimized by https://github.com/tudor-berariu\n",
    "        offset = torch.linspace(0, ((batch_sz - 1) * self.atoms_no), batch_sz)\\\n",
    "            .type(self.dtype.LT)\\\n",
    "            .unsqueeze(1).expand(batch_sz, self.atoms_no)\n",
    "\n",
    "        m.view(-1).index_add_(0, (l + offset).view(-1),\n",
    "                              (qa_probs * (u.float() - b)).view(-1))\n",
    "        m.view(-1).index_add_(0, (u + offset).view(-1),\n",
    "                              (qa_probs * (b - l.float())).view(-1))\n",
    "        return Variable(m)\n",
    "\n",
    "    def update_target_net(self):\n",
    "        \"\"\" Update the target net with the parameters in the online model.\"\"\"\n",
    "        self.target_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def get_model_stats(self):\n",
    "        param_abs_mean = 0\n",
    "        grad_abs_mean = 0\n",
    "        t_param_abs_mean = 0\n",
    "        n_params = 0\n",
    "        for p in self.policy.parameters():\n",
    "            param_abs_mean += p.data.abs().sum()\n",
    "            grad_abs_mean += p.grad.data.abs().sum()\n",
    "            n_params += p.data.nelement()\n",
    "        for t in self.target_policy.parameters():\n",
    "            t_param_abs_mean += t.data.abs().sum()\n",
    "\n",
    "        print(\"Wm: %.9f | Gm: %.9f | Tm: %.9f\" % (\n",
    "            param_abs_mean / n_params,\n",
    "            grad_abs_mean / n_params,\n",
    "            t_param_abs_mean / n_params))\n",
    "\n",
    "    def _debug_transitions(self, mask, reward_batch):\n",
    "        if mask[0] == 0:\n",
    "            r = reward_batch[0, 0]\n",
    "            if r == 1.0:\n",
    "                print(r)\n",
    "\n",
    "    def _debug_states(self, state_batch, next_state_batch, mask):\n",
    "        for i in range(24):\n",
    "            for j in range(24):\n",
    "                px = state_batch[0, 0, i, j]\n",
    "                if px < 0.90:\n",
    "                    print(clr(\"%.2f  \" % px, 'magenta'), end=\"\")\n",
    "                else:\n",
    "                    print((\"%.2f  \" % px), end=\"\")\n",
    "            print()\n",
    "        for i in range(24):\n",
    "            for j in range(24):\n",
    "                px = next_state_batch[0, 0, i, j]\n",
    "                if px < 0.90:\n",
    "                    print(clr(\"%.2f  \" % px, 'magenta'), end=\"\")\n",
    "                else:\n",
    "                    print(clr(\"%.2f  \" % px, 'white'), end=\"\")\n",
    "            print()\n",
    "        if mask[0] == 0:\n",
    "            print(clr(\"Done batch ............\", 'magenta'))\n",
    "        else:\n",
    "            print(\".......................\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
