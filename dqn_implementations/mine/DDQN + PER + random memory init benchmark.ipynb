{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized experience replay with random agent initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from imp import reload\n",
    "import os\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import seed as rng_seed\n",
    "from smooth import smooth\n",
    "import tensorflow as tf\n",
    "\n",
    "import ddqn_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pin RNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/reproducible-results-neural-networks-keras/#comment-414394\n",
    "\n",
    "rng_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(agent_class, num_runs = 3, random_seeds = (0, 100, 1000), env_name='CartPole-v1', **kwargs):\n",
    "    assert len(random_seeds) == num_runs\n",
    "    runs = []\n",
    "    for idx in range(num_runs):  \n",
    "        np.random.seed(random_seeds[idx])\n",
    "        start = datetime.now()\n",
    "        agent = agent_class(env_name, **kwargs)\n",
    "        print(f'{start}')\n",
    "        agent.run()\n",
    "        end = datetime.now()\n",
    "        data = {'instance': agent, 'total_time': (end-start).total_seconds()}\n",
    "        runs.append(data)\n",
    "        print(f'\\nRun {idx} total time: {(end-start).total_seconds()/60:.2f} mins')        \n",
    "    return runs\n",
    "\n",
    "def print_benchmark(data):\n",
    "    for run in data:\n",
    "        agent = run['instance']\n",
    "        time = run['total_time']\n",
    "        for key, val in agent.__dict__.items():\n",
    "            if isinstance(val, list):\n",
    "                continue\n",
    "            print(f'{key}: {val}')\n",
    "\n",
    "        print(f'\\nTotal training time: {time/60:.2f} minutes')\n",
    "        print('---')    \n",
    "        \n",
    "\n",
    "def plot_timeseries(data, hline_at=100):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for series in data:\n",
    "        plt.plot(series, alpha=0.7)\n",
    "    plt.hlines(hline_at, 0, len(series), linestyles='dotted', )\n",
    "    plt.legend(list(range(0, len(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-15__19--10--01/PER/CartPole-v1_agent.params\n",
      "2018-05-15 19:10:01.260419\n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.data\n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.datad: 0.0, 100-episode mean reward: 0.0 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataeward: 10.3, 100-episode mean reward: 11.82 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.data\n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataward: 10.3, 100-episode mean reward: 11.41 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataeward: 10.1, 100-episode mean reward: 10.91 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataeward: 10.9, 100-episode mean reward: 10.56 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataeward: 13.3, 100-episode mean reward: 11.42 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataeward: 81.5, 100-episode mean reward: 42.56   \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.data reward: 223.6, 100-episode mean reward: 275.66 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.datan reward: 268.3, 100-episode mean reward: 249.77 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.data\n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.datan reward: 287.8, 100-episode mean reward: 252.36 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 264.8, 100-episode mean reward: 381.94 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 269.5, 100-episode mean reward: 288.51 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 258.0, 100-episode mean reward: 284.82 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 258.1, 100-episode mean reward: 250.76 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.data\n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 256.6, 100-episode mean reward: 249.43 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 451.6, 100-episode mean reward: 311.83 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 273.0, 100-episode mean reward: 281.64 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 259.0, 100-episode mean reward: 277.34 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 298.7, 100-episode mean reward: 279.81 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 236.0, 100-episode mean reward: 278.15 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 222.0, 100-episode mean reward: 273.29 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 216.8, 100-episode mean reward: 271.19 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 217.9, 100-episode mean reward: 245.11 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 223.3, 100-episode mean reward: 236.13 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.data\n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 219.8, 100-episode mean reward: 236.43 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 221.0, 100-episode mean reward: 236.08 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 224.1, 100-episode mean reward: 229.76 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 208.7, 100-episode mean reward: 267.5  \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 140.5, 100-episode mean reward: 212.25 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 238.1, 100-episode mean reward: 240.66 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 304.2, 100-episode mean reward: 258.94 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 254.2, 100-episode mean reward: 440.6  \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 251.5, 100-episode mean reward: 245.81 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 500.0, 100-episode mean reward: 275.1  \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 415.2, 100-episode mean reward: 278.59 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 350.3, 100-episode mean reward: 309.95 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 310.4, 100-episode mean reward: 324.74 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 362.0, 100-episode mean reward: 381.43 \n",
      "Saved data to data/2018-05-15__19--10--01/PER/history.dataan reward: 400.3, 100-episode mean reward: 407.59 \n",
      "\n",
      "Run 0 total time: 81.51 mins\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-15__20--31--32/PER/CartPole-v1_agent.params\n",
      "2018-05-15 20:31:31.715007\n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data\n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datad: 0.0, 100-episode mean reward: 0.0 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataard: 20.0, 100-episode mean reward: 2.0 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataward: 10.3, 100-episode mean reward: 12.73 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataward: 10.2, 100-episode mean reward: 11.06  \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataeward: 10.3, 100-episode mean reward: 10.17 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datareward: 107.3, 100-episode mean reward: 29.65 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data reward: 242.0, 100-episode mean reward: 100.92 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data\n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data reward: 251.6, 100-episode mean reward: 352.15 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data reward: 275.4, 100-episode mean reward: 334.41 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data reward: 281.2, 100-episode mean reward: 270.05 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data reward: 233.3, 100-episode mean reward: 264.34 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datan reward: 300.4, 100-episode mean reward: 294.17 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datan reward: 327.1, 100-episode mean reward: 380.17 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datan reward: 317.8, 100-episode mean reward: 377.36 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data\n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datan reward: 290.0, 100-episode mean reward: 366.4 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 452.9, 100-episode mean reward: 423.41 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 465.7, 100-episode mean reward: 472.49 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data\n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 468.5, 100-episode mean reward: 451.43 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data\n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 381.6, 100-episode mean reward: 408.15 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 409.3, 100-episode mean reward: 400.98 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 462.2, 100-episode mean reward: 412.64 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 200.9, 100-episode mean reward: 279.23 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 246.5, 100-episode mean reward: 208.84 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 237.6, 100-episode mean reward: 279.43 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 260.3, 100-episode mean reward: 244.24 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 410.4, 100-episode mean reward: 270.37 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 296.9, 100-episode mean reward: 337.24 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 297.4, 100-episode mean reward: 336.87 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 306.5, 100-episode mean reward: 330.71 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.data\n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datan reward: 262.5, 100-episode mean reward: 405.23  \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.datan reward: 157.8, 100-episode mean reward: 321.98 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 218.9, 100-episode mean reward: 203.45 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 255.3, 100-episode mean reward: 233.31 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 232.2, 100-episode mean reward: 240.99 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 224.3, 100-episode mean reward: 242.33 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 225.5, 100-episode mean reward: 231.37 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 207.6, 100-episode mean reward: 232.27 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 228.6, 100-episode mean reward: 224.82 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 197.4, 100-episode mean reward: 203.83 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 193.6, 100-episode mean reward: 197.41 \n",
      "Saved data to data/2018-05-15__20--31--32/PER/history.dataan reward: 163.2, 100-episode mean reward: 175.69 \n",
      "\n",
      "Run 1 total time: 72.40 mins\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-15__21--43--57/PER/CartPole-v1_agent.params\n",
      "2018-05-15 21:43:56.013544\n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data\n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.datad: 0.0, 100-episode mean reward: 0.0 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataward: 10.2, 100-episode mean reward: 10.48  \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataeward: 10.3, 100-episode mean reward: 10.25 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataeward: 62.0, 100-episode mean reward: 29.08 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data\n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataeward: 48.4, 100-episode mean reward: 32.9 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataeward: 47.4, 100-episode mean reward: 40.72 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.datareward: 58.9, 100-episode mean reward: 57.09 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data reward: 284.3, 100-episode mean reward: 267.39 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data reward: 248.2, 100-episode mean reward: 250.95 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data reward: 273.3, 100-episode mean reward: 249.25 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data reward: 243.6, 100-episode mean reward: 243.17 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.datan reward: 121.1, 100-episode mean reward: 150.53 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.datan reward: 106.0, 100-episode mean reward: 134.64 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.datan reward: 115.0, 100-episode mean reward: 129.08 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.datan reward: 119.5, 100-episode mean reward: 126.33 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 232.3, 100-episode mean reward: 147.11 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 300.5, 100-episode mean reward: 176.67 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 325.3, 100-episode mean reward: 299.42 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 258.8, 100-episode mean reward: 320.28 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 310.4, 100-episode mean reward: 317.57 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 451.5, 100-episode mean reward: 372.07 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data\n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 291.1, 100-episode mean reward: 403.66 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 221.7, 100-episode mean reward: 375.83 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 256.0, 100-episode mean reward: 353.38 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 278.7, 100-episode mean reward: 276.41 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 192.0, 100-episode mean reward: 226.48 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.data\n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 252.5, 100-episode mean reward: 268.06 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 324.0, 100-episode mean reward: 271.14 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 183.6, 100-episode mean reward: 262.94 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 269.9, 100-episode mean reward: 212.09 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 269.1, 100-episode mean reward: 226.05 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 229.0, 100-episode mean reward: 261.81 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 240.0, 100-episode mean reward: 259.28 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 265.7, 100-episode mean reward: 266.03 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 339.8, 100-episode mean reward: 286.27 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 318.8, 100-episode mean reward: 295.65 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 341.7, 100-episode mean reward: 301.98 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 395.3, 100-episode mean reward: 412.29 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 343.5, 100-episode mean reward: 389.33 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 367.4, 100-episode mean reward: 384.68 \n",
      "Saved data to data/2018-05-15__21--43--57/PER/history.dataan reward: 401.2, 100-episode mean reward: 384.52 \n",
      "\n",
      "Run 2 total time: 48.98 mins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instance': PrioritizedExperienceReplayDDQN(env_name='CartPole-v1', memory_size=100000, save_every=50, render_every=0, num_episodes=3000, update_target_every=300, report_every=10, max_episode_len=700, batch_size=64, discount_rate=0.99, epsilon_max=1.0, epsilon_min=0.01, annealing_const=0.001, data_directory='data', random_init_steps=1000),\n",
       "  'total_time': 4890.45441},\n",
       " {'instance': PrioritizedExperienceReplayDDQN(env_name='CartPole-v1', memory_size=100000, save_every=50, render_every=0, num_episodes=3000, update_target_every=300, report_every=10, max_episode_len=700, batch_size=64, discount_rate=0.99, epsilon_max=1.0, epsilon_min=0.01, annealing_const=0.001, data_directory='data', random_init_steps=1000),\n",
       "  'total_time': 4344.298366},\n",
       " {'instance': PrioritizedExperienceReplayDDQN(env_name='CartPole-v1', memory_size=100000, save_every=50, render_every=0, num_episodes=3000, update_target_every=300, report_every=10, max_episode_len=700, batch_size=64, discount_rate=0.99, epsilon_max=1.0, epsilon_min=0.01, annealing_const=0.001, data_directory='data', random_init_steps=1000),\n",
       "  'total_time': 2938.919341}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ddqn_per_class.PrioritizedExperienceReplayDDQN\n",
    "benchmark(agent, render_every=0, save_every=50, update_target_every=300, num_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ddqn_per_class' from '/Users/liavkoren/AI Curriculum/dqn_implementations/mine/ddqn_per_class.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ddqn_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-16__10--12--49/PER/CartPole-v1_agent.params\n"
     ]
    }
   ],
   "source": [
    "ddq_per = ddqn_per_class.PrioritizedExperienceReplayDDQN('CartPole-v1', save_every=50, update_target_every=300, num_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 90, steps: 2027, reward: 34.0, 10-episode mean reward: 14.8, 100-episode mean reward: 19.93 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-55a473bb0aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddq_per\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/ddqn_per_class.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/ddqn_per_class.py\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# A list of (idx, observation) tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/ddqn_per_class.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msumtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mdataIdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapacity\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataIdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI Curriculum/dqn_implementations/mine/SumTree.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self, idx, s)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddq_per.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-16__10--13--32/PER/CartPole-v1_agent.params\n"
     ]
    }
   ],
   "source": [
    "ddq_per_bigger_memory = ddqn_per_class.PrioritizedExperienceReplayDDQN(\n",
    "    'CartPole-v1', \n",
    "    save_every=50, \n",
    "    update_target_every=300, \n",
    "    num_episodes=3000, \n",
    "    memory_size=int(1e6),\n",
    "    render_every=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2990, steps: 603270, reward: 500.0, 10-episode mean reward: 467.9, 100-episode mean reward: 304.98 \r"
     ]
    }
   ],
   "source": [
    "ddq_per_bigger_memory.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger memory seems to make the q histories more stable, but convergence also was slower. What if we increase the target network update freq back to default?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-16__12--22--13/PER/CartPole-v1_agent.params\n",
      "Episode: 2990, steps: 1090352, reward: 216.0, 10-episode mean reward: 360.2, 100-episode mean reward: 463.43 \r"
     ]
    }
   ],
   "source": [
    "ddq_per_bigger_memory2 = ddqn_per_class.PrioritizedExperienceReplayDDQN(\n",
    "    'CartPole-v1', \n",
    "    save_every=50, \n",
    "    memory_size=int(1e6),\n",
    "    render_every=20,\n",
    "    num_episodes=3000, \n",
    ")\n",
    "ddq_per_bigger_memory2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Importance Sampling Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ddqn_per_class' from '/Users/liavkoren/AI Curriculum/dqn_implementations/mine/ddqn_per_class.py'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ddqn_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-17__15--31--18/PER/CartPole-v1_agent.params\n",
      "2018-05-17 15:31:18.100732\n",
      "Episode: 999, steps: 257769, reward: 275.0, 10-episode mean reward: 254.9, 100-episode mean reward: 255.28 \n",
      "Run 0 total time: 17.43 mins\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-17__15--48--44/PER/CartPole-v1_agent.params\n",
      "2018-05-17 15:48:44.079839\n",
      "Episode: 999, steps: 209173, reward: 224.0, 10-episode mean reward: 325.0, 100-episode mean reward: 286.57 \n",
      "Run 1 total time: 14.24 mins\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-17__16--02--58/PER/CartPole-v1_agent.params\n",
      "2018-05-17 16:02:58.268060\n",
      "Episode: 999, steps: 292567, reward: 500.0, 10-episode mean reward: 257.8, 100-episode mean reward: 360.14 \n",
      "Run 2 total time: 19.27 mins\n"
     ]
    }
   ],
   "source": [
    "agent = ddqn_per_class.PrioritizedExperienceReplayDDQN\n",
    "use_importance_weights = benchmark(\n",
    "    agent, \n",
    "    render_every=0, \n",
    "    save_every=50, \n",
    "    use_importance_weights=True,\n",
    "    memory_size=int(1e6),\n",
    "    report_every=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-17__17--52--33/PER/CartPole-v1_agent.params\n",
      "2018-05-17 17:52:33.290678\n",
      "Episode: 999, steps: 292222, reward: 325.0, 10-episode mean reward: 424.6, 100-episode mean reward: 387.57 \n",
      "Run 0 total time: 26.46 mins\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-17__18--19--01/PER/CartPole-v1_agent.params\n",
      "2018-05-17 18:19:00.918385\n",
      "Episode: 999, steps: 241992, reward: 246.0, 10-episode mean reward: 270.2, 100-episode mean reward: 343.24 \n",
      "Run 1 total time: 17.82 mins\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting to initializing memory with random agent.\n",
      "Finished initializing memory with random agent.\n",
      "Saved network to data/2018-05-17__18--36--50/PER/CartPole-v1_agent.params\n",
      "2018-05-17 18:36:50.173250\n",
      "Episode: 999, steps: 190095, reward: 186.0, 10-episode mean reward: 236.4, 100-episode mean reward: 315.69 \n",
      "Run 2 total time: 18.13 mins\n"
     ]
    }
   ],
   "source": [
    "agent = ddqn_per_class.PrioritizedExperienceReplayDDQN\n",
    "use_importance_weights = benchmark(\n",
    "    agent, \n",
    "    render_every=0, \n",
    "    save_every=50, \n",
    "    use_importance_weights=False,\n",
    "    memory_size=int(1e6),\n",
    "    report_every=1,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
